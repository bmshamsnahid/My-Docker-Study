{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home My Daily Docker Study Notes Select the notes from the top Navigation","title":"Home"},{"location":"#home","text":"My Daily Docker Study Notes","title":"Home"},{"location":"Notes/01 Docker Essential/01 Docker 101/","text":"Docker 101 What is Docker? According to wikipedia Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Just like we can manage our application, docker enables us to manage the application infrastructure as well. Why Docker? While we try to run an existing code base, we often have to troubleshoot environment issues. This could be dependency issue, module installation problem or environment mis-match. Docker, in its core is trying to fix these problems. Docker is trying to make it super easy and really straight-forward for anyone to run any code-base or software in any pc, desktop or even server. In a nutshell Docker make it really easy to install and run software without worrying about setup and dependencies. Docker Ecosystem Dockers ecosystem contains Docker Client Docker Server Docker Machine Docker Image Docker Hub Docker Compose Docker Image : Single file with all the dependencies and config required to run a program. Docker Container : Instance of the Docker Image . Docker Hub : Repository of free public Docker Images , can be downloaded to local machine to use. Docker Client : Took the command from the user Do the pre-processing Pass it to the Docker Server Docker Server do the heavy processing An Docker Example : Assuming you have already installed docker in your system, let's run, docker run hello-world It imply to run an container from the image hello-world This hello-world is a tiny little program, whose sole purpose is to print Hello from Docker! Docker Server check the local image cache . If it is not exist in the local image cache it goes to Docker Hub and download the image . Finally the Docker Server run the image as container or image instance . If we run the same command again and the image is already in the cache, It does not download it from the Docker Hub .","title":"01 Docker 101"},{"location":"Notes/01 Docker Essential/01 Docker 101/#docker-101","text":"","title":"Docker 101"},{"location":"Notes/01 Docker Essential/01 Docker 101/#what-is-docker","text":"According to wikipedia Docker is a set of platform as a service (PaaS) products that use OS-level virtualization to deliver software in packages called containers. Just like we can manage our application, docker enables us to manage the application infrastructure as well.","title":"What is Docker?"},{"location":"Notes/01 Docker Essential/01 Docker 101/#why-docker","text":"While we try to run an existing code base, we often have to troubleshoot environment issues. This could be dependency issue, module installation problem or environment mis-match. Docker, in its core is trying to fix these problems. Docker is trying to make it super easy and really straight-forward for anyone to run any code-base or software in any pc, desktop or even server. In a nutshell Docker make it really easy to install and run software without worrying about setup and dependencies.","title":"Why Docker?"},{"location":"Notes/01 Docker Essential/01 Docker 101/#docker-ecosystem","text":"Dockers ecosystem contains Docker Client Docker Server Docker Machine Docker Image Docker Hub Docker Compose Docker Image : Single file with all the dependencies and config required to run a program. Docker Container : Instance of the Docker Image . Docker Hub : Repository of free public Docker Images , can be downloaded to local machine to use. Docker Client : Took the command from the user Do the pre-processing Pass it to the Docker Server Docker Server do the heavy processing An Docker Example : Assuming you have already installed docker in your system, let's run, docker run hello-world It imply to run an container from the image hello-world This hello-world is a tiny little program, whose sole purpose is to print Hello from Docker! Docker Server check the local image cache . If it is not exist in the local image cache it goes to Docker Hub and download the image . Finally the Docker Server run the image as container or image instance . If we run the same command again and the image is already in the cache, It does not download it from the Docker Hub .","title":"Docker Ecosystem"},{"location":"Notes/01 Docker Essential/02 Docker Container/","text":"Docker Container How OS runs on machine? Kernel : Most OS has a kernel. It runs the software processes that govern the access between all the programs running on the computer and all the physical hardware connected to the computer. For example, If we write a file in physical hard-disk using node.js, it's not node.js that speaking directly to the physical device. Actually node.js make a system call with necessary information to the kernel and the kernel persist the file in the physical storage. So the kernel is an intermediary layer that govern the access between the program and physical devices. System Call : The way program pass instruction to the kernel is called system call. These system calls are very much like function invocation. The kernel exposes different endpoint as system call and program uses these system call to perform an operation. For example, to write a file, the kernel expose an endpoint as system call. Program like node.js invoke that system call with necessary information as parameters. This parameters can contain the file name, file content etc. A Chaos Scenario : Lets consider a situation, where there are two program that require two types of node.js runtime. Program 01 (Require Node.js 8.0) Program 02 (Require Node.js 12.0) Since, we can not install these 2 versions in runtime, only one program can work at a time. A Solution With Namespace : OS has a feature to use namespace. Using namespace, we can segment hard-disk in multiple parts. For example, to use two versions of Node.js during runtime, one of the segment will contain node.js runtime version of 8.0 and another segment will contain node.js runtime version 12.0 In this way, we can ensure the program 01 will use the segment of node.js with 8.0 runtime and the program 02 will use the segment of node.js 12.0 as runtime. Here the kernel will determine the segment during system call by the programs. So for program 01 the kernel drag the system call from segment with 8.0 node js and for program 02 the kernel will drag the system call from the segment with 12.0 node.js runtime. Selecting segment,, using system call, based on the program is called namespacing. This allow isolating resources per processes or a group of processes. Namespace can be expanded for both cases Hardware Elements Physical Storage Device Network Devices Software Elements Inter process communications Observer and use of processes Control Group : Also known as cgroup . This limits the amount of resources that a particular process can use. For example it determine, Amount of memory can be used by a process Number of I/O can be used by a process How much CPU a process can use How much Network bandwidth can a process use Namespacing Vs cgroup : Namespacing allow to restrict using a resource. While the cgroup restrict the amount of resource a process can use. The Docker Container Docker container is a group of Process or Group of processes Kernel Isolated resource In a container kernel observes the system call and for a process, guide to the specified isolated resources . The windows and Mac OS does not have the feature of namespacing and cgroup . When we install docker in these machines, a linux virtual machine is installed to handle the isolation of resources for the processes. Relations Between Image and Container : An image have the following components File System snapshot Startup commands When we take an image and turn it into a container, the kernel isolated the specified resource just for the container. Then the process or file system snapshot takes places in the physical storage. While we run the startup commands, it installed these process from the physical storage and start making the system call using the kernel .","title":"02 Docker Container"},{"location":"Notes/01 Docker Essential/02 Docker Container/#docker-container","text":"","title":"Docker Container"},{"location":"Notes/01 Docker Essential/02 Docker Container/#how-os-runs-on-machine","text":"Kernel : Most OS has a kernel. It runs the software processes that govern the access between all the programs running on the computer and all the physical hardware connected to the computer. For example, If we write a file in physical hard-disk using node.js, it's not node.js that speaking directly to the physical device. Actually node.js make a system call with necessary information to the kernel and the kernel persist the file in the physical storage. So the kernel is an intermediary layer that govern the access between the program and physical devices. System Call : The way program pass instruction to the kernel is called system call. These system calls are very much like function invocation. The kernel exposes different endpoint as system call and program uses these system call to perform an operation. For example, to write a file, the kernel expose an endpoint as system call. Program like node.js invoke that system call with necessary information as parameters. This parameters can contain the file name, file content etc. A Chaos Scenario : Lets consider a situation, where there are two program that require two types of node.js runtime. Program 01 (Require Node.js 8.0) Program 02 (Require Node.js 12.0) Since, we can not install these 2 versions in runtime, only one program can work at a time. A Solution With Namespace : OS has a feature to use namespace. Using namespace, we can segment hard-disk in multiple parts. For example, to use two versions of Node.js during runtime, one of the segment will contain node.js runtime version of 8.0 and another segment will contain node.js runtime version 12.0 In this way, we can ensure the program 01 will use the segment of node.js with 8.0 runtime and the program 02 will use the segment of node.js 12.0 as runtime. Here the kernel will determine the segment during system call by the programs. So for program 01 the kernel drag the system call from segment with 8.0 node js and for program 02 the kernel will drag the system call from the segment with 12.0 node.js runtime. Selecting segment,, using system call, based on the program is called namespacing. This allow isolating resources per processes or a group of processes. Namespace can be expanded for both cases Hardware Elements Physical Storage Device Network Devices Software Elements Inter process communications Observer and use of processes Control Group : Also known as cgroup . This limits the amount of resources that a particular process can use. For example it determine, Amount of memory can be used by a process Number of I/O can be used by a process How much CPU a process can use How much Network bandwidth can a process use Namespacing Vs cgroup : Namespacing allow to restrict using a resource. While the cgroup restrict the amount of resource a process can use.","title":"How OS runs on machine?"},{"location":"Notes/01 Docker Essential/02 Docker Container/#the-docker-container","text":"Docker container is a group of Process or Group of processes Kernel Isolated resource In a container kernel observes the system call and for a process, guide to the specified isolated resources . The windows and Mac OS does not have the feature of namespacing and cgroup . When we install docker in these machines, a linux virtual machine is installed to handle the isolation of resources for the processes. Relations Between Image and Container : An image have the following components File System snapshot Startup commands When we take an image and turn it into a container, the kernel isolated the specified resource just for the container. Then the process or file system snapshot takes places in the physical storage. While we run the startup commands, it installed these process from the physical storage and start making the system call using the kernel .","title":"The Docker Container"},{"location":"Notes/01 Docker Essential/03 Docker Lifecycle/","text":"Docker Lifecycle When we run a docker container using run command, docker run image_name Then, the docker run is equivalent to the following 2 commands: docker create docker start With docker create , the file system snapshot of the image is being copied to isolated physical storage . Then with docker start we start the container . Example : Let's do the hands on what we are claiming with a image hello-world . docker create hello-world This will return the id of the created container. Using the id we can now start the docker. docker start -a id This will give us the output Hello from Docker! . Here the -a flag watch out the container output and print it in console .","title":"03 Docker Lifecycle"},{"location":"Notes/01 Docker Essential/03 Docker Lifecycle/#docker-lifecycle","text":"When we run a docker container using run command, docker run image_name Then, the docker run is equivalent to the following 2 commands: docker create docker start With docker create , the file system snapshot of the image is being copied to isolated physical storage . Then with docker start we start the container . Example : Let's do the hands on what we are claiming with a image hello-world . docker create hello-world This will return the id of the created container. Using the id we can now start the docker. docker start -a id This will give us the output Hello from Docker! . Here the -a flag watch out the container output and print it in console .","title":"Docker Lifecycle"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/","text":"Docker Basic Commands Creating and run a container from an image docker run hello-world Here, docker is the docker-client name, run is create and run the container and hello-world is the docker image name. With this command we also run the start-up script. That start-up script is responsible to print the text Hello from Docker! . Override startup or default command Anytime we execute docker run with an image, We took the file snapshot to the physical storage Execute the startup script To override the default startup command, we can pass our command as 4th parameter. docker run busybox echo hi there We will get the output hi there Here, busybox is the image and echo hi there is our over ride start up command. We use busybox image instead of hello-world because busy box has the program like echo . Listing container in local machine Check the currently running container. docker ps If there is no docker container is running the list will be empty. Let's run a container for long time and check it docker ps works. docker run busybox ping google.com This will run a significant time to measure the latency. In the meantime let's run the listing command docker ps Now we should see the busybox is running along with it's other properties. To check all the containers ever been created in the local machine, we can use a flag all . docker pas --all This will output all the containers we been created in local machine. Restart stopped docker container We can restart the docker container in future at any point. To do so, we need the container_id . We can get the container_id by following command docker ps --all In the first column of the table, contains container_id . Now we can start the stopped docker using the following commands docker start -a cee8f8c62478 Here -a watch out the output of the container and print it in the console . Removing stopped containers We can see all the containers we run before by docker ps --all To remove all these containers along with the build file we can use the following docker system prune With docker system prune , we have to re-download all the images we downloaded earlier Retrieve the logs of a container In some scenario, we might need to see the output of the docker container. For example, # get ready the busybox container that will print `Hi there!` docker create busybox echo Hi there! # start the container docker start container_id Now the docker provide the output Hi there , but since we did not use -a flag, the output will not be printed in the console. To see logs, we can use the following commands docker logs container_id This will give us the output Hi there! . docker logs does not re-run or restart the docker container . It just get the logs emitted from the container . Stopping a container To stop a docker container we can use the following method Stop a container by docker stop container_id Kill a container by docker kill container_id When we use the stop command a SIGTERM signal is passed to the primary process of the container. It gives the process 10 seconds to close the process and do the post process, like save files, do logging etc. If the primary process does not close within 10 seconds , it then pass SIGKILL signal that kill the process immediately. The kill command pass SIGKILL signal that kill the process immediately. SIGTERM stands for Terminate Signal . SIGKILL stands for Kill Signal . Multiple commands in container Let's say we need to tweak the redis-cli . In this case, if redis server is installed in my local machine, in one terminal, we can start the redis server . In another terminal, we can start the redis-cli . With docker , if we install the redis , we can start the redis server by the startup command. But in this case, if we go to another terminal window and try to start the redis-cli , we can not access it. Because, from a regular machine terminal window, we can not access the redis server that is running in a isolated namespace. Example : Lets run the redis container by docker run redis This will install the redis server and start the server also. Now If we go to another terminal and try to access redis-cli , we will get an error. Solution : To execute the redis-cli inside the container we need to follow the skeleton, docker exec -it container_id redis-cli This will execute the redis-cli command inside the container and with -it flag, we can pass the command through the terminal and get the emitted output by the container inside our terminal. In this way we can interact with redis server like 127.0.0.1:6379> set myVal 5 127.0.0.1:6379> get MyVal Here the exec stands for Execute . Without -it the redis-cli will run inside the container but we can not interact using host machine terminal. Each process running in the docker or virtual linux environment contains STDIN , STDOUT , STDERR . The STDIN stands for Standard In , STDOUT stands for Standard Output and STDERR stands for Standard Error . -it is combined of two standalone flag. -i stands connect with processes STDIN and -t ensure formatted text. Start a terminal/shell of container context along with docker start May be we do not want to always use the exec command to open a terminal for a container every time we want to execute some command inside the container. To open a terminal inside the container we can use the following command docker exec -it container_id sh Now a terminal inside the container will be appeared and we can execute all the unix command there. For example we can run the previous redis-cli command here and interact with redis-server . sh is program execute inside the container, known as command processor . Similar programs are zsh , powershell , bash etc. Traditionally most of the container has sh program included. If we want to run the terminal inside the container on startup, we can use the following docker run -it image_name sh This will start the container and also start a terminal in the container context. This sh program will prevent other startup command supposed to be run during the container start.","title":"04 Docker Basic Commands"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#docker-basic-commands","text":"","title":"Docker Basic Commands"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#creating-and-run-a-container-from-an-image","text":"docker run hello-world Here, docker is the docker-client name, run is create and run the container and hello-world is the docker image name. With this command we also run the start-up script. That start-up script is responsible to print the text Hello from Docker! .","title":"Creating and run a container from an image"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#override-startup-or-default-command","text":"Anytime we execute docker run with an image, We took the file snapshot to the physical storage Execute the startup script To override the default startup command, we can pass our command as 4th parameter. docker run busybox echo hi there We will get the output hi there Here, busybox is the image and echo hi there is our over ride start up command. We use busybox image instead of hello-world because busy box has the program like echo .","title":"Override startup or default command"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#listing-container-in-local-machine","text":"Check the currently running container. docker ps If there is no docker container is running the list will be empty. Let's run a container for long time and check it docker ps works. docker run busybox ping google.com This will run a significant time to measure the latency. In the meantime let's run the listing command docker ps Now we should see the busybox is running along with it's other properties. To check all the containers ever been created in the local machine, we can use a flag all . docker pas --all This will output all the containers we been created in local machine.","title":"Listing container in local machine"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#restart-stopped-docker-container","text":"We can restart the docker container in future at any point. To do so, we need the container_id . We can get the container_id by following command docker ps --all In the first column of the table, contains container_id . Now we can start the stopped docker using the following commands docker start -a cee8f8c62478 Here -a watch out the output of the container and print it in the console .","title":"Restart stopped docker container"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#removing-stopped-containers","text":"We can see all the containers we run before by docker ps --all To remove all these containers along with the build file we can use the following docker system prune With docker system prune , we have to re-download all the images we downloaded earlier","title":"Removing stopped containers"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#retrieve-the-logs-of-a-container","text":"In some scenario, we might need to see the output of the docker container. For example, # get ready the busybox container that will print `Hi there!` docker create busybox echo Hi there! # start the container docker start container_id Now the docker provide the output Hi there , but since we did not use -a flag, the output will not be printed in the console. To see logs, we can use the following commands docker logs container_id This will give us the output Hi there! . docker logs does not re-run or restart the docker container . It just get the logs emitted from the container .","title":"Retrieve the logs of a container"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#stopping-a-container","text":"To stop a docker container we can use the following method Stop a container by docker stop container_id Kill a container by docker kill container_id When we use the stop command a SIGTERM signal is passed to the primary process of the container. It gives the process 10 seconds to close the process and do the post process, like save files, do logging etc. If the primary process does not close within 10 seconds , it then pass SIGKILL signal that kill the process immediately. The kill command pass SIGKILL signal that kill the process immediately. SIGTERM stands for Terminate Signal . SIGKILL stands for Kill Signal .","title":"Stopping a container"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#multiple-commands-in-container","text":"Let's say we need to tweak the redis-cli . In this case, if redis server is installed in my local machine, in one terminal, we can start the redis server . In another terminal, we can start the redis-cli . With docker , if we install the redis , we can start the redis server by the startup command. But in this case, if we go to another terminal window and try to start the redis-cli , we can not access it. Because, from a regular machine terminal window, we can not access the redis server that is running in a isolated namespace. Example : Lets run the redis container by docker run redis This will install the redis server and start the server also. Now If we go to another terminal and try to access redis-cli , we will get an error. Solution : To execute the redis-cli inside the container we need to follow the skeleton, docker exec -it container_id redis-cli This will execute the redis-cli command inside the container and with -it flag, we can pass the command through the terminal and get the emitted output by the container inside our terminal. In this way we can interact with redis server like 127.0.0.1:6379> set myVal 5 127.0.0.1:6379> get MyVal Here the exec stands for Execute . Without -it the redis-cli will run inside the container but we can not interact using host machine terminal. Each process running in the docker or virtual linux environment contains STDIN , STDOUT , STDERR . The STDIN stands for Standard In , STDOUT stands for Standard Output and STDERR stands for Standard Error . -it is combined of two standalone flag. -i stands connect with processes STDIN and -t ensure formatted text.","title":"Multiple commands in container"},{"location":"Notes/01 Docker Essential/04 Docker Basic Commands/#start-a-terminalshell-of-container-context-along-with-docker-start","text":"May be we do not want to always use the exec command to open a terminal for a container every time we want to execute some command inside the container. To open a terminal inside the container we can use the following command docker exec -it container_id sh Now a terminal inside the container will be appeared and we can execute all the unix command there. For example we can run the previous redis-cli command here and interact with redis-server . sh is program execute inside the container, known as command processor . Similar programs are zsh , powershell , bash etc. Traditionally most of the container has sh program included. If we want to run the terminal inside the container on startup, we can use the following docker run -it image_name sh This will start the container and also start a terminal in the container context. This sh program will prevent other startup command supposed to be run during the container start.","title":"Start a terminal/shell of container context along with docker start"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/","text":"Create Custom Image Throughout the previous notes, we been using images created by other engineers. To create our own image, we can do the followings, Create a dockerfile . Once a dockerfile is created, we will pass it to the docker client This docker client will provide the image to the docker server The docker-server will make the image , that we can use A dockerfile is a plain text file. It contains all the configs along with the startup commands to define how a docker container should behave. It determine what programs should be inside the container and how will these program behave during container star up. Docker server does most of heavy lifting while creating a image . It takes the docker file, go through the configuration and build the useable image For each docker file , there's always a patter A base image Some config to install additional programs, dependencies that is required to create and execute the container A start up command, that will be executed on the moment a container start Hands On Let's create our own docker image that will run a redis-server . We will follow the following procedures Define base image Download and install the dependencies Instruct the images initial behaviour So, First create a file named Dockerfile bash touch Dockerfile In the Dockerfile , add a base image bash FROM alpine Download the dependency bash RUN apk add --update redis Define the initial command bash CMD [\"redis-server\"] Finally our Dockerfile should be the following FROM alpine RUN apk add --update redis CMD [\"redis-server\"] Now let's build the container docker build . This will create the image and return the image_id . Now we can run the container from the image_id using the followings, docker run image_id This will run the redis server. Dockerfile is a plain file with no extension Dockerfile Teardown We just going through the process of creating a docker image. But we don't explain what really happen there. Now lets explain what actually we are done inside the Dockerfile configuration. We ran 3 commands to build the image, and all three has a very similar pattern. Each command like FROM , RUN , CMD are the docker instruction and they took some arguments . The FROM instruction specify a docker image we want to use as base. While we are preparing our custom image, the RUN execute some commands. The CMD specify, what should run on startup when our image will be used to create a new container . Every line of configuration we are going to add inside the Dockerfile will always start with a instruction A base image is an initial set of programs that can be used to to further customize the the image. Alpine is a base image that comes with a package manager named apk (Apache Package Manager). apk can be used to download the redis-server and install in the system. With FROM alpine we are using an initial operating system named alpine . This alpine operating system has couple of preinstalled program, that are very much useful for what we are trying to accomplish. Since we are here to create redis server and we need to install some dependencies, alpine has these tools and programs preinstalled like apk . With RUN apk add --update redis we download and install the redis server. Here apk is nothing related to the docker . It's a dependency manager preinstalled in the base image , download and install the redis server. The CMD [\"redis-server\"] ensure, when we create a container from this docker image , the redis server will be started using redis-server command. Image build process To build a image from the Dockerfile , we use docker build . This ship our Dockerfile to the docker client . build is responsible for take the Dockerfile and build an image out of it. The . is the build context. The build context contains all the set of files and folders that belongs to our project needs to wrap or encapsulate in our docker container . If we notice the logs of the docker build . , we can see except the first instruction FROM alpine , every other instruction has an intermediary container that being removed automatically. This means, except first step, each step took the container from the previous step and step itself acts as the startup instruction. Then when step is done, it simply pass the updated file system snapshot and remove the temporary container from itself. For the last step, it took the container from the previous step, and do not run itself as the start-up command. Instead it only set itself as the first instruction and ensure if someone in future create and run the container out of the image, it then execute this last step as the startup instruction . For each step, except first one, we take the image from the previous step, create a temporary container, execute instructions, make changes, took the snapshot of the file system and return the file system output as output, so it can be used as the image for the next step. For last step, the final instruction is considered as the start-up instruction of the docker container . Rebuild image from cache Let's update the Dockerfile with an additional command RUN apk add --update gcc Now the Dockerfile should be like FROM alpine RUN apk add --update redis RUN apk add --update gcc CMD [\"redis-server\"] Let's build a image out of this Dockerfile , docker build . If we observer the logs closely, we see, in the second step, it does not create a intermediary container from the previous step. Instead it is using the cache. So we do not need to install the redis-server multiple times. This gives the docker robustness and faster build performance. From the instruction RUN apk add --update gcc it will create the intermediary container and since the file snapshot is being changed, it will do the same from the next steps. If we build the image from the same Dockerfile for the 3rd time, it will take all the changes from the cache instead of the intermediary container . An intermediary container is created from the previous step image and use to run current instruction, make changes, take the snapshot of the new changed file system and got removed. This snapshot is being used for the next step to create another intermediary container and goes on. Altering the instruction sequence will not use the cache. Tagging a docker image Till now, we have created image from the Dockerfile and getting an image_id . When we want a name instead of an image_id we can use the tagging . To get a name, after a image being created, we can use the following docker build -t user_name/image_name:version_number . This will return Successfully tagged user_name/image_name:version_number Here the user_name is the username , that is used to login to the docker-hub . image_name is our desired image name. The version_number is the image version number. Instead of version_number we can use lated keyword to use the latest version number handled by the docker itself. We can now run the docker with the tag docker run user_name/image_name:version_number Here only the version_number is the tag itself. The user_name itself is always the docker id and image_name is the project name or the repo name . While running our tagged custom image we can ignore the version_number . It will simply take the latest version.","title":"05 Create Custom Image"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/#create-custom-image","text":"Throughout the previous notes, we been using images created by other engineers. To create our own image, we can do the followings, Create a dockerfile . Once a dockerfile is created, we will pass it to the docker client This docker client will provide the image to the docker server The docker-server will make the image , that we can use A dockerfile is a plain text file. It contains all the configs along with the startup commands to define how a docker container should behave. It determine what programs should be inside the container and how will these program behave during container star up. Docker server does most of heavy lifting while creating a image . It takes the docker file, go through the configuration and build the useable image For each docker file , there's always a patter A base image Some config to install additional programs, dependencies that is required to create and execute the container A start up command, that will be executed on the moment a container start","title":"Create Custom Image"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/#hands-on","text":"Let's create our own docker image that will run a redis-server . We will follow the following procedures Define base image Download and install the dependencies Instruct the images initial behaviour So, First create a file named Dockerfile bash touch Dockerfile In the Dockerfile , add a base image bash FROM alpine Download the dependency bash RUN apk add --update redis Define the initial command bash CMD [\"redis-server\"] Finally our Dockerfile should be the following FROM alpine RUN apk add --update redis CMD [\"redis-server\"] Now let's build the container docker build . This will create the image and return the image_id . Now we can run the container from the image_id using the followings, docker run image_id This will run the redis server. Dockerfile is a plain file with no extension","title":"Hands On"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/#dockerfile-teardown","text":"We just going through the process of creating a docker image. But we don't explain what really happen there. Now lets explain what actually we are done inside the Dockerfile configuration. We ran 3 commands to build the image, and all three has a very similar pattern. Each command like FROM , RUN , CMD are the docker instruction and they took some arguments . The FROM instruction specify a docker image we want to use as base. While we are preparing our custom image, the RUN execute some commands. The CMD specify, what should run on startup when our image will be used to create a new container . Every line of configuration we are going to add inside the Dockerfile will always start with a instruction A base image is an initial set of programs that can be used to to further customize the the image. Alpine is a base image that comes with a package manager named apk (Apache Package Manager). apk can be used to download the redis-server and install in the system. With FROM alpine we are using an initial operating system named alpine . This alpine operating system has couple of preinstalled program, that are very much useful for what we are trying to accomplish. Since we are here to create redis server and we need to install some dependencies, alpine has these tools and programs preinstalled like apk . With RUN apk add --update redis we download and install the redis server. Here apk is nothing related to the docker . It's a dependency manager preinstalled in the base image , download and install the redis server. The CMD [\"redis-server\"] ensure, when we create a container from this docker image , the redis server will be started using redis-server command.","title":"Dockerfile Teardown"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/#image-build-process","text":"To build a image from the Dockerfile , we use docker build . This ship our Dockerfile to the docker client . build is responsible for take the Dockerfile and build an image out of it. The . is the build context. The build context contains all the set of files and folders that belongs to our project needs to wrap or encapsulate in our docker container . If we notice the logs of the docker build . , we can see except the first instruction FROM alpine , every other instruction has an intermediary container that being removed automatically. This means, except first step, each step took the container from the previous step and step itself acts as the startup instruction. Then when step is done, it simply pass the updated file system snapshot and remove the temporary container from itself. For the last step, it took the container from the previous step, and do not run itself as the start-up command. Instead it only set itself as the first instruction and ensure if someone in future create and run the container out of the image, it then execute this last step as the startup instruction . For each step, except first one, we take the image from the previous step, create a temporary container, execute instructions, make changes, took the snapshot of the file system and return the file system output as output, so it can be used as the image for the next step. For last step, the final instruction is considered as the start-up instruction of the docker container .","title":"Image build process"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/#rebuild-image-from-cache","text":"Let's update the Dockerfile with an additional command RUN apk add --update gcc Now the Dockerfile should be like FROM alpine RUN apk add --update redis RUN apk add --update gcc CMD [\"redis-server\"] Let's build a image out of this Dockerfile , docker build . If we observer the logs closely, we see, in the second step, it does not create a intermediary container from the previous step. Instead it is using the cache. So we do not need to install the redis-server multiple times. This gives the docker robustness and faster build performance. From the instruction RUN apk add --update gcc it will create the intermediary container and since the file snapshot is being changed, it will do the same from the next steps. If we build the image from the same Dockerfile for the 3rd time, it will take all the changes from the cache instead of the intermediary container . An intermediary container is created from the previous step image and use to run current instruction, make changes, take the snapshot of the new changed file system and got removed. This snapshot is being used for the next step to create another intermediary container and goes on. Altering the instruction sequence will not use the cache.","title":"Rebuild image from cache"},{"location":"Notes/01 Docker Essential/05 Create Custom Image/#tagging-a-docker-image","text":"Till now, we have created image from the Dockerfile and getting an image_id . When we want a name instead of an image_id we can use the tagging . To get a name, after a image being created, we can use the following docker build -t user_name/image_name:version_number . This will return Successfully tagged user_name/image_name:version_number Here the user_name is the username , that is used to login to the docker-hub . image_name is our desired image name. The version_number is the image version number. Instead of version_number we can use lated keyword to use the latest version number handled by the docker itself. We can now run the docker with the tag docker run user_name/image_name:version_number Here only the version_number is the tag itself. The user_name itself is always the docker id and image_name is the project name or the repo name . While running our tagged custom image we can ignore the version_number . It will simply take the latest version.","title":"Tagging a docker image"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/","text":"Wrap Web App in Docker Let's create a tiny node.js web application and wrap it inside the docker container . Then we should access the app from the browser of the local machine. Right now we will not be worry about deploying the app. Create a Node.js app First create a directory, enter into it and run npm init -y Install express.js package by npm i express Now create a file named index.js and create a server. index.js should be like following const express = require('express'); const app = express(); // from browser, a get request in `/` route, should return `Hi There!!` app.get('/', (req, res) => res.send('Hi There!!')); // app will be run on port `8080` app.listen(8080); Time to create a Dockerfile . Before we create the Dockerfile , we need to consider some concept and instructions. Selecting A Base Image A base image contains is a collection of preinstalled programs. In the docker hub , if we look for the node image, we should find images with different version along with alpine tag. This node images has the node and npm installed. alpine is being used in the docker world to signify an image as small and compact as possible. docker hub is a repository of docker images. We can select a base image of node with the command. FROM node:alpine Load Application Files After building a container, we can stat the image. As soon as the container boots up, file snapshot of the image is copied to the namespace area. And this file snapshot is the base image file snapshot , does not contains our working directory, like package.json or index.js . We need to use an instruction to take our files to the container file systems. To copy our local files to the container file system, we need to use COPY instruction. It takes two arguments, first one is the path of local file system path. The second one is the container file path. COPY ./ ./ Port Mapping When we run a web application in the container tha port in the container can not be accessed from the local machine by default. We need to do a port mapping that will define which port from the local server direct the traffic to the container port. So while we start the container we have to map the port . docker run -p local_machine_port:container_app_port image_id Create A Working Directory By default while we copy the file from the local machine to the container , in container the files persist on the root directory . We might want to put the project files in the separate directory. We can define our working directory by WORKDIR instruction. This WORKDIR will create if not exist and use as the project directory when we copy the files from the local file system to the container . WORKDIR /usr/app Avoid Unnecessary Builds We need to be careful, for each file changes, we should not reinstall the packages. We can copy the package.json file before the package installation . Then we will copy the all other files. In this case, for random files changes, the package installation will be taken from the cache. Example COPY ./package.json ./ RUN npm install COPY ./ ./ Final Docker File With all these consideration, create a Dockerfile in the project root directory. our Dockerfile should be, FROM node:alpine WORKDIR /usr/app COPY ./package.json ./ RUN npm install COPY ./ ./ CMD [\"node\", \"index.js\"] Let's build a image from this Dockerfile , docker build -t bmshamsnahid/myapp . After build the image, let run it with the tag , # Import the base image FROM node:alpine # Create a working directory for the app WORKDIR /usr/app # Only if we change the `package.json` file, the npm will install all the modules along with the new one COPY ./package.json ./ RUN npm install # Copy all the project files to the `container` COPY ./ ./ # Set the start up command to run the server CMD [\"node\", \"index.js\"] In the web browser http://localhost:5000/ should print the following Hi There!! Congratulations!!! You just ran a node application in docker container.","title":"06 Wrap Web App in Docker"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#wrap-web-app-in-docker","text":"Let's create a tiny node.js web application and wrap it inside the docker container . Then we should access the app from the browser of the local machine. Right now we will not be worry about deploying the app.","title":"Wrap Web App in Docker"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#create-a-nodejs-app","text":"First create a directory, enter into it and run npm init -y Install express.js package by npm i express Now create a file named index.js and create a server. index.js should be like following const express = require('express'); const app = express(); // from browser, a get request in `/` route, should return `Hi There!!` app.get('/', (req, res) => res.send('Hi There!!')); // app will be run on port `8080` app.listen(8080); Time to create a Dockerfile . Before we create the Dockerfile , we need to consider some concept and instructions.","title":"Create a Node.js app"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#selecting-a-base-image","text":"A base image contains is a collection of preinstalled programs. In the docker hub , if we look for the node image, we should find images with different version along with alpine tag. This node images has the node and npm installed. alpine is being used in the docker world to signify an image as small and compact as possible. docker hub is a repository of docker images. We can select a base image of node with the command. FROM node:alpine","title":"Selecting A Base Image"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#load-application-files","text":"After building a container, we can stat the image. As soon as the container boots up, file snapshot of the image is copied to the namespace area. And this file snapshot is the base image file snapshot , does not contains our working directory, like package.json or index.js . We need to use an instruction to take our files to the container file systems. To copy our local files to the container file system, we need to use COPY instruction. It takes two arguments, first one is the path of local file system path. The second one is the container file path. COPY ./ ./","title":"Load Application Files"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#port-mapping","text":"When we run a web application in the container tha port in the container can not be accessed from the local machine by default. We need to do a port mapping that will define which port from the local server direct the traffic to the container port. So while we start the container we have to map the port . docker run -p local_machine_port:container_app_port image_id","title":"Port Mapping"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#create-a-working-directory","text":"By default while we copy the file from the local machine to the container , in container the files persist on the root directory . We might want to put the project files in the separate directory. We can define our working directory by WORKDIR instruction. This WORKDIR will create if not exist and use as the project directory when we copy the files from the local file system to the container . WORKDIR /usr/app","title":"Create A Working Directory"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#avoid-unnecessary-builds","text":"We need to be careful, for each file changes, we should not reinstall the packages. We can copy the package.json file before the package installation . Then we will copy the all other files. In this case, for random files changes, the package installation will be taken from the cache. Example COPY ./package.json ./ RUN npm install COPY ./ ./","title":"Avoid Unnecessary Builds"},{"location":"Notes/01 Docker Essential/06 Wrap Web App in Docker/#final-docker-file","text":"With all these consideration, create a Dockerfile in the project root directory. our Dockerfile should be, FROM node:alpine WORKDIR /usr/app COPY ./package.json ./ RUN npm install COPY ./ ./ CMD [\"node\", \"index.js\"] Let's build a image from this Dockerfile , docker build -t bmshamsnahid/myapp . After build the image, let run it with the tag , # Import the base image FROM node:alpine # Create a working directory for the app WORKDIR /usr/app # Only if we change the `package.json` file, the npm will install all the modules along with the new one COPY ./package.json ./ RUN npm install # Copy all the project files to the `container` COPY ./ ./ # Set the start up command to run the server CMD [\"node\", \"index.js\"] In the web browser http://localhost:5000/ should print the following Hi There!! Congratulations!!! You just ran a node application in docker container.","title":"Final Docker File"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/","text":"Using Docker Compose: Managing Multiple Local Containers Docker Compose docker-compose helps us to avoid repetitive commands, that we might have to write with docker-cli during a container start up. For example, if we have two container and need a networking between them, we have to configure these with docker-cli every time we start the container . Using docker-compose we can resolve the issue. This docker-compose allows us to start up multiple container at the same time in a very easy and straightforward way. Also it will set up some sort of networking between them and all behind the scene. To make use of docker-compose , we essentially going to the docker-cli startup commands of long form and encode these command in docker-compose.yml file. We do not entirely copy and paste the start-up commands, instead we use special syntax more or less similar to the start up commands. After creating the docker-compose.yml file, we will feed the file to the docker-compose-cli to parse the file and create container with our desired configurations. In windows and mac OS the docker-compose is shipped with the docker installation. But for linux machines, you might need to install the docker-compose library separately. To install the docker-compose in Ubuntu 20.04 , that I am using, can follow the instructions sudo curl -L \"https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo mv /usr/local/bin/docker-compose /usr/bin/docker-compose sudo chmod +x /usr/bin/docker-compose First instruction download the library, second one move it to /usr/bin/docker-compose and using the third instruction, we are giving the docker-compose appropriate permissions. A Hands On Let's develop a classic docker example. Here we will create a little docker container that will host a web application. This web application will count the number of visit to that web app. We will need a node app that will response on the HTTP request and a redis server that will count the number of visit. Although redis is a in memory server, in this case we will consider itself as our tiny database. Off course we can use the node server to store the number of visits. To make the container a little bit complex we are using both a node server and a redis server. We can consider a single container with both node server and redis server in it. But this will create problem on scalability. For more traffic, if we increase the number of containers, for each container, there will be individual node server and redis server . Also each redis server will be isolated from each others. So one redis server will give us total visit of 10 , another redis server will give us total visit of 5 . So our actual approach will be both node server and redis server will be in isolated container. An in case of scaling we will scale the node-app-container and all the node-app-container will be connected to the single redis-server-container . Creating The Node Server Create a project directory named visits , mkdir visits Now go to the directory and create a node project cd visits yarn init -y Create a file index.js touch index.js The index.js will be responsible for creating the node server and connect with the redis server to display the number of site visits in the browser on response of a HTTP request. The code of the index.js will be like followings, // import required modules const express = require('express'); const redis = require('redis'); const app = express(); // create app instance const client = redis.createClient({ host: 'redis-server', // service name of the `redis-server` we are using, will be defined in the `services` section of `docker-compose.yml` file port: 6379 // default port of the `redis-server` }); // connect the node server with redis server client.set('visits', 0); // initially set number of visits to 0 app.get('/', (req, res) => { client.get('visits', (err, visits) => { res.send(`Number of visits: ${visits}`); // in browser, showing the client, number of visits client.set('visits', parseInt(visits) + 1); // increase the number visits }); }); const PORT = 8081; // determine node server port no // run the server app.listen(PORT, () => console.log(`App is listening on port: ${PORT}`)); Except the redis connection on line const client = redis.createClient(); Here we will have to put necessary networking config of the redis server. Assembling Dockerfile to Node Server Our Dockerfile will be very simple to just run the node server # define base image FROM node:alpine # define working directory inside the container WORKDIR /app # Copy the package.json file to the project directory COPY package.json . # install all the dependencies RUN npm install # Copy all the source code from host machine to the container project directory COPY . . # define the start up command of the container to run the server CMD [\"node\", \"index.js\"] Now let's build the image, docker build -t docker_user_id/repo_name:latest . This will create the image of our node-app named docker_user_id/repo_name . Now if we try to run the node-app (Although it will throw error, because redis server is not running yet), docker run docker_user_id/repo_name Here, we will get an error message, ode:events:356 throw er; // Unhandled 'error' event ^ Error: connect ECONNREFUSED 127.0.0.1:6379 at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1139:16) Emitted 'error' event on RedisClient instance at: at RedisClient.on_error (/app/node_modules/redis/index.js:406:14) at Socket.<anonymous> (/app/node_modules/redis/index.js:279:14) at Socket.emit (node:events:379:20) at emitErrorNT (node:internal/streams/destroy:188:8) at emitErrorCloseNT (node:internal/streams/destroy:153:3) at processTicksAndRejections (node:internal/process/task_queues:81:21) { errno: -111, code: 'ECONNREFUSED', syscall: 'connect', address: '127.0.0.1', port: 6379 } In summary, it says, the node-app can not connect to the redis-server , as expected. We will fix it now. Redis Server We can use vanilla redis image from docker-hub . We will simply run the redis-server by docker run redis Even with running the redis-server , if we run the node-app again, we will get the same error as before. Since both node-app and redis-server is in isolated container and there is no networking communication between them, the node-app will not be able to communicate with the redis-server . Bring The Docker Compose With docker-compose first we do the following configurations: For redis-server make use of the redis image For node-app make use of the Dockerfile Also for the node-app map port 8081 from local machine port 4001 By defining multiple services in the docker-compose , docker will put all the services essentially the same network. And as a result the containers can access each other freely. There are different versions of docker-compose . Here we will use version 3 as our docker-version . In the docker-compose the redis-server and the node-app are considered as services . For a service in the docker-compose , we have to define how we get the image. It could be an image from the docker-hub or from the Dockerfile we wrote. For redis-server we will use docker-hub image and for the node-app we will use our made up Dockerfile For a service we can do the port mapping between local machine and container. For a service we can use a restart policy, discussed in details in the maintenance section. To do so, in the project directory, first create the docker-compose.yml file touch docker-compose.yml Now our docker-compose.yml file be version: '3' services: redis-server: image: 'redis' node-app: restart: always build: . ports: - '4001:8081' We used to run the container by docker run my_image_name , which is similar to docker-compose up . We used 2 commands, one for build the image and another for run the container. The docker-compose up --build is similar to the followings existing commands, docker build . docker run my_image_name So to build and run our two docker image we can use the followings, docker-compose up --build This will Run container for redis-server Build image for node-app Run container for node-app Put both container in same network Start the redis-server container Start the node-app container In output we should see App is listening on port: 8081 Since we map port 4001 from local machine to 8081 of the host machine, from browser, we can access the node-app by http://localhost:4001/ . If we go the browser http://localhost:4001/ , we should see Number of Visits: visit_count Stop Containers with Docker Compose With docker-cli we used to run a container background using docker run -d image_id To stop the instance, we used docker stop image_id With docker-compose , to run the containers in background, we can use the following command, docker-compose up -d Also to stop all the containers using the docker-compose we can use the followings, docker-compose down We can verify if the container being stopped or not by docker ps . Container Maintenance with Docker Compose It is possible that, out node-server may crash or hang over time. In this case, we might want to restart the server. In docker-compose there are 4 different restart policy, no : Never restart the container, no matter what happen. This is default restart policy. always : If the container stops for any reason, the docker-compose will restart the server. on-failure : Only restart the container, if it crashes with an error-code . unless-stopped : Always restart the container on crash except developer forcibly stop it. We need to put this restart-policy under the service declaration. In node.js we can exit from the app with process.exit(exit_code) . As exit code, we have 0 , means everything is okay and we want to exit from the node application non-zero , any value other than 0 means, there's something wrong and the error-code specifies that issue. The restart-policy of always work on when it encounters the 0 as error-code . If we use restart-policy as on-failure , we have to use error-code other than zero. Finally, If we use unless-stopped as restart-policy , then the container will always restart, unless we ( develop ) stop in from the terminal . As restart-policy , if we use no then we have to put no inside a quote. Because in yml file, no is interpreted as false . For other restart-policy like, always , on-failure or unless-stopped we can use plain text without the quote . Between, always and on-failure use cases, we might 100% time want a public web server restarted on crash. In this case, we use always . If we do some worker process, that is meant to do some specific job and then exit, then thats a good case to use on-failure exit policy. Checking Container Status With Docker Compose Traditionally, we used to check the container status using the docker-cli by docker ps Docker Compose has a similar command, docker-compose ps This command should be executed inside the project directory, where the docker-compose.yml file exist. So, only the containers defined inside the docker-compose status will be displayed.","title":"07 Using Docker Compose"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#using-docker-compose-managing-multiple-local-containers","text":"","title":"Using Docker Compose: Managing Multiple Local Containers"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#docker-compose","text":"docker-compose helps us to avoid repetitive commands, that we might have to write with docker-cli during a container start up. For example, if we have two container and need a networking between them, we have to configure these with docker-cli every time we start the container . Using docker-compose we can resolve the issue. This docker-compose allows us to start up multiple container at the same time in a very easy and straightforward way. Also it will set up some sort of networking between them and all behind the scene. To make use of docker-compose , we essentially going to the docker-cli startup commands of long form and encode these command in docker-compose.yml file. We do not entirely copy and paste the start-up commands, instead we use special syntax more or less similar to the start up commands. After creating the docker-compose.yml file, we will feed the file to the docker-compose-cli to parse the file and create container with our desired configurations. In windows and mac OS the docker-compose is shipped with the docker installation. But for linux machines, you might need to install the docker-compose library separately. To install the docker-compose in Ubuntu 20.04 , that I am using, can follow the instructions sudo curl -L \"https://github.com/docker/compose/releases/download/1.26.0/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose sudo mv /usr/local/bin/docker-compose /usr/bin/docker-compose sudo chmod +x /usr/bin/docker-compose First instruction download the library, second one move it to /usr/bin/docker-compose and using the third instruction, we are giving the docker-compose appropriate permissions.","title":"Docker Compose"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#a-hands-on","text":"Let's develop a classic docker example. Here we will create a little docker container that will host a web application. This web application will count the number of visit to that web app. We will need a node app that will response on the HTTP request and a redis server that will count the number of visit. Although redis is a in memory server, in this case we will consider itself as our tiny database. Off course we can use the node server to store the number of visits. To make the container a little bit complex we are using both a node server and a redis server. We can consider a single container with both node server and redis server in it. But this will create problem on scalability. For more traffic, if we increase the number of containers, for each container, there will be individual node server and redis server . Also each redis server will be isolated from each others. So one redis server will give us total visit of 10 , another redis server will give us total visit of 5 . So our actual approach will be both node server and redis server will be in isolated container. An in case of scaling we will scale the node-app-container and all the node-app-container will be connected to the single redis-server-container .","title":"A Hands On"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#creating-the-node-server","text":"Create a project directory named visits , mkdir visits Now go to the directory and create a node project cd visits yarn init -y Create a file index.js touch index.js The index.js will be responsible for creating the node server and connect with the redis server to display the number of site visits in the browser on response of a HTTP request. The code of the index.js will be like followings, // import required modules const express = require('express'); const redis = require('redis'); const app = express(); // create app instance const client = redis.createClient({ host: 'redis-server', // service name of the `redis-server` we are using, will be defined in the `services` section of `docker-compose.yml` file port: 6379 // default port of the `redis-server` }); // connect the node server with redis server client.set('visits', 0); // initially set number of visits to 0 app.get('/', (req, res) => { client.get('visits', (err, visits) => { res.send(`Number of visits: ${visits}`); // in browser, showing the client, number of visits client.set('visits', parseInt(visits) + 1); // increase the number visits }); }); const PORT = 8081; // determine node server port no // run the server app.listen(PORT, () => console.log(`App is listening on port: ${PORT}`)); Except the redis connection on line const client = redis.createClient(); Here we will have to put necessary networking config of the redis server.","title":"Creating The Node Server"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#assembling-dockerfile-to-node-server","text":"Our Dockerfile will be very simple to just run the node server # define base image FROM node:alpine # define working directory inside the container WORKDIR /app # Copy the package.json file to the project directory COPY package.json . # install all the dependencies RUN npm install # Copy all the source code from host machine to the container project directory COPY . . # define the start up command of the container to run the server CMD [\"node\", \"index.js\"] Now let's build the image, docker build -t docker_user_id/repo_name:latest . This will create the image of our node-app named docker_user_id/repo_name . Now if we try to run the node-app (Although it will throw error, because redis server is not running yet), docker run docker_user_id/repo_name Here, we will get an error message, ode:events:356 throw er; // Unhandled 'error' event ^ Error: connect ECONNREFUSED 127.0.0.1:6379 at TCPConnectWrap.afterConnect [as oncomplete] (node:net:1139:16) Emitted 'error' event on RedisClient instance at: at RedisClient.on_error (/app/node_modules/redis/index.js:406:14) at Socket.<anonymous> (/app/node_modules/redis/index.js:279:14) at Socket.emit (node:events:379:20) at emitErrorNT (node:internal/streams/destroy:188:8) at emitErrorCloseNT (node:internal/streams/destroy:153:3) at processTicksAndRejections (node:internal/process/task_queues:81:21) { errno: -111, code: 'ECONNREFUSED', syscall: 'connect', address: '127.0.0.1', port: 6379 } In summary, it says, the node-app can not connect to the redis-server , as expected. We will fix it now.","title":"Assembling Dockerfile to Node Server"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#redis-server","text":"We can use vanilla redis image from docker-hub . We will simply run the redis-server by docker run redis Even with running the redis-server , if we run the node-app again, we will get the same error as before. Since both node-app and redis-server is in isolated container and there is no networking communication between them, the node-app will not be able to communicate with the redis-server .","title":"Redis Server"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#bring-the-docker-compose","text":"With docker-compose first we do the following configurations: For redis-server make use of the redis image For node-app make use of the Dockerfile Also for the node-app map port 8081 from local machine port 4001 By defining multiple services in the docker-compose , docker will put all the services essentially the same network. And as a result the containers can access each other freely. There are different versions of docker-compose . Here we will use version 3 as our docker-version . In the docker-compose the redis-server and the node-app are considered as services . For a service in the docker-compose , we have to define how we get the image. It could be an image from the docker-hub or from the Dockerfile we wrote. For redis-server we will use docker-hub image and for the node-app we will use our made up Dockerfile For a service we can do the port mapping between local machine and container. For a service we can use a restart policy, discussed in details in the maintenance section. To do so, in the project directory, first create the docker-compose.yml file touch docker-compose.yml Now our docker-compose.yml file be version: '3' services: redis-server: image: 'redis' node-app: restart: always build: . ports: - '4001:8081' We used to run the container by docker run my_image_name , which is similar to docker-compose up . We used 2 commands, one for build the image and another for run the container. The docker-compose up --build is similar to the followings existing commands, docker build . docker run my_image_name So to build and run our two docker image we can use the followings, docker-compose up --build This will Run container for redis-server Build image for node-app Run container for node-app Put both container in same network Start the redis-server container Start the node-app container In output we should see App is listening on port: 8081 Since we map port 4001 from local machine to 8081 of the host machine, from browser, we can access the node-app by http://localhost:4001/ . If we go the browser http://localhost:4001/ , we should see Number of Visits: visit_count","title":"Bring The Docker Compose"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#stop-containers-with-docker-compose","text":"With docker-cli we used to run a container background using docker run -d image_id To stop the instance, we used docker stop image_id With docker-compose , to run the containers in background, we can use the following command, docker-compose up -d Also to stop all the containers using the docker-compose we can use the followings, docker-compose down We can verify if the container being stopped or not by docker ps .","title":"Stop Containers with Docker Compose"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#container-maintenance-with-docker-compose","text":"It is possible that, out node-server may crash or hang over time. In this case, we might want to restart the server. In docker-compose there are 4 different restart policy, no : Never restart the container, no matter what happen. This is default restart policy. always : If the container stops for any reason, the docker-compose will restart the server. on-failure : Only restart the container, if it crashes with an error-code . unless-stopped : Always restart the container on crash except developer forcibly stop it. We need to put this restart-policy under the service declaration. In node.js we can exit from the app with process.exit(exit_code) . As exit code, we have 0 , means everything is okay and we want to exit from the node application non-zero , any value other than 0 means, there's something wrong and the error-code specifies that issue. The restart-policy of always work on when it encounters the 0 as error-code . If we use restart-policy as on-failure , we have to use error-code other than zero. Finally, If we use unless-stopped as restart-policy , then the container will always restart, unless we ( develop ) stop in from the terminal . As restart-policy , if we use no then we have to put no inside a quote. Because in yml file, no is interpreted as false . For other restart-policy like, always , on-failure or unless-stopped we can use plain text without the quote . Between, always and on-failure use cases, we might 100% time want a public web server restarted on crash. In this case, we use always . If we do some worker process, that is meant to do some specific job and then exit, then thats a good case to use on-failure exit policy.","title":"Container Maintenance with Docker Compose"},{"location":"Notes/01 Docker Essential/07 Using Docker Compose/#checking-container-status-with-docker-compose","text":"Traditionally, we used to check the container status using the docker-cli by docker ps Docker Compose has a similar command, docker-compose ps This command should be executed inside the project directory, where the docker-compose.yml file exist. So, only the containers defined inside the docker-compose status will be displayed.","title":"Checking Container Status With Docker Compose"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/","text":"Single Container Workflow Let's use docker in a production type environment. The important thing to keep in mind, we first explain the workflow without docker. Instead we discuss outside services that we are going to use to set up this development workflow. Once we get the bird view of the workflow and get the core design behind the workflow, then we introduce the docker and find how docker can facilitate everything. We will create an application that will use docker and eventually push the application to AWS . Our workflow is going to be Develop Testing Deployment Also for any changes, we will repeat the workflow again. Development Phase : Our dev workflow is starting by creating a git repository. This git repository is going to be the center of coordination of all the code we will write. Out git repository will have two types of branch, master and feature branch. We make changes on the feature branch. By filing a PR we will merge the feature branch to master branch. When we do the PR there will be a series of actions, defining how we govern the codebase. The master branch will contain the very clean copy of our code base. Test Phase : As soon as we make the PR, the Travis CI will pull the new and updated code and run the test. If all the tests executed successfully, then we will merge the code to the master branch. Production Phase : After merging the feature branch, We then again push the code to Travis CI and run tests of the code. Any changes on the master branch will eventually and automatically be hosted in the AWS Beanstalk . Now we need to find out how docker fits in this place. To execute this workflow, we do not need to make use of Docker . But using docker will make the workflow lot lot easier. Thats the soul purpose of docker. It's not necessarily a requirement, it just make developer life easier. Docker is not the focus here, it's more of using these 3rd party services (like, Github, Travis CI, AWS Beanstalk) with docker. Generating a Project We will use a react for simplicity. To create a react project, make sure node.js is installed in your system. Then create the project named frontend by the followings, npm create-react-app frontend This will create the react project and install all the necessary dependencies. Now go to the project directory, cd frontend To run the test in local machine, we can use npm run test To build the application for future deployment, we can build using npm run build An finally to run the application in the local environment, we can use npm run start Generating Dev Dockerfile First go to the project directory. Here we will create a Dockerfile named Dockerfile.dev . The purpose of using .dev with the Dockerfile is to make clear that, this docker file is only using in the development environment. In future, we will use another Dockerfile with simply name Dockerfile , without any extension for our production environment. So, lets create the Dockerfile.dev in the project root directory, touch Dockerfile.dev Inside the Dockerfile.dev we will write configuration to make our image. Our Dockerfile.dev will be similar like the following. # Define base image FROM node:alpine # Define working directory inside the container WORKDIR /app # Copy the package.json file to the project directory COPY package.json . # Install the dependencies RUN npm install # Copy all the source code from host machine to the container project directory COPY . ./ # Start up command of the container to run the server CMD [\"npm\", \"run\", \"start\"] Building Image From Dockerfile Since we are not using default name of Dockerfile , instead we are using Dockerfile.dev we have to specify the name during building image. To do so, we can use -f flag, that helps to specify the docker file name. This time, during development, our build instruction will be, docker build -f Dockerfile.dev . When we initially create our react application using create-react-app , the create-react-app automatically install all the node modules inside the node_modules directory. Now, when we are building the image out of the app, we will again installing the dependencies using RUN npm install instruction in the Dockerfile.dev file. We do not need the node_modules in the host machine, it usually increase the duration of image building process, since we are running the app inside the container . So we can delete the node_modules directory from the host machine . node_modules directory is not necessary in the host machine project directory, it increase the time to build the image. We can delete node_modules from project directory. If we again run the previous instruction to build the image, we will get much faster output, docker build -f Dockerfile.dev . Run Container in Interactive Mode Now we can run a container from the image by docker run -it -p 3000:3000 IMAGE_ID If we observe closely, we can notice, while we made changes in the codebase, the hot reloading is not working. The image was build on the snapshot of files. One way is after each changes in the code base we will rebuild the image. But this is a costly and inefficient approach. So we need to find a way to enable hot reload inside inside the image. Enabling Hot Reload Using Docker Volume In the last section, we made changes in a file and the changes was not reflected in the container. Now we will figure out a way to solve the issue without stopping, rebuild and restarting the container. A docker volume essentially a mapping of directory between host machine and container. With docker volume, we can use the reference of local machine directory from the host machine. In this case, we do not copy the directory of the local machine in the container, instead, use the host machine directory by reference from container. To use the volume mapping we need to use the followings, docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app image_id Here we have used two switches. Each switch has a -v flag which is used to set up a volume. The ${pwd} is stands for print working directory , used to get the current application path. Here first switch is -v /app/node_modules , is not using the reference. Instead it says, not to map the /app/node_modules of the host machine. For the second switch, -v ${pwd}:/app , we are using the host machine volume. The : stands when we use host machine directory as reference in the docker. If the hot reload not work, need to create a .env in the root directory and need to add CHOKIDAR_USEPOLLING=true . The .env file should be CHOKIDAR_USEPOLLING=true Shorthand of Docker CLI The downside of previous run command is it is ridiculously long. We have to specify the port-mapping , volume-mapping and also the image id. With docker compose we can dramatically shorten the command to run the container in development phase. Let's create a docker-compose inside the project directory and encode the port mapping and volume mapping. First go to the project root directory and create a file named docker-compose.yml touch docker-compose.yml Our docker-compose.yml file will be like, version: \"3\" services: web: stdin_open: true build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app With docker compose, from now on, we can simple run the application by, docker-compose up Executing Test Cases Now we have a solid development infrastructure for the react application with docker container. It's time to focus on running the tests inside the container. First we run the tests to our development environment and then shift to Travis CI . The good things is, running test cases inside the container is very much straight forward. To run the test cases, we have to follow two steps, Create the image out of the Dockerfile Override the startup command with the test command First let's build the image, docker build -f Dockerfile.dev . Now, to override our container startup command and replace it with the npm run test , we can do, docker run image_id npm run test This should run all the test cases for the app. To open the test runner in an interactive mode, we can utilize the -it flag. docker run -it image_id npm run test Now the test suite will run in the interactive mode. Live Update Test Cases If we run the tests in the container and update the test suite, we will notice the test cases changes does not impact inside the container. May be you got the reason. Here we created a special container by docker build -f Dockerfile.dev . that take the snapshot of the working files and put then inside the container. Now this special temporary container does not have volume mapping set up. So changes inside the files, does not impact the test cases changes. To resolve the issue we can take multiple approach. One is, start the container with docker compose and then run the docker exec to use the web service. Let's try this one, From one terminal, run the container with docker compose docker-compose up Now from another terminal, first take the running container id by docker ps From the out put we can get the container_id and run npm run test directly inside the container, docker exec -it container_id npm run test This should run the test suite in interactive mode with live update. Now this solution is not as good as it should be. Here in the docker-compose.yml file we will add another service, which will be solely responsible for run the test suite whenever we change the files. We already have a service named web that is responsible for run the web application. Our new service, we can named tests will be responsible for run the test suites. Except the web service, the tests service do not require the port-mapping , instead it needs to override the start up command. In this tests service the startup command should be npm run test . Our new docker-compose.yml file will be, version: \"3\" services: web: stdin_open: true build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app tests: build: context: . dockerfile: Dockerfile.dev volumes: - /app/node_modules - .:/app command: [\"npm\", \"run\", \"test\"] Now run the container with a new build docker-compose up --build This will run both, the web application and the test suite. So bottom line is both approaches has some downside. For the first approach, we have to find out the running container id and remember the docker exec command. For the second approach, we get the test suite result in the docker log area. Also in this approach we can get the test suite in an interactive mode. Nginx in Production Server In development phase, we have a development server provided by the create-react-app that handle the request of port 3000 . In production, we need a web server whose sole purpose will be to respond to browser request. In this case, we can use an extremely popular web server Nginx . It's a minimal server basically do the routing. So we create a separate docker file, that will create a production version of web container. This production version of docker container will start an instance of Nginx. This Nginx server will be used to serve our index.html and main.js file. Multi Step Docker Builds To use Nginx in the production environment, we will need Build Phase (To build the react app) Use Node base image Copy the package.json file Install dependencies Build the app by npm run build Run Phase (To run the Nginx server) Use Nginx image Copy the build file generated in the build phase Start the Nginx server Let's create a docker file for our production environment, touch Dockerfile In the Dockerfile we will have two distinctly different section. One is for build the app and second is run the Nginx server with build files. Nginx base image has already integrated a startup command. We do not explicitly start the Nginx Server . In the Run Phage , when we will use the Nginx using docker, it will automatically remove all files and folder from the Build Phase except the build directory. So our production image will be very small. Our Dockerfile will be, FROM node:alpine WORKDIR '/app' COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx COPY --from=0 /app/build /usr/share/nginx/html Now, let's build the image docker build . The end result of the build command will be an image_id . To to run the container, since Nginx is a web server, we have to do the port mapping. Nginx use 80 as default port. We can run the container using the following, docker run -p 8080:80 image_id Now, if we go to http://localhost:8080 , we should see the react app running. So, now we got a docker application, that can be build our application and serve the application from a Nginx server. Now we need to ship all our work to the outside world, the deployment. Setting Up Git Repository Let's create a git repository named ix-docker-react . Please make sure the git repository is public. Off-course, the private repo will work but for private we have to set up couple of additional config. Push all your code base to the github master branch. Make sure in the github, your react-application source with Dockerfile exist. You can follow the commands, git init git add . git commit -m 'initial commit' git remote add origin remote_git_repository_address git push origin master Setting Up Travis CI We have not discussed about Travis CI a lot. Travis CI essentially set a sky limit for our code base, we can do whatever we want. When we push some changes to the github, github taps the Travis CI that some code changes. In this case, Travis CI pull the code base and do whatever we are asked to. Some developer use Travis CI for test the code base after any changes, someone use the Travis CI to deploy the codebase. In our case, we will use Travis CI for both, Test the codebase and Deploy to the AWS . To set up Travis CI , Sign in Travis CI with your github account (For simplicity) Enable Github App (This will load all your github repo in Travis CI dashboard) Find ix-docker-react and enable tracking by tapping the switch button Travis CI ui is being changed time to time. If you have trouble finding the green switch button, please find legacy site here and tap the button Now we have to explicitly tell Travis CI what to do when a code base is being changed. For now we ignore the AWS Deployment and focus on testing. We essentially tell Travis CI how to start the Docker , run the test suite and interpret the test result. In order to do so, we will require a file .travis.yml in the project root directory. Let's create the file .travis.yml , touch .travis.yml In the .travis.yml we make sure the super user privilege is being configured. We require the super user privilege for running the docker. We also specify that, we are using the docker , so Travis CI will bring a copy of the Docker . Finally, we build an image out of the Dockerfile.dev to run our test suite. In our local machine, after we build the image we used to get the image_id and used the image_id to run the container. In Travis CI , we can not manually copy and paste the image_id , so we can utilize the use of tagging . Our .travis.yml file should be like the following, language: generic sudo: required services: - docker before_install: - docker build -t docker_username/github_repo_name -f Dockerfile.dev script: - docker run -e CI=true docker_username/github_repo_name npm run test -- --coverage As tag, we can use any name. But its a good practice to use the convention docker_username / github_repo_name Default behaviour of Jest is run the test for the first time and bring an interactive terminal to run test according to developer input. With -- -- coverage we can change that default behaviour and the will run once and return status code and terminate. When test results return with status code 0 , means test coverage is as expected Now push the changed codebase to the github repository. git add .travis.yml git commit -m 'Added travis file' git push origin master This should trigger the test in the Travis CI . In the dashboard, inside react-docker application, we should see the test suite running and should get passed all the tests. Now, we have a pipeline in place to automatically watch out our github repository for changes and pull down the source code to run the tests and report back if everything is alright. Set Up AWS Elastic Beanstalk Elastic Beanstalk is the easiest way to run Single Container application in AWS Infrastructure . When we deploy a code base to AWS Beanstalk using Travis CI , in background, the Travis CI upload a zipped version of the code to the S3 Bucket and tap the Beanstalk to notify there's a new version of code is being pushed. Then the AWS Beanstalk pull the codebase from the S3 Bucket and redeploy. And good things is, when we create a AWS Elastic Beanstalk environment, this S3 Bucket is being automatically created. AWS recently update the AWS Elastic Beanstalk that can work with docker compose . By default, when platform branch is Docker Running on 64bit Linux , then the new feature with docker-compose works. In our case we will make use vanilla Dockerfile . When we set a docker environment in the AWS Beanstalk , AWS Beanstalk create a virtual machine that's sole purpose is to run the docker container we provide. In the AWS Beanstalk there is already an built in load-balancer . So whenever the traffic flows increase, the AWS Beanstalk automatically increase the number of Virtual Machine , as well as, our container and react-app inside the container. Login to your AWS account and select the Elastic Beanstalk service. Now create an application with the following config, Application name can be anything, I am using ix-docker-react Platform should be Docker Platform branch should be Docker Running on 64bit Amazon Linux Platform version as AWS Recommended , I am using 2.16.4 Creating the application environment might take couple of minutes. After the environment being created, the environment will be listed in the environment section. Travis Config AWS Deployment For deployment, we will require the following config, provider , should be elasticbeanstalk , already configured and heavy lifting by the Travis CI region , Where the AWS Elastic Beanstalk is being created app , our Elastic Beanstalk app name env , our Elastic Beanstalk environment name bucket_name , automatically generated bucket name by the Elastic Beanstalk bucket_path , same as the app name on -> branch , on which branch code changes, we should re-deploy the code base credentials , to get the credentials to access Elastic Beanstalk by Travis CI , we have to create a new IAM user with full programmatic access to Elastic Beanstalk . For security purpose, we will use the Travis CI environment variables to store our aws access key and secret. Our new .travis.yml file should be, language: generic sudo: required services: - docker before_install: - docker build -t docker_user_name/ix-docker-react -f Dockerfile.dev . script: - docker run -e CI=true docker_user_name/ix-docker-react npm run test -- --coverage deploy: provider: elasticbeanstalk region: \"ap-south-1\" app: \"ix-docker-react\" env: \"Dockerreact-env\" bucket_name: \"elasticbeanstalk-ap-south-1-366735605679\" bucket_path: \"ix-docker-react\" on: branch: master access_key_id: $AWS_ACCESS_KEY secret_access_key: $AWS_SECRET_KEY In the deploy config, the path name of the S3 bucket should be same as the app name. In the Elastic Beanstalk , it should open port 80 for Nginx . We have to specify the port in the Dockerfile . We can expose a port in the Elastic Beanstalk with Dockerfile by EXPOSE 80 . Our new Dockerfile with updated configuration should be FROM node:alpine WORKDIR '/app' COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx EXPOSE 80 COPY --from=0 /app/build /usr/share/nginx/html Now, if we push the changes to the github the application should be deployed to the AWS Elastic Beanstalk . From Elastic Beanstalk we can get the web url and access our react application. Now we have a complete CI/CD with docker , github , Travis CI and AWS Beanstalk . In a team, engineers make commit in the feature branch. Other engineers will review and merge the code base to master branch. The moment, codebase is being merged to the master branch, the CI/CD will be triggered and make the deployment. Cleanup For the AWS Beanstalk , when we create a environment, the AWS Beanstalk create a EC2 Instance internally. This EC2 Instance is costing money. So after we are done our experiments, we need to delete the AWS Beanstalk application. We can delete the Beanstalk Application from the dashboard.","title":"01 Single Container CI CD Workflow"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#single-container-workflow","text":"Let's use docker in a production type environment. The important thing to keep in mind, we first explain the workflow without docker. Instead we discuss outside services that we are going to use to set up this development workflow. Once we get the bird view of the workflow and get the core design behind the workflow, then we introduce the docker and find how docker can facilitate everything. We will create an application that will use docker and eventually push the application to AWS . Our workflow is going to be Develop Testing Deployment Also for any changes, we will repeat the workflow again. Development Phase : Our dev workflow is starting by creating a git repository. This git repository is going to be the center of coordination of all the code we will write. Out git repository will have two types of branch, master and feature branch. We make changes on the feature branch. By filing a PR we will merge the feature branch to master branch. When we do the PR there will be a series of actions, defining how we govern the codebase. The master branch will contain the very clean copy of our code base. Test Phase : As soon as we make the PR, the Travis CI will pull the new and updated code and run the test. If all the tests executed successfully, then we will merge the code to the master branch. Production Phase : After merging the feature branch, We then again push the code to Travis CI and run tests of the code. Any changes on the master branch will eventually and automatically be hosted in the AWS Beanstalk . Now we need to find out how docker fits in this place. To execute this workflow, we do not need to make use of Docker . But using docker will make the workflow lot lot easier. Thats the soul purpose of docker. It's not necessarily a requirement, it just make developer life easier. Docker is not the focus here, it's more of using these 3rd party services (like, Github, Travis CI, AWS Beanstalk) with docker.","title":"Single Container Workflow"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#generating-a-project","text":"We will use a react for simplicity. To create a react project, make sure node.js is installed in your system. Then create the project named frontend by the followings, npm create-react-app frontend This will create the react project and install all the necessary dependencies. Now go to the project directory, cd frontend To run the test in local machine, we can use npm run test To build the application for future deployment, we can build using npm run build An finally to run the application in the local environment, we can use npm run start","title":"Generating a Project"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#generating-dev-dockerfile","text":"First go to the project directory. Here we will create a Dockerfile named Dockerfile.dev . The purpose of using .dev with the Dockerfile is to make clear that, this docker file is only using in the development environment. In future, we will use another Dockerfile with simply name Dockerfile , without any extension for our production environment. So, lets create the Dockerfile.dev in the project root directory, touch Dockerfile.dev Inside the Dockerfile.dev we will write configuration to make our image. Our Dockerfile.dev will be similar like the following. # Define base image FROM node:alpine # Define working directory inside the container WORKDIR /app # Copy the package.json file to the project directory COPY package.json . # Install the dependencies RUN npm install # Copy all the source code from host machine to the container project directory COPY . ./ # Start up command of the container to run the server CMD [\"npm\", \"run\", \"start\"]","title":"Generating Dev Dockerfile"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#building-image-from-dockerfile","text":"Since we are not using default name of Dockerfile , instead we are using Dockerfile.dev we have to specify the name during building image. To do so, we can use -f flag, that helps to specify the docker file name. This time, during development, our build instruction will be, docker build -f Dockerfile.dev . When we initially create our react application using create-react-app , the create-react-app automatically install all the node modules inside the node_modules directory. Now, when we are building the image out of the app, we will again installing the dependencies using RUN npm install instruction in the Dockerfile.dev file. We do not need the node_modules in the host machine, it usually increase the duration of image building process, since we are running the app inside the container . So we can delete the node_modules directory from the host machine . node_modules directory is not necessary in the host machine project directory, it increase the time to build the image. We can delete node_modules from project directory. If we again run the previous instruction to build the image, we will get much faster output, docker build -f Dockerfile.dev .","title":"Building Image From Dockerfile"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#run-container-in-interactive-mode","text":"Now we can run a container from the image by docker run -it -p 3000:3000 IMAGE_ID If we observe closely, we can notice, while we made changes in the codebase, the hot reloading is not working. The image was build on the snapshot of files. One way is after each changes in the code base we will rebuild the image. But this is a costly and inefficient approach. So we need to find a way to enable hot reload inside inside the image.","title":"Run Container in Interactive Mode"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#enabling-hot-reload-using-docker-volume","text":"In the last section, we made changes in a file and the changes was not reflected in the container. Now we will figure out a way to solve the issue without stopping, rebuild and restarting the container. A docker volume essentially a mapping of directory between host machine and container. With docker volume, we can use the reference of local machine directory from the host machine. In this case, we do not copy the directory of the local machine in the container, instead, use the host machine directory by reference from container. To use the volume mapping we need to use the followings, docker run -p 3000:3000 -v /app/node_modules -v $(pwd):/app image_id Here we have used two switches. Each switch has a -v flag which is used to set up a volume. The ${pwd} is stands for print working directory , used to get the current application path. Here first switch is -v /app/node_modules , is not using the reference. Instead it says, not to map the /app/node_modules of the host machine. For the second switch, -v ${pwd}:/app , we are using the host machine volume. The : stands when we use host machine directory as reference in the docker. If the hot reload not work, need to create a .env in the root directory and need to add CHOKIDAR_USEPOLLING=true . The .env file should be CHOKIDAR_USEPOLLING=true","title":"Enabling Hot Reload Using Docker Volume"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#shorthand-of-docker-cli","text":"The downside of previous run command is it is ridiculously long. We have to specify the port-mapping , volume-mapping and also the image id. With docker compose we can dramatically shorten the command to run the container in development phase. Let's create a docker-compose inside the project directory and encode the port mapping and volume mapping. First go to the project root directory and create a file named docker-compose.yml touch docker-compose.yml Our docker-compose.yml file will be like, version: \"3\" services: web: stdin_open: true build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app With docker compose, from now on, we can simple run the application by, docker-compose up","title":"Shorthand of Docker CLI"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#executing-test-cases","text":"Now we have a solid development infrastructure for the react application with docker container. It's time to focus on running the tests inside the container. First we run the tests to our development environment and then shift to Travis CI . The good things is, running test cases inside the container is very much straight forward. To run the test cases, we have to follow two steps, Create the image out of the Dockerfile Override the startup command with the test command First let's build the image, docker build -f Dockerfile.dev . Now, to override our container startup command and replace it with the npm run test , we can do, docker run image_id npm run test This should run all the test cases for the app. To open the test runner in an interactive mode, we can utilize the -it flag. docker run -it image_id npm run test Now the test suite will run in the interactive mode.","title":"Executing Test Cases"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#live-update-test-cases","text":"If we run the tests in the container and update the test suite, we will notice the test cases changes does not impact inside the container. May be you got the reason. Here we created a special container by docker build -f Dockerfile.dev . that take the snapshot of the working files and put then inside the container. Now this special temporary container does not have volume mapping set up. So changes inside the files, does not impact the test cases changes. To resolve the issue we can take multiple approach. One is, start the container with docker compose and then run the docker exec to use the web service. Let's try this one, From one terminal, run the container with docker compose docker-compose up Now from another terminal, first take the running container id by docker ps From the out put we can get the container_id and run npm run test directly inside the container, docker exec -it container_id npm run test This should run the test suite in interactive mode with live update. Now this solution is not as good as it should be. Here in the docker-compose.yml file we will add another service, which will be solely responsible for run the test suite whenever we change the files. We already have a service named web that is responsible for run the web application. Our new service, we can named tests will be responsible for run the test suites. Except the web service, the tests service do not require the port-mapping , instead it needs to override the start up command. In this tests service the startup command should be npm run test . Our new docker-compose.yml file will be, version: \"3\" services: web: stdin_open: true build: context: . dockerfile: Dockerfile.dev ports: - \"3000:3000\" volumes: - /app/node_modules - .:/app tests: build: context: . dockerfile: Dockerfile.dev volumes: - /app/node_modules - .:/app command: [\"npm\", \"run\", \"test\"] Now run the container with a new build docker-compose up --build This will run both, the web application and the test suite. So bottom line is both approaches has some downside. For the first approach, we have to find out the running container id and remember the docker exec command. For the second approach, we get the test suite result in the docker log area. Also in this approach we can get the test suite in an interactive mode.","title":"Live Update Test Cases"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#nginx-in-production-server","text":"In development phase, we have a development server provided by the create-react-app that handle the request of port 3000 . In production, we need a web server whose sole purpose will be to respond to browser request. In this case, we can use an extremely popular web server Nginx . It's a minimal server basically do the routing. So we create a separate docker file, that will create a production version of web container. This production version of docker container will start an instance of Nginx. This Nginx server will be used to serve our index.html and main.js file.","title":"Nginx in Production Server"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#multi-step-docker-builds","text":"To use Nginx in the production environment, we will need Build Phase (To build the react app) Use Node base image Copy the package.json file Install dependencies Build the app by npm run build Run Phase (To run the Nginx server) Use Nginx image Copy the build file generated in the build phase Start the Nginx server Let's create a docker file for our production environment, touch Dockerfile In the Dockerfile we will have two distinctly different section. One is for build the app and second is run the Nginx server with build files. Nginx base image has already integrated a startup command. We do not explicitly start the Nginx Server . In the Run Phage , when we will use the Nginx using docker, it will automatically remove all files and folder from the Build Phase except the build directory. So our production image will be very small. Our Dockerfile will be, FROM node:alpine WORKDIR '/app' COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx COPY --from=0 /app/build /usr/share/nginx/html Now, let's build the image docker build . The end result of the build command will be an image_id . To to run the container, since Nginx is a web server, we have to do the port mapping. Nginx use 80 as default port. We can run the container using the following, docker run -p 8080:80 image_id Now, if we go to http://localhost:8080 , we should see the react app running. So, now we got a docker application, that can be build our application and serve the application from a Nginx server. Now we need to ship all our work to the outside world, the deployment.","title":"Multi Step Docker Builds"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#setting-up-git-repository","text":"Let's create a git repository named ix-docker-react . Please make sure the git repository is public. Off-course, the private repo will work but for private we have to set up couple of additional config. Push all your code base to the github master branch. Make sure in the github, your react-application source with Dockerfile exist. You can follow the commands, git init git add . git commit -m 'initial commit' git remote add origin remote_git_repository_address git push origin master","title":"Setting Up Git Repository"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#setting-up-travis-ci","text":"We have not discussed about Travis CI a lot. Travis CI essentially set a sky limit for our code base, we can do whatever we want. When we push some changes to the github, github taps the Travis CI that some code changes. In this case, Travis CI pull the code base and do whatever we are asked to. Some developer use Travis CI for test the code base after any changes, someone use the Travis CI to deploy the codebase. In our case, we will use Travis CI for both, Test the codebase and Deploy to the AWS . To set up Travis CI , Sign in Travis CI with your github account (For simplicity) Enable Github App (This will load all your github repo in Travis CI dashboard) Find ix-docker-react and enable tracking by tapping the switch button Travis CI ui is being changed time to time. If you have trouble finding the green switch button, please find legacy site here and tap the button Now we have to explicitly tell Travis CI what to do when a code base is being changed. For now we ignore the AWS Deployment and focus on testing. We essentially tell Travis CI how to start the Docker , run the test suite and interpret the test result. In order to do so, we will require a file .travis.yml in the project root directory. Let's create the file .travis.yml , touch .travis.yml In the .travis.yml we make sure the super user privilege is being configured. We require the super user privilege for running the docker. We also specify that, we are using the docker , so Travis CI will bring a copy of the Docker . Finally, we build an image out of the Dockerfile.dev to run our test suite. In our local machine, after we build the image we used to get the image_id and used the image_id to run the container. In Travis CI , we can not manually copy and paste the image_id , so we can utilize the use of tagging . Our .travis.yml file should be like the following, language: generic sudo: required services: - docker before_install: - docker build -t docker_username/github_repo_name -f Dockerfile.dev script: - docker run -e CI=true docker_username/github_repo_name npm run test -- --coverage As tag, we can use any name. But its a good practice to use the convention docker_username / github_repo_name Default behaviour of Jest is run the test for the first time and bring an interactive terminal to run test according to developer input. With -- -- coverage we can change that default behaviour and the will run once and return status code and terminate. When test results return with status code 0 , means test coverage is as expected Now push the changed codebase to the github repository. git add .travis.yml git commit -m 'Added travis file' git push origin master This should trigger the test in the Travis CI . In the dashboard, inside react-docker application, we should see the test suite running and should get passed all the tests. Now, we have a pipeline in place to automatically watch out our github repository for changes and pull down the source code to run the tests and report back if everything is alright.","title":"Setting Up Travis CI"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#set-up-aws-elastic-beanstalk","text":"Elastic Beanstalk is the easiest way to run Single Container application in AWS Infrastructure . When we deploy a code base to AWS Beanstalk using Travis CI , in background, the Travis CI upload a zipped version of the code to the S3 Bucket and tap the Beanstalk to notify there's a new version of code is being pushed. Then the AWS Beanstalk pull the codebase from the S3 Bucket and redeploy. And good things is, when we create a AWS Elastic Beanstalk environment, this S3 Bucket is being automatically created. AWS recently update the AWS Elastic Beanstalk that can work with docker compose . By default, when platform branch is Docker Running on 64bit Linux , then the new feature with docker-compose works. In our case we will make use vanilla Dockerfile . When we set a docker environment in the AWS Beanstalk , AWS Beanstalk create a virtual machine that's sole purpose is to run the docker container we provide. In the AWS Beanstalk there is already an built in load-balancer . So whenever the traffic flows increase, the AWS Beanstalk automatically increase the number of Virtual Machine , as well as, our container and react-app inside the container. Login to your AWS account and select the Elastic Beanstalk service. Now create an application with the following config, Application name can be anything, I am using ix-docker-react Platform should be Docker Platform branch should be Docker Running on 64bit Amazon Linux Platform version as AWS Recommended , I am using 2.16.4 Creating the application environment might take couple of minutes. After the environment being created, the environment will be listed in the environment section.","title":"Set Up AWS Elastic Beanstalk"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#travis-config-aws-deployment","text":"For deployment, we will require the following config, provider , should be elasticbeanstalk , already configured and heavy lifting by the Travis CI region , Where the AWS Elastic Beanstalk is being created app , our Elastic Beanstalk app name env , our Elastic Beanstalk environment name bucket_name , automatically generated bucket name by the Elastic Beanstalk bucket_path , same as the app name on -> branch , on which branch code changes, we should re-deploy the code base credentials , to get the credentials to access Elastic Beanstalk by Travis CI , we have to create a new IAM user with full programmatic access to Elastic Beanstalk . For security purpose, we will use the Travis CI environment variables to store our aws access key and secret. Our new .travis.yml file should be, language: generic sudo: required services: - docker before_install: - docker build -t docker_user_name/ix-docker-react -f Dockerfile.dev . script: - docker run -e CI=true docker_user_name/ix-docker-react npm run test -- --coverage deploy: provider: elasticbeanstalk region: \"ap-south-1\" app: \"ix-docker-react\" env: \"Dockerreact-env\" bucket_name: \"elasticbeanstalk-ap-south-1-366735605679\" bucket_path: \"ix-docker-react\" on: branch: master access_key_id: $AWS_ACCESS_KEY secret_access_key: $AWS_SECRET_KEY In the deploy config, the path name of the S3 bucket should be same as the app name. In the Elastic Beanstalk , it should open port 80 for Nginx . We have to specify the port in the Dockerfile . We can expose a port in the Elastic Beanstalk with Dockerfile by EXPOSE 80 . Our new Dockerfile with updated configuration should be FROM node:alpine WORKDIR '/app' COPY package.json . RUN npm install COPY . . RUN npm run build FROM nginx EXPOSE 80 COPY --from=0 /app/build /usr/share/nginx/html Now, if we push the changes to the github the application should be deployed to the AWS Elastic Beanstalk . From Elastic Beanstalk we can get the web url and access our react application. Now we have a complete CI/CD with docker , github , Travis CI and AWS Beanstalk . In a team, engineers make commit in the feature branch. Other engineers will review and merge the code base to master branch. The moment, codebase is being merged to the master branch, the CI/CD will be triggered and make the deployment.","title":"Travis Config AWS Deployment"},{"location":"Notes/02 CI-CD With Docker/01 Single Container CI-CD Workflow/#cleanup","text":"For the AWS Beanstalk , when we create a environment, the AWS Beanstalk create a EC2 Instance internally. This EC2 Instance is costing money. So after we are done our experiments, we need to delete the AWS Beanstalk application. We can delete the Beanstalk Application from the dashboard.","title":"Cleanup"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/","text":"Multi-container development environment Today we are going to make an app that is responsible to generate fibonacci number. But instead of writing a simple method, we will take it to next level and put couple of complexity layer to serve our sole purpose, multi container CI/CD. We will have a react application, that will take the input for a user to get the fibonacci number of a certain index. This index will pass to the backend server. We will use an express server in the backend. The express server will save the index in Postgres and also store the index in the redis server. A worker process will be responsible for generating the fibonacci number. It will generate, put the result in the redis and then finally return the response to the react application. Too much complexity!! We are taking this complexity just to make multiple container and implement CI/CD. Application Architecture With Image Architecture image goes here. Boilerplate Code A boilerplate code is being found in the resource directory . description of boilerplate goes here To make the development process smoother, we will make development version of each docker container. This will help us not to rebuild the image every time we make changes in development phase. For each of the projects, we will set up pretty similar docker file workflow. For each of the project we will go through, Copy the package.json to the container Run npm install to install all the dependencies Copy everything else Volume mapping for hot-reload feature Docker Dev For React App First go to the client directory and create a Dockerfile.dev , cd client touch Dockerfile.dev And our Dockerfile.dev should be, FROM node:alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] Let's build an image out of this Dockerfile , docker build -f Dockerfile.dev . This should build an image and give us a image_id . Now we can run the react app using the image id, docker run -it image_id This should start the development server of our react app. Since we have not port mapping yet, we can not access the site. Docker Dev For Express Server Go to the server directory and create a file named Dockerfile.dev , cd server touch Dockerfile.dev Our Dockerfile.dev file should be like following, FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"dev\"] Let's build an image out of this Dockerfile , docker build -f Dockerfile.dev . This should build an image and give us a image_id . Now we can run the react app using the image id, docker run -it image_id This should start the express server on port 5000 . Docker Dev For Worker Go to the worker directory and create a docker-file named Dockerfile.dev , cd worker touch Dockerfile.dev Our Dockerfile.dev should be like the following, same as the express server Dockerfile.dev , FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"dev\"] Let's build an image out of this Dockerfile , docker build -f Dockerfile.dev . This should build an image and give us a image_id . Now we can run the react app using the image id, docker run -it image_id This should make the worker process standby, so it can listen whenever we insert a message in the redis server. Adding Postgres , Server , Worker and Client Service Now we have docker-file for the client, server and worker process. Now, we are going to put a docker-compose file to make all the application start up more easy. Each of the application container require different arguments, like the express server require a port mapping for port 5000 , react app need a port mapping 3000 . We also need to make sure the worker process has the access to redis server. Also the express server needs access of redis server and postgres server. Along with these integrations, we have to provide all the environment variables to the container. To do so, we first integrate the express server with the redis-server and postgres-database . After that, we will connect all other pieces, the Nginx server , react app and worker process. Let's create the docker-compose.yml file in the project root directory, touch docker-compose.yml Our docker-compose.yml file should be, version: \"3\" services: postgres: image: \"postgres:latest\" environment: - POSTGRES_PASSWORD=postgres_password redis: image: \"redis:latest\" api: build: dockerfile: Dockerfile.dev context: ./server volumes: - /app/node_modules - ./server:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client: stdin_open: true build: dockerfile: Dockerfile.dev context: ./client volumes: - /app/node_modules - ./client:/app worker: build: dockerfile: Dockerfile.dev context: ./worker volumes: - /app/node_modules - ./worker:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 We cn build and run the container from our root directory by, docker-compose up --build Nginx Configuration From browser, we will make request for static resources and seek API . For react application, we will make the call similar like, /main.js , /index.html . But for server api, we will make call on endpoints like /api/values/all , /api/values/current etc. You might notice our express server does not have /api as prefix. It has endpoints like /values/all , /values/current . Our Nginx server will handle and do the separation. For api endpoints, start with /api it will remove the /api part and redirect to the express server. Other request will be send to the react application. Whenever we create a Nginx server, it will use a configuration file named default.conf . Here in this default.conf file, we have to put couple of following information, Notify Nginx that, we have a upstream server at client:3000 Notify Nginx that, we have a upstream server at server:5000 Both client:3000 and server:3000 should listen to port 80 Add a condition to pass all the / request to client:3000 Add another condition to pass all the /api request to server:5000 Here client:3000 and server:5000 , comes from the service name we are using in the docker-compose file. Lets create a directory named nginx inside the root project and create a file default.conf inside the directory. mkdir nginx cd nginx touch default.conf Our default.conf file should be, upstream client { server client:3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } In Nginx config rewrite /api/(.*) /$1 break; means, replace /api with $1 and $1 stands for the matching part (.*) of the url. break keyword stands for stopping any other rewriting rules after applying the current one. Nginx Container We set up the nginx configuration. Time to set up a docker file for the nginx server . Go to the nginx directory and create a file named Dockerfile.dev , cd nginx touch Dockerfile.dev Our Dockerfile.dev should look like the following, FROM nginx COPY ./default.conf /etc/nginx/conf.d/default.conf Thats pretty much it. Last thing we need to do, is add the nginx service in our docker-compose.yml file. We need to add the following nginx service to our docker-compose file, nginx: restart: always build: dockerfile: Dockerfile.dev context: ./nginx ports: - \"3050:80\" Since our nginx server is do all the routing, no matter what, we want our nginx server up and running. So, we put restart property always . In this case, we also do the port mapping from local machine to the container. With adding the nginx service to our existing docker-compose , our docker-compose.yml file should be, version: \"3\" services: postgres: image: \"postgres:latest\" environment: - POSTGRES_PASSWORD=postgres_password redis: image: \"redis:latest\" nginx: restart: always build: dockerfile: Dockerfile.dev context: ./nginx ports: - \"3050:80\" api: build: dockerfile: Dockerfile.dev context: ./server volumes: - /app/node_modules - ./server:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client: stdin_open: true build: dockerfile: Dockerfile.dev context: ./client volumes: - /app/node_modules - ./client:/app worker: build: dockerfile: Dockerfile.dev context: ./worker volumes: - /app/node_modules - ./worker:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 Now time to start all the containers by, docker-compose up --build Most probably, first time, the server and worker both try to get the redis instance, even it might not being copied. So In case of any error, we just have do run the container one more time by, docker-compose up Now, from the local machine browser, if we go to http://localhost:3050/ , we should see the react app and calculation should work with manual refresh. Enable Websocket Connection The react application keep an connection with it's development server to maintain hot reload. Every time there is a source code changes, react app listen these changes via websocket connection and reload the web app. We need to configure the nginx server to enable the websocket to handle the issue. To add websocket connection we need a route in the default.config file, location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } So our final configuration for the nginx server will be, upstream client { server client:3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } Now, we can test all the container by running, docker-compose up --build Update the UI Go to client directory and from the /src directory, update the App.js by the followings, import React from 'react'; import { BrowserRouter as Router, Route } from 'react-router-dom'; import OtherPage from './OtherPage'; import Fib from './Fib'; function App() { return ( <Router> <div> <Route exact path=\"/\" component={Fib} /> <Route path=\"/otherpage\" component={OtherPage} /> </div> </Router> ); } export default App; Our app should be running on http://localhost:3050/ . Go to browser and go the address http://localhost:3050/ . In the input box, put the value 2 and click submit. Now if we reload the web page, the value 4 should be appeared. Seems like our app is running smoothly on the development machine as expected.","title":"01 Multi container development environment"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#multi-container-development-environment","text":"Today we are going to make an app that is responsible to generate fibonacci number. But instead of writing a simple method, we will take it to next level and put couple of complexity layer to serve our sole purpose, multi container CI/CD. We will have a react application, that will take the input for a user to get the fibonacci number of a certain index. This index will pass to the backend server. We will use an express server in the backend. The express server will save the index in Postgres and also store the index in the redis server. A worker process will be responsible for generating the fibonacci number. It will generate, put the result in the redis and then finally return the response to the react application. Too much complexity!! We are taking this complexity just to make multiple container and implement CI/CD.","title":"Multi-container development environment"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#application-architecture-with-image","text":"Architecture image goes here.","title":"Application Architecture With Image"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#boilerplate-code","text":"A boilerplate code is being found in the resource directory . description of boilerplate goes here To make the development process smoother, we will make development version of each docker container. This will help us not to rebuild the image every time we make changes in development phase. For each of the projects, we will set up pretty similar docker file workflow. For each of the project we will go through, Copy the package.json to the container Run npm install to install all the dependencies Copy everything else Volume mapping for hot-reload feature","title":"Boilerplate Code"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#docker-dev-for-react-app","text":"First go to the client directory and create a Dockerfile.dev , cd client touch Dockerfile.dev And our Dockerfile.dev should be, FROM node:alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] Let's build an image out of this Dockerfile , docker build -f Dockerfile.dev . This should build an image and give us a image_id . Now we can run the react app using the image id, docker run -it image_id This should start the development server of our react app. Since we have not port mapping yet, we can not access the site.","title":"Docker Dev For React App"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#docker-dev-for-express-server","text":"Go to the server directory and create a file named Dockerfile.dev , cd server touch Dockerfile.dev Our Dockerfile.dev file should be like following, FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"dev\"] Let's build an image out of this Dockerfile , docker build -f Dockerfile.dev . This should build an image and give us a image_id . Now we can run the react app using the image id, docker run -it image_id This should start the express server on port 5000 .","title":"Docker Dev For Express Server"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#docker-dev-for-worker","text":"Go to the worker directory and create a docker-file named Dockerfile.dev , cd worker touch Dockerfile.dev Our Dockerfile.dev should be like the following, same as the express server Dockerfile.dev , FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"dev\"] Let's build an image out of this Dockerfile , docker build -f Dockerfile.dev . This should build an image and give us a image_id . Now we can run the react app using the image id, docker run -it image_id This should make the worker process standby, so it can listen whenever we insert a message in the redis server.","title":"Docker Dev For Worker"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#adding-postgres-server-worker-and-client-service","text":"Now we have docker-file for the client, server and worker process. Now, we are going to put a docker-compose file to make all the application start up more easy. Each of the application container require different arguments, like the express server require a port mapping for port 5000 , react app need a port mapping 3000 . We also need to make sure the worker process has the access to redis server. Also the express server needs access of redis server and postgres server. Along with these integrations, we have to provide all the environment variables to the container. To do so, we first integrate the express server with the redis-server and postgres-database . After that, we will connect all other pieces, the Nginx server , react app and worker process. Let's create the docker-compose.yml file in the project root directory, touch docker-compose.yml Our docker-compose.yml file should be, version: \"3\" services: postgres: image: \"postgres:latest\" environment: - POSTGRES_PASSWORD=postgres_password redis: image: \"redis:latest\" api: build: dockerfile: Dockerfile.dev context: ./server volumes: - /app/node_modules - ./server:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client: stdin_open: true build: dockerfile: Dockerfile.dev context: ./client volumes: - /app/node_modules - ./client:/app worker: build: dockerfile: Dockerfile.dev context: ./worker volumes: - /app/node_modules - ./worker:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 We cn build and run the container from our root directory by, docker-compose up --build","title":"Adding Postgres, Server, Worker and Client Service"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#nginx-configuration","text":"From browser, we will make request for static resources and seek API . For react application, we will make the call similar like, /main.js , /index.html . But for server api, we will make call on endpoints like /api/values/all , /api/values/current etc. You might notice our express server does not have /api as prefix. It has endpoints like /values/all , /values/current . Our Nginx server will handle and do the separation. For api endpoints, start with /api it will remove the /api part and redirect to the express server. Other request will be send to the react application. Whenever we create a Nginx server, it will use a configuration file named default.conf . Here in this default.conf file, we have to put couple of following information, Notify Nginx that, we have a upstream server at client:3000 Notify Nginx that, we have a upstream server at server:5000 Both client:3000 and server:3000 should listen to port 80 Add a condition to pass all the / request to client:3000 Add another condition to pass all the /api request to server:5000 Here client:3000 and server:5000 , comes from the service name we are using in the docker-compose file. Lets create a directory named nginx inside the root project and create a file default.conf inside the directory. mkdir nginx cd nginx touch default.conf Our default.conf file should be, upstream client { server client:3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } In Nginx config rewrite /api/(.*) /$1 break; means, replace /api with $1 and $1 stands for the matching part (.*) of the url. break keyword stands for stopping any other rewriting rules after applying the current one.","title":"Nginx Configuration"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#nginx-container","text":"We set up the nginx configuration. Time to set up a docker file for the nginx server . Go to the nginx directory and create a file named Dockerfile.dev , cd nginx touch Dockerfile.dev Our Dockerfile.dev should look like the following, FROM nginx COPY ./default.conf /etc/nginx/conf.d/default.conf Thats pretty much it. Last thing we need to do, is add the nginx service in our docker-compose.yml file. We need to add the following nginx service to our docker-compose file, nginx: restart: always build: dockerfile: Dockerfile.dev context: ./nginx ports: - \"3050:80\" Since our nginx server is do all the routing, no matter what, we want our nginx server up and running. So, we put restart property always . In this case, we also do the port mapping from local machine to the container. With adding the nginx service to our existing docker-compose , our docker-compose.yml file should be, version: \"3\" services: postgres: image: \"postgres:latest\" environment: - POSTGRES_PASSWORD=postgres_password redis: image: \"redis:latest\" nginx: restart: always build: dockerfile: Dockerfile.dev context: ./nginx ports: - \"3050:80\" api: build: dockerfile: Dockerfile.dev context: ./server volumes: - /app/node_modules - ./server:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 - PGUSER=postgres - PGHOST=postgres - PGDATABASE=postgres - PGPASSWORD=postgres_password - PGPORT=5432 client: stdin_open: true build: dockerfile: Dockerfile.dev context: ./client volumes: - /app/node_modules - ./client:/app worker: build: dockerfile: Dockerfile.dev context: ./worker volumes: - /app/node_modules - ./worker:/app environment: - REDIS_HOST=redis - REDIS_PORT=6379 Now time to start all the containers by, docker-compose up --build Most probably, first time, the server and worker both try to get the redis instance, even it might not being copied. So In case of any error, we just have do run the container one more time by, docker-compose up Now, from the local machine browser, if we go to http://localhost:3050/ , we should see the react app and calculation should work with manual refresh.","title":"Nginx Container"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#enable-websocket-connection","text":"The react application keep an connection with it's development server to maintain hot reload. Every time there is a source code changes, react app listen these changes via websocket connection and reload the web app. We need to configure the nginx server to enable the websocket to handle the issue. To add websocket connection we need a route in the default.config file, location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } So our final configuration for the nginx server will be, upstream client { server client:3000; } upstream api { server api:5000; } server { listen 80; location / { proxy_pass http://client; } location /sockjs-node { proxy_pass http://client; proxy_http_version 1.1; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \"Upgrade\"; } location /api { rewrite /api/(.*) /$1 break; proxy_pass http://api; } } Now, we can test all the container by running, docker-compose up --build","title":"Enable Websocket Connection"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/01 Multi-container development environment/#update-the-ui","text":"Go to client directory and from the /src directory, update the App.js by the followings, import React from 'react'; import { BrowserRouter as Router, Route } from 'react-router-dom'; import OtherPage from './OtherPage'; import Fib from './Fib'; function App() { return ( <Router> <div> <Route exact path=\"/\" component={Fib} /> <Route path=\"/otherpage\" component={OtherPage} /> </div> </Router> ); } export default App; Our app should be running on http://localhost:3050/ . Go to browser and go the address http://localhost:3050/ . In the input box, put the value 2 and click submit. Now if we reload the web page, the value 4 should be appeared. Seems like our app is running smoothly on the development machine as expected.","title":"Update the UI"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/","text":"Managing multiple containers in local environment Approach We already got our development environment for multiple environment. To implement the continuous integration, we can do the following steps, Make sure, codebase is already in the Github repository Travis CI will pull the code Travis CI will build the test image, run the tests and remove the test image Travis CI will build the production image Travis CI will push the image to Docker Hub Travis CI will notify Elastic Beanstalk that the image is being uploaded to Docker Hub Elastic Beanstalk will pull the image from the Docker Hub , run the container and serve web request Worker Process Production Dockerfile For production, in the worker process, we will use the very similar configuration of development config. The only change will be the start-up command of the container. First go to the worker directory and create a Dockerfile , cd worker touch Dockerfile Our production version of Dockerfile should look like the followings, FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] Production API Dockerfile For production, like previous worker process, our Dockerfile will be similar to the Dockerfile.dev except the container startup command. So, go to the server directory and create a Dockerfile , cd server touch Dockerfile And our Dockerfile for API should be the following, FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"] Nginx Server Production Dockerfile Sometimes, our docker configuration for both development and production will be same. It's always good practice to use separate files for development and production. For our Nginx server, since both development and production docker config file is same, we will just copy the Dockerfile.dev to Dockerfile . cd nginx cp Dockerfile.dev Dockerfile Since we have a route for handling socket connection in development phase, which is not required in the production, We might consider a separate config file for nginx server. Dockerfile For React App For the react we will make use of a separate nginx file. We can definitely use our existing nginx , but to make the application more robust and independent, we will use the another one. In the client directory, create a folder named nginx and a file default.conf , cd client mkdir nginx cd nginx touch default.conf Our default.conf file should be, server { listen 3000; location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html; } } Now we have to create the production version of our react application Dockerfile . Go to the client directory and create Dockerfile , cd client touch Dockerfile Our production version of Dockerfile will be, FROM node:alpine as builder WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . RUN npm run build FROM nginx EXPOSE 3000 COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf COPY --from=builder /app/build /usr/share/nginx/html In the react application, if we try to run the test, it will throw an error. The reason is in the test file App.test.js , we are testing if the App component can render without any crash. This test flow do the followings, First try to render the App component App component try to render the Fib component Fib component try to invoke the express server Since our express server is not running, this will throw an error. In real application, we might simulate an face api response. in this case, we can keep a dummy test execution by removing the following 3 lines from App.test.js , const { getByText } = render(<App />); const linkElement = getByText(/learn react/i); expect(linkElement).toBeInTheDocument(); Our App.test.js file will look like the following, test(\"renders learn react link\", () => {}); Travis CI Setup We now completely set up the production version of docker for each of the application container. Now we have to create a github account, push all our code to github and then hook it to the Travis CI . Then, in our code, we need a Travis CI configuration file, responsible for Build a test image and test code Building production image Push the production image to Docker Hub Notify Elastic Beanstalk on code changes Elastic Beanstalk will pull the image and run the containers First push all the codebase, make sure your github repository is integrated and synced with Travis CI . Now for settings , find and enable the switch button to mark as build project. Since we have not create the Elastic Beanstalk instance in the AWS , we will not configure that part in the Travis CI for now. Also, we are only do the test coverage for the react app, not the api or worker service. Let's create a Travis CI config file named .travis.yml in the root project directory, touch .travis.yml Our .travis.yml file should be like the following, sudo: required services: - docker before_install: - docker build -t DOCKER_HUB_USER_NAME/react-test -f ./client/Dockerfile.dev ./client script: - docker run -e CI=true USERNAME/react-test npm test -- --coverage after_success: - docker build -t DOCKER_HUB_USER_NAME/multi-client ./client - docker build -t DOCKER_HUB_USER_NAME/multi-nginx ./nginx - docker build -t DOCKER_HUB_USER_NAME/multi-server ./server - docker build -t DOCKER_HUB_USER_NAME/multi-worker ./worker - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker push DOCKER_HUB_USER_NAME/multi-client - docker push DOCKER_HUB_USER_NAME/multi-nginx - docker push DOCKER_HUB_USER_NAME/multi-server - docker push DOCKER_HUB_USER_NAME/multi-worker To access the docker hub to upload the image, we need to put the credentials in the Travis CI environment section. To set environment variable, go to Dashboard -> Select Repository -> Options -> Settings -> Environment Variables . In the environment variables section, add the following environment variables, DOCKER_ID DOCKER_PASSWORD Now, if e push the codebase to the github, the Travis CI should test the react app and pushed the build image to the docker hub.","title":"02 Multi container Continuous Integration"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#managing-multiple-containers-in-local-environment","text":"","title":"Managing multiple containers in local environment"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#approach","text":"We already got our development environment for multiple environment. To implement the continuous integration, we can do the following steps, Make sure, codebase is already in the Github repository Travis CI will pull the code Travis CI will build the test image, run the tests and remove the test image Travis CI will build the production image Travis CI will push the image to Docker Hub Travis CI will notify Elastic Beanstalk that the image is being uploaded to Docker Hub Elastic Beanstalk will pull the image from the Docker Hub , run the container and serve web request","title":"Approach"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#worker-process-production-dockerfile","text":"For production, in the worker process, we will use the very similar configuration of development config. The only change will be the start-up command of the container. First go to the worker directory and create a Dockerfile , cd worker touch Dockerfile Our production version of Dockerfile should look like the followings, FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"]","title":"Worker Process Production Dockerfile"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#production-api-dockerfile","text":"For production, like previous worker process, our Dockerfile will be similar to the Dockerfile.dev except the container startup command. So, go to the server directory and create a Dockerfile , cd server touch Dockerfile And our Dockerfile for API should be the following, FROM node:14.14.0-alpine WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . CMD [\"npm\", \"run\", \"start\"]","title":"Production API Dockerfile"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#nginx-server-production-dockerfile","text":"Sometimes, our docker configuration for both development and production will be same. It's always good practice to use separate files for development and production. For our Nginx server, since both development and production docker config file is same, we will just copy the Dockerfile.dev to Dockerfile . cd nginx cp Dockerfile.dev Dockerfile Since we have a route for handling socket connection in development phase, which is not required in the production, We might consider a separate config file for nginx server.","title":"Nginx Server Production Dockerfile"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#dockerfile-for-react-app","text":"For the react we will make use of a separate nginx file. We can definitely use our existing nginx , but to make the application more robust and independent, we will use the another one. In the client directory, create a folder named nginx and a file default.conf , cd client mkdir nginx cd nginx touch default.conf Our default.conf file should be, server { listen 3000; location / { root /usr/share/nginx/html; index index.html index.htm; try_files $uri $uri/ /index.html; } } Now we have to create the production version of our react application Dockerfile . Go to the client directory and create Dockerfile , cd client touch Dockerfile Our production version of Dockerfile will be, FROM node:alpine as builder WORKDIR '/app' COPY ./package.json ./ RUN npm install COPY . . RUN npm run build FROM nginx EXPOSE 3000 COPY ./nginx/default.conf /etc/nginx/conf.d/default.conf COPY --from=builder /app/build /usr/share/nginx/html In the react application, if we try to run the test, it will throw an error. The reason is in the test file App.test.js , we are testing if the App component can render without any crash. This test flow do the followings, First try to render the App component App component try to render the Fib component Fib component try to invoke the express server Since our express server is not running, this will throw an error. In real application, we might simulate an face api response. in this case, we can keep a dummy test execution by removing the following 3 lines from App.test.js , const { getByText } = render(<App />); const linkElement = getByText(/learn react/i); expect(linkElement).toBeInTheDocument(); Our App.test.js file will look like the following, test(\"renders learn react link\", () => {});","title":"Dockerfile For React App"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/02 Multi-container Continuous Integration/#travis-ci-setup","text":"We now completely set up the production version of docker for each of the application container. Now we have to create a github account, push all our code to github and then hook it to the Travis CI . Then, in our code, we need a Travis CI configuration file, responsible for Build a test image and test code Building production image Push the production image to Docker Hub Notify Elastic Beanstalk on code changes Elastic Beanstalk will pull the image and run the containers First push all the codebase, make sure your github repository is integrated and synced with Travis CI . Now for settings , find and enable the switch button to mark as build project. Since we have not create the Elastic Beanstalk instance in the AWS , we will not configure that part in the Travis CI for now. Also, we are only do the test coverage for the react app, not the api or worker service. Let's create a Travis CI config file named .travis.yml in the root project directory, touch .travis.yml Our .travis.yml file should be like the following, sudo: required services: - docker before_install: - docker build -t DOCKER_HUB_USER_NAME/react-test -f ./client/Dockerfile.dev ./client script: - docker run -e CI=true USERNAME/react-test npm test -- --coverage after_success: - docker build -t DOCKER_HUB_USER_NAME/multi-client ./client - docker build -t DOCKER_HUB_USER_NAME/multi-nginx ./nginx - docker build -t DOCKER_HUB_USER_NAME/multi-server ./server - docker build -t DOCKER_HUB_USER_NAME/multi-worker ./worker - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker push DOCKER_HUB_USER_NAME/multi-client - docker push DOCKER_HUB_USER_NAME/multi-nginx - docker push DOCKER_HUB_USER_NAME/multi-server - docker push DOCKER_HUB_USER_NAME/multi-worker To access the docker hub to upload the image, we need to put the credentials in the Travis CI environment section. To set environment variable, go to Dashboard -> Select Repository -> Options -> Settings -> Environment Variables . In the environment variables section, add the following environment variables, DOCKER_ID DOCKER_PASSWORD Now, if e push the codebase to the github, the Travis CI should test the react app and pushed the build image to the docker hub.","title":"Travis CI Setup"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/","text":"Multi-container Continuous Deployment Our Travis CI si already configured to build the image and push the image to the Docker Hub . Now we have to think about how use these images and deploy them to production. To deploy these image we are going to make use of Elastic Beanstalk . When we have only one container, the Elastic Beanstalk will automatically build and run the the container, we do not have to set up any custom configuration. In the root directory, we just have to take the project files and a Dockerfile and Elastic Beanstalk do the rest. Now, the scenario is different. we have multiple Dockerfile in different folder. Anytime we have multiple Dockerfile , we have to tell the Elastic Beanstalk with little configuration, how these Dockerfile will be treated. To put the configuration for Elastic Beanstalk , how our multiple Dockerfile will be treated, we have to create special config file in the project directory named Dockerrun.aws.json . This config file will define, From where the image files will be pulled Resources allocated for the image Port mapping Some associated configurations like handling environment variables The configurations of Dockerrun.aws.json will be very much similar to the docker-compose.yml configuration. docker-compose.yml is all about, how we build images, whereas Dockerrun.aws.json is all about definitions of container. When it comes to handle multiple container, the Elastic Beanstalk does not know, how to handle multiple containers. For multiple container, it delegates the tasks to another AWS server named Elastic Container Service aka ECS . In the ECS for each container, we will define Task , also known as Task Definition . A doc for defining the container configuration is given here . Container Definition Go to the project root directory and create a file named Dockerrun.aws.json , touch Dockerrun.aws.json For the Dockerrun.aws.json file, we have to consider the following conventions, version : We have to specify the template version. Different version compile differently List of containers Definitions For each container name : Name should be the directory name image : Image name that is deployed to the docker hub with the docker hub user name hostname : Hostname should be the service name. This is also being used in the nginx configuration essential : If this is true, crashing this container will make crash other containers. Among all the container, one should marked as essential memory : Need to define the allocated memory in mega bytes, required for a container links : To do the routing we have to use the directory name For the worker and nginx, since no one is routing to these, we do not need any hostname in these configuration. Since, the nginx server is responsible for communicating with the outside world, we need to do the port mapping. Also, in the nginx server configuration we have to specify the routes to other containers It's challenging to allocate exactly essential memory for each of the services. Traditionally there are couple of stackoverflow posts, can be used to find out the desired memory allocation. Our Dockerrun.aws.json should be like the following, { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"client\", \"image\": \"bmshamsnahid/multi-client\", \"hostname\": \"client\", \"essential\": false, \"memory\": 128 }, { \"name\": \"server\", \"image\": \"bmshamsnahid/multi-server\", \"hostname\": \"api\", \"essential\": false, \"memory\": 128 }, { \"name\": \"worker\", \"image\": \"bmshamsnahid/multi-worker\", \"hostname\": \"worker\", \"essential\": false, \"memory\": 128 }, { \"name\": \"nginx\", \"image\": \"bmshamsnahid/multi-nginx\", \"hostname\": \"nginx\", \"essential\": true, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], \"links\": [\"client\", \"server\"], \"memory\": 128 } ] } Using Managed DB Service in Production In development, we are using the redis and postgres in our own development machine. But for production, we might need to consider a managed version of redis aka AWS Elastic Cache and for postgres we will make use of AWS Relational Database Service . Advantages of AWS Elastic Cache and AWS Relational Database Service , Managed creation and maintenance Easy scaling policy Built in logging Better security Easy migration Multi AZ configuration Additionally AWS Relational Database Service has some advantages, Database backup and rollback facility Easy tuning on Multi AZ and Read Replicas Set Up Managed Services Let's build our cloud infrastructure to the using manged AWS Services . Along with the AWS Elastic Beanstalk we will make use of AWS RDS Instance AWS Elastic Cache Set Up Associated Security Group : Go to VPC section and from security group create one named multi-docker . The inbound rules should allow port range of 5432-6379 and source should be the newly created security group multi-docker . Set Up Elastic Beanstalk : Go to Elastic Beanstalk service and create application with the following config Name as multi-docker Platform as Docker Platform Branch as Multi Container Docker running on 64bit Amazon Linux After creating the environment, go to configuration and edit instances security by adding the security group multi-docker Set Up RDS Postgres Instance : Go to AWS RDS service and create database of Postgres instance with following configuration, Identifier as multi-docker-postgres Username as postgres Master Password as postgrespassword Initial database name from the Additional Settings should be fibvalues After creation of DB instance, modify the network security by adding security group multi-docker Set Up Elastic Cache Redis Instance : Go to Elastic Cache service and create a redis instance with following configuration, Name should be multi-docker-redis Node type as cache.t2.micro and replicas 0 per shard for less pricing After creating the instance, from action add the security group multi-docker Generate a IAM User With Appropriate Roles : For simplicity of the IAM user existing policies, search elasticbeanstalk and mark all the services. This will provide an AWS Access Key and AWS Secret Key . This key and secret has to be provided to the Travis cI for invoking the Elastic Beanstalk . Update Travis CI Config File For Production Deployment We have left two config for the production deployment. Notify the Elastic Beanstalk , a new changes being happen in the codebase Push the entire project to Elastic Beanstalk Although we are pushing the whole codebase to Elastic Beanstalk , the only file ELB care about is, Dockerrun.aws.json . From Dockerrun.aws.json , ELB download all the images from the docker hub and will run the container. Oue final .travis.yml with deploy configuration should be look like the following, sudo: required services: - docker before_install: - docker build -t bmshamsnahid/react-test -f ./client/Dockerfile.dev ./client script: - docker run -e CI=true bmshamsnahid/react-test npm test -- --coverage after_success: - docker build -t bmshamsnahid/multi-client ./client - docker build -t bmshamsnahid/multi-nginx ./nginx - docker build -t bmshamsnahid/multi-server ./server - docker build -t bmshamsnahid/multi-worker ./worker - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker push bmshamsnahid/multi-client - docker push bmshamsnahid/multi-nginx - docker push bmshamsnahid/multi-server - docker push bmshamsnahid/multi-worker deploy: provider: elasticbeanstalk region: \"ap-south-1\" app: \"multi-docker\" env: \"MultiDocker-env\" bucket_name: \"elasticbeanstalk-ap-south-1-366735605679\" bucket_path: \"docker-multi\" on: branch: master access_key_id: $AWS_ACCESS_KEY secret_access_key: $AWS_SECRET_KEY In the Travis CI dashboard, select the environment and from options of the multi-container-ci-cd repository, add the following environment variables, AWS_ACCESS_KEY AWS_SECRET_KEY Now we make a commit and push the changed code to the master branch, Travis CI should automatically ensure the continuous integration and Elastic Beanstalk should ensure continuous deployment. After successful deployment, the Elastic Beanstalk environment should be show Green success check. Our application should be automatically deployed to the Elastic Beanstalk . Cleaning Up AWS Resources Along the way, we have been using the following services, Elastic Beanstalk RDS Service Elastic Cache (Managed Redis) Security Group IAM User with necessary permissions Deleting Elastic Beanstalk : Go to the Elastic Beanstalk Service and select the multi-docker environment From action select the Terminate Application Deleting RDS Service : Go to RDS service Select the multi-docker-postgres and from action select Delete Deleting Elastic Cache : Go to Elastic Cache From redis select our instance multi-docker-redis Select and from action, click the Delete option Deleting Security Group (Optional) : Go to VPC service and from left panel select Security Groups Delete the security group named multi-docker and all its associates if there any Deleting IAM Users (Optional) : Go to IAM service and delete the user we have created For security groups, we might not need to delete them, as they are not billing service. Same goes for IAM user, it is not included in a billing service, but good to delete if not necessary.","title":"03 Multi container Continuous Deployment"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/#multi-container-continuous-deployment","text":"Our Travis CI si already configured to build the image and push the image to the Docker Hub . Now we have to think about how use these images and deploy them to production. To deploy these image we are going to make use of Elastic Beanstalk . When we have only one container, the Elastic Beanstalk will automatically build and run the the container, we do not have to set up any custom configuration. In the root directory, we just have to take the project files and a Dockerfile and Elastic Beanstalk do the rest. Now, the scenario is different. we have multiple Dockerfile in different folder. Anytime we have multiple Dockerfile , we have to tell the Elastic Beanstalk with little configuration, how these Dockerfile will be treated. To put the configuration for Elastic Beanstalk , how our multiple Dockerfile will be treated, we have to create special config file in the project directory named Dockerrun.aws.json . This config file will define, From where the image files will be pulled Resources allocated for the image Port mapping Some associated configurations like handling environment variables The configurations of Dockerrun.aws.json will be very much similar to the docker-compose.yml configuration. docker-compose.yml is all about, how we build images, whereas Dockerrun.aws.json is all about definitions of container. When it comes to handle multiple container, the Elastic Beanstalk does not know, how to handle multiple containers. For multiple container, it delegates the tasks to another AWS server named Elastic Container Service aka ECS . In the ECS for each container, we will define Task , also known as Task Definition . A doc for defining the container configuration is given here .","title":"Multi-container Continuous Deployment"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/#container-definition","text":"Go to the project root directory and create a file named Dockerrun.aws.json , touch Dockerrun.aws.json For the Dockerrun.aws.json file, we have to consider the following conventions, version : We have to specify the template version. Different version compile differently List of containers Definitions For each container name : Name should be the directory name image : Image name that is deployed to the docker hub with the docker hub user name hostname : Hostname should be the service name. This is also being used in the nginx configuration essential : If this is true, crashing this container will make crash other containers. Among all the container, one should marked as essential memory : Need to define the allocated memory in mega bytes, required for a container links : To do the routing we have to use the directory name For the worker and nginx, since no one is routing to these, we do not need any hostname in these configuration. Since, the nginx server is responsible for communicating with the outside world, we need to do the port mapping. Also, in the nginx server configuration we have to specify the routes to other containers It's challenging to allocate exactly essential memory for each of the services. Traditionally there are couple of stackoverflow posts, can be used to find out the desired memory allocation. Our Dockerrun.aws.json should be like the following, { \"AWSEBDockerrunVersion\": 2, \"containerDefinitions\": [ { \"name\": \"client\", \"image\": \"bmshamsnahid/multi-client\", \"hostname\": \"client\", \"essential\": false, \"memory\": 128 }, { \"name\": \"server\", \"image\": \"bmshamsnahid/multi-server\", \"hostname\": \"api\", \"essential\": false, \"memory\": 128 }, { \"name\": \"worker\", \"image\": \"bmshamsnahid/multi-worker\", \"hostname\": \"worker\", \"essential\": false, \"memory\": 128 }, { \"name\": \"nginx\", \"image\": \"bmshamsnahid/multi-nginx\", \"hostname\": \"nginx\", \"essential\": true, \"portMappings\": [ { \"hostPort\": 80, \"containerPort\": 80 } ], \"links\": [\"client\", \"server\"], \"memory\": 128 } ] }","title":"Container Definition"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/#using-managed-db-service-in-production","text":"In development, we are using the redis and postgres in our own development machine. But for production, we might need to consider a managed version of redis aka AWS Elastic Cache and for postgres we will make use of AWS Relational Database Service . Advantages of AWS Elastic Cache and AWS Relational Database Service , Managed creation and maintenance Easy scaling policy Built in logging Better security Easy migration Multi AZ configuration Additionally AWS Relational Database Service has some advantages, Database backup and rollback facility Easy tuning on Multi AZ and Read Replicas","title":"Using Managed DB Service in Production"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/#set-up-managed-services","text":"Let's build our cloud infrastructure to the using manged AWS Services . Along with the AWS Elastic Beanstalk we will make use of AWS RDS Instance AWS Elastic Cache Set Up Associated Security Group : Go to VPC section and from security group create one named multi-docker . The inbound rules should allow port range of 5432-6379 and source should be the newly created security group multi-docker . Set Up Elastic Beanstalk : Go to Elastic Beanstalk service and create application with the following config Name as multi-docker Platform as Docker Platform Branch as Multi Container Docker running on 64bit Amazon Linux After creating the environment, go to configuration and edit instances security by adding the security group multi-docker Set Up RDS Postgres Instance : Go to AWS RDS service and create database of Postgres instance with following configuration, Identifier as multi-docker-postgres Username as postgres Master Password as postgrespassword Initial database name from the Additional Settings should be fibvalues After creation of DB instance, modify the network security by adding security group multi-docker Set Up Elastic Cache Redis Instance : Go to Elastic Cache service and create a redis instance with following configuration, Name should be multi-docker-redis Node type as cache.t2.micro and replicas 0 per shard for less pricing After creating the instance, from action add the security group multi-docker Generate a IAM User With Appropriate Roles : For simplicity of the IAM user existing policies, search elasticbeanstalk and mark all the services. This will provide an AWS Access Key and AWS Secret Key . This key and secret has to be provided to the Travis cI for invoking the Elastic Beanstalk .","title":"Set Up Managed Services"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/#update-travis-ci-config-file-for-production-deployment","text":"We have left two config for the production deployment. Notify the Elastic Beanstalk , a new changes being happen in the codebase Push the entire project to Elastic Beanstalk Although we are pushing the whole codebase to Elastic Beanstalk , the only file ELB care about is, Dockerrun.aws.json . From Dockerrun.aws.json , ELB download all the images from the docker hub and will run the container. Oue final .travis.yml with deploy configuration should be look like the following, sudo: required services: - docker before_install: - docker build -t bmshamsnahid/react-test -f ./client/Dockerfile.dev ./client script: - docker run -e CI=true bmshamsnahid/react-test npm test -- --coverage after_success: - docker build -t bmshamsnahid/multi-client ./client - docker build -t bmshamsnahid/multi-nginx ./nginx - docker build -t bmshamsnahid/multi-server ./server - docker build -t bmshamsnahid/multi-worker ./worker - echo \"$DOCKER_PASSWORD\" | docker login -u \"$DOCKER_ID\" --password-stdin - docker push bmshamsnahid/multi-client - docker push bmshamsnahid/multi-nginx - docker push bmshamsnahid/multi-server - docker push bmshamsnahid/multi-worker deploy: provider: elasticbeanstalk region: \"ap-south-1\" app: \"multi-docker\" env: \"MultiDocker-env\" bucket_name: \"elasticbeanstalk-ap-south-1-366735605679\" bucket_path: \"docker-multi\" on: branch: master access_key_id: $AWS_ACCESS_KEY secret_access_key: $AWS_SECRET_KEY In the Travis CI dashboard, select the environment and from options of the multi-container-ci-cd repository, add the following environment variables, AWS_ACCESS_KEY AWS_SECRET_KEY Now we make a commit and push the changed code to the master branch, Travis CI should automatically ensure the continuous integration and Elastic Beanstalk should ensure continuous deployment. After successful deployment, the Elastic Beanstalk environment should be show Green success check. Our application should be automatically deployed to the Elastic Beanstalk .","title":"Update Travis CI Config File For Production Deployment"},{"location":"Notes/02 CI-CD With Docker/02  multi-container/03 Multi-container Continuous Deployment/#cleaning-up-aws-resources","text":"Along the way, we have been using the following services, Elastic Beanstalk RDS Service Elastic Cache (Managed Redis) Security Group IAM User with necessary permissions Deleting Elastic Beanstalk : Go to the Elastic Beanstalk Service and select the multi-docker environment From action select the Terminate Application Deleting RDS Service : Go to RDS service Select the multi-docker-postgres and from action select Delete Deleting Elastic Cache : Go to Elastic Cache From redis select our instance multi-docker-redis Select and from action, click the Delete option Deleting Security Group (Optional) : Go to VPC service and from left panel select Security Groups Delete the security group named multi-docker and all its associates if there any Deleting IAM Users (Optional) : Go to IAM service and delete the user we have created For security groups, we might not need to delete them, as they are not billing service. Same goes for IAM user, it is not included in a billing service, but good to delete if not necessary.","title":"Cleaning Up AWS Resources"},{"location":"Notes/03 Running container Like A Boss/01 Intro/","text":"Using Container Like A Boss Containers are the fundamental building blocks of Docker tool-kit. Before we get stared, we have to make sure the latest version of Docker is installed in the system. It is important to keep in mind, Docker is different from the VM . In this article, we are going to discuss, how Docker is different from VM , play with Nginx server container and look into basic networking of Docker . Before we dive, let's check the Docker Version in our system. docker version With this command, we should get an output, something like this, Client: Docker Engine - Community Version: 20.10.5 API version: 1.41 Go version: go1.13.15 Git commit: 55c4c88 Built: Tue Mar 2 20:18:20 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.5 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: 363e9a8 Built: Tue Mar 2 20:16:15 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.4 GitCommit: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc: Version: 1.0.0-rc93 GitCommit: 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init: Version: 0.19.0 GitCommit: de40ad0 If you get the Docker server information, that means we can talk to the server. Otherwise, there is something wrong with the Docker installation, like not installed properly, having permission error etc. Since, we are going to a lot of new features, ideally you should get the latest version as possible. Additionally, to get more information about the installed Docker server, we can run, docker info Commands and Management Commands If we want to look all the available commands for Docker , we can run, docker From this output, we can notice two sections Management Commands Commands Previously, all the Docker commands was available like docker command . Since the number of commands increase, the Docker team decided to separate them in sum-commands aka Management Commands . With Management Commands , we run commands like docker <command> <sub-command> . Old Way: docker <command> (options) New Way: docker <command> <sub-command> (options) All the old commands are working fine with latest Docker . But Docker is pushing forward to use the new model Management Command . Image Vs Container An image is an application we want to run. On the other hand, a Container is an instance of the image . Essentially, we can have multiple container running of the same image.","title":"Using Container Like A Boss"},{"location":"Notes/03 Running container Like A Boss/01 Intro/#using-container-like-a-boss","text":"Containers are the fundamental building blocks of Docker tool-kit. Before we get stared, we have to make sure the latest version of Docker is installed in the system. It is important to keep in mind, Docker is different from the VM . In this article, we are going to discuss, how Docker is different from VM , play with Nginx server container and look into basic networking of Docker . Before we dive, let's check the Docker Version in our system. docker version With this command, we should get an output, something like this, Client: Docker Engine - Community Version: 20.10.5 API version: 1.41 Go version: go1.13.15 Git commit: 55c4c88 Built: Tue Mar 2 20:18:20 2021 OS/Arch: linux/amd64 Context: default Experimental: true Server: Docker Engine - Community Engine: Version: 20.10.5 API version: 1.41 (minimum version 1.12) Go version: go1.13.15 Git commit: 363e9a8 Built: Tue Mar 2 20:16:15 2021 OS/Arch: linux/amd64 Experimental: false containerd: Version: 1.4.4 GitCommit: 05f951a3781f4f2c1911b05e61c160e9c30eaa8e runc: Version: 1.0.0-rc93 GitCommit: 12644e614e25b05da6fd08a38ffa0cfe1903fdec docker-init: Version: 0.19.0 GitCommit: de40ad0 If you get the Docker server information, that means we can talk to the server. Otherwise, there is something wrong with the Docker installation, like not installed properly, having permission error etc. Since, we are going to a lot of new features, ideally you should get the latest version as possible. Additionally, to get more information about the installed Docker server, we can run, docker info","title":"Using Container Like A Boss"},{"location":"Notes/03 Running container Like A Boss/01 Intro/#commands-and-management-commands","text":"If we want to look all the available commands for Docker , we can run, docker From this output, we can notice two sections Management Commands Commands Previously, all the Docker commands was available like docker command . Since the number of commands increase, the Docker team decided to separate them in sum-commands aka Management Commands . With Management Commands , we run commands like docker <command> <sub-command> . Old Way: docker <command> (options) New Way: docker <command> <sub-command> (options) All the old commands are working fine with latest Docker . But Docker is pushing forward to use the new model Management Command .","title":"Commands and Management Commands"},{"location":"Notes/03 Running container Like A Boss/01 Intro/#image-vs-container","text":"An image is an application we want to run. On the other hand, a Container is an instance of the image . Essentially, we can have multiple container running of the same image.","title":"Image Vs Container"},{"location":"Notes/03 Running container Like A Boss/02 Play With Nginx Server/","text":"Play With Nginx Server Run a Container To play around the Docker Container , we will use Nginx Server Image . We will pull it from the Docker Registry aka Docker Hub and run in our local machine. Go to terminal and run docker container run --publish 80:80 nginx In browser, if we go to http://localhost/ , we should see the Nginx server is up and running. With this command docker container run --publish 80:80 nginx , The Docker engine looks for the image for the Nginx web server and if not found, pull it from the docker hub Run the Nginx web server Expose our local machine port 80 and server all the traffic to the 80 port of the Nginx server To stop the container, we can simply try Ctrl + c . We can run the container in background, by using detach flag. docker container run --publish 80:80 --detach nginx This will run the Docker container in background. We can look all the containers list by docker container ls We can stop the container by docker container stop <container-id> Now, if we look the container list by docker container ls , the list should be empty. To observe all the stopped and running containers we can do, docker container ls -a In the last column of the list, we see random name of the Container . We can define the name while creating and running a container by, docker container run --publish 80:80 --detach --name <container-name> nginx Now, if we print the list of running container by docker container ls , we will notice the new container with our defined name. We might want to look for the logs of the Docker container, by docker container logs <container-name> There are couple of options, while looking for logs, We can remove docker containers by, docker container rm <container-id-1> <container-id-2> This will only remove the stopped container. To stop running containers, we can first stop the running container and then then remove. Also, we can use force flag to remove the running container by, docker container rm -f <container-id> This will remove the running container. With Docker , in a matter a of time, we are able to run a Nginx server with default configuration. We are also able to look the logs and eventually stop the container.","title":"Play With Nginx Server"},{"location":"Notes/03 Running container Like A Boss/02 Play With Nginx Server/#play-with-nginx-server","text":"","title":"Play With Nginx Server"},{"location":"Notes/03 Running container Like A Boss/02 Play With Nginx Server/#run-a-container","text":"To play around the Docker Container , we will use Nginx Server Image . We will pull it from the Docker Registry aka Docker Hub and run in our local machine. Go to terminal and run docker container run --publish 80:80 nginx In browser, if we go to http://localhost/ , we should see the Nginx server is up and running. With this command docker container run --publish 80:80 nginx , The Docker engine looks for the image for the Nginx web server and if not found, pull it from the docker hub Run the Nginx web server Expose our local machine port 80 and server all the traffic to the 80 port of the Nginx server To stop the container, we can simply try Ctrl + c . We can run the container in background, by using detach flag. docker container run --publish 80:80 --detach nginx This will run the Docker container in background. We can look all the containers list by docker container ls We can stop the container by docker container stop <container-id> Now, if we look the container list by docker container ls , the list should be empty. To observe all the stopped and running containers we can do, docker container ls -a In the last column of the list, we see random name of the Container . We can define the name while creating and running a container by, docker container run --publish 80:80 --detach --name <container-name> nginx Now, if we print the list of running container by docker container ls , we will notice the new container with our defined name. We might want to look for the logs of the Docker container, by docker container logs <container-name> There are couple of options, while looking for logs, We can remove docker containers by, docker container rm <container-id-1> <container-id-2> This will only remove the stopped container. To stop running containers, we can first stop the running container and then then remove. Also, we can use force flag to remove the running container by, docker container rm -f <container-id> This will remove the running container. With Docker , in a matter a of time, we are able to run a Nginx server with default configuration. We are also able to look the logs and eventually stop the container.","title":"Run a Container"},{"location":"Notes/03 Running container Like A Boss/03 With Container Run/","text":"With Container Run When we run the container using docker container run --publish 80:80 --name <container-name> nginx , it actually interpret by docker container run --publish 80:80 --name <container-name> -d nginx:latest nginx -T . This command do the following steps: Look for Nginx image in local cache, if not found get it from Docker registry. If we specify the image version, it will be downloaded If we do not specify the image, it will download the latest version Create a container out of the image Give a virtual IP on a private network inside the Docker Engine Open port 80 in host machine and forward all traffic of 80 to docker container port 80 Start executing container start up command So while run the container using the specified command, we can change the port mapping and also the image version.","title":"With Container Run"},{"location":"Notes/03 Running container Like A Boss/03 With Container Run/#with-container-run","text":"When we run the container using docker container run --publish 80:80 --name <container-name> nginx , it actually interpret by docker container run --publish 80:80 --name <container-name> -d nginx:latest nginx -T . This command do the following steps: Look for Nginx image in local cache, if not found get it from Docker registry. If we specify the image version, it will be downloaded If we do not specify the image, it will download the latest version Create a container out of the image Give a virtual IP on a private network inside the Docker Engine Open port 80 in host machine and forward all traffic of 80 to docker container port 80 Start executing container start up command So while run the container using the specified command, we can change the port mapping and also the image version.","title":"With Container Run"},{"location":"Notes/03 Running container Like A Boss/04 Container vs VM/","text":"Container vs VM Container is just a process. We can observe this specific process from the host machine and even stop anytime.","title":"Container vs VM"},{"location":"Notes/03 Running container Like A Boss/04 Container vs VM/#container-vs-vm","text":"Container is just a process. We can observe this specific process from the host machine and even stop anytime.","title":"Container vs VM"},{"location":"Notes/03 Running container Like A Boss/05 Windows Containers/","text":"Windows Containers Docker container is no longer mean to be Linux Container . Since 2017, windows server 2016, introduce the Windows Container . And they are .exe binaries run on the windows machine without linux being installed.","title":"Windows Containers"},{"location":"Notes/03 Running container Like A Boss/05 Windows Containers/#windows-containers","text":"Docker container is no longer mean to be Linux Container . Since 2017, windows server 2016, introduce the Windows Container . And they are .exe binaries run on the windows machine without linux being installed.","title":"Windows Containers"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/","text":"Assignment of Multiple Containers We will do the following task: No networking between containers are required Run Nginx server With detach mode With a defined name proxy At port 80 Run mysql server With detach mode With a defined name db At port 3306 Invoke to generate random password and find it from logs Run apache server With detach mode With a defined name web_server At port 8080 Nginx We can run Nginx by docker container run -d --name proxy -p 80:80 nginx The server should run in http://localhost/ Ref: 1. image 2. Docker run 3. Docker Detach 4. Assign Name 5. Expose Port MySQL We can run mySQL by docker run -d --name db -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql We can look for logs docker logs In logs, we should find the root password GENERATED ROOT PASSWORD: auto_generated_password Ref: Image from Docker Hub MYSQL_RANDOM_ROOT_PASSWORD environment name Docker Hub Passing environment Apache (httpd) We can run the apache server by, docker container run -d --name web_server -p 8080:80 httpd The server should run in http://localhost:8080/ . Ref: image Stopping all the containers We can list down the containers by, docker container ls We can stop all these containers, docker container stop <nginx-container-id> <mysql-container-id> <nginx-container-id> Now docker container ls should return list of containers as stopped status. Delete all the images We can list down the containers by, docker container ls We can remove all these containers, docker rm -f <nginx-container-id> <mysql-container-id> <nginx-container-id> Now docker container ls should return empty list.","title":"06 Assignment of Multiple Containers"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/#assignment-of-multiple-containers","text":"We will do the following task: No networking between containers are required Run Nginx server With detach mode With a defined name proxy At port 80 Run mysql server With detach mode With a defined name db At port 3306 Invoke to generate random password and find it from logs Run apache server With detach mode With a defined name web_server At port 8080","title":"Assignment of Multiple Containers"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/#nginx","text":"We can run Nginx by docker container run -d --name proxy -p 80:80 nginx The server should run in http://localhost/ Ref: 1. image 2. Docker run 3. Docker Detach 4. Assign Name 5. Expose Port","title":"Nginx"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/#mysql","text":"We can run mySQL by docker run -d --name db -p 3306:3306 -e MYSQL_RANDOM_ROOT_PASSWORD=yes mysql We can look for logs docker logs In logs, we should find the root password GENERATED ROOT PASSWORD: auto_generated_password Ref: Image from Docker Hub MYSQL_RANDOM_ROOT_PASSWORD environment name Docker Hub Passing environment","title":"MySQL"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/#apache-httpd","text":"We can run the apache server by, docker container run -d --name web_server -p 8080:80 httpd The server should run in http://localhost:8080/ . Ref: image","title":"Apache (httpd)"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/#stopping-all-the-containers","text":"We can list down the containers by, docker container ls We can stop all these containers, docker container stop <nginx-container-id> <mysql-container-id> <nginx-container-id> Now docker container ls should return list of containers as stopped status.","title":"Stopping all the containers"},{"location":"Notes/03 Running container Like A Boss/06 Assignment of Multiple Containers/#delete-all-the-images","text":"We can list down the containers by, docker container ls We can remove all these containers, docker rm -f <nginx-container-id> <mysql-container-id> <nginx-container-id> Now docker container ls should return empty list.","title":"Delete all the images"},{"location":"Notes/03 Running container Like A Boss/07 CLI Process Monitoring/","text":"CLI Process Monitoring Using docker-cli , for containers, we will test the followings, Running processes of a container Container metadata Performance stats of running container Before we start, let's start two container, nginx and mysql , docker container run -d --name nginx nginx docker container run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql Running Processes To check the running process of a container we can use the top command. top display running processes of a container. docker container top mysql This will show the running process of mysql container. Inspect Container Metadata inspect returns low level information and metadata of docker container, like startup config volume mapping networking docker container inspect mysql Monitoring running container stats We can use stats to stream the running container stats, like CPU usage, memory usage etc. docker container stats","title":"07 CLI Process Monitoring"},{"location":"Notes/03 Running container Like A Boss/07 CLI Process Monitoring/#cli-process-monitoring","text":"Using docker-cli , for containers, we will test the followings, Running processes of a container Container metadata Performance stats of running container Before we start, let's start two container, nginx and mysql , docker container run -d --name nginx nginx docker container run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql","title":"CLI Process Monitoring"},{"location":"Notes/03 Running container Like A Boss/07 CLI Process Monitoring/#running-processes","text":"To check the running process of a container we can use the top command. top display running processes of a container. docker container top mysql This will show the running process of mysql container.","title":"Running Processes"},{"location":"Notes/03 Running container Like A Boss/07 CLI Process Monitoring/#inspect-container-metadata","text":"inspect returns low level information and metadata of docker container, like startup config volume mapping networking docker container inspect mysql","title":"Inspect Container Metadata"},{"location":"Notes/03 Running container Like A Boss/07 CLI Process Monitoring/#monitoring-running-container-stats","text":"We can use stats to stream the running container stats, like CPU usage, memory usage etc. docker container stats","title":"Monitoring running container stats"},{"location":"Notes/03 Running container Like A Boss/08 Getting a Shell Inside a Container/","text":"Getting a Shell Inside a Container To interact in the container, we need to get inside the container. We can consider 2 scenarios here, Get interactive terminal in container Run a program inside a container Interactive Terminal on Container Startup -it is a combination of two flag i and t . They provide an standard input output along with a terminal for a container. To get an interactive terminal for nginx server, we can run, docker container run -it --name proxy nginx bash We can exit using exit We can do detail experiments for ubuntu os, docker container run -it --name ubuntu ubuntu Now in the interactive terminal, first update the packages and install curl apt-get update apt-get install -y curl We can use curl from the terminal. curl google.com We can exit from the terminal by exit If we again want to get the terminal in the running ubuntu machine, docker container start ubuntu -ai Run Program Inside Container We can run program inside the container using the exec command. Let's run mysql inside a container, docker container run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql In the mysql container, there is a preinstalled program, called bash . We can execute the bash and get access to the interactive terminal. docker container exec -it mysql bash Let's get image of alpine from the docker-hub , docker pull alpine Since, the bash is not installed in the alpine and we try to run it inside the container, we will get an error, the program is not avaiable. docker container run -it alpine bash In alpine there is another program called sh with similar functionality. We can run sh in the alpine by, docker container run -it alpine sh Alpine is a minimal featured, security focused linux distribution.","title":"08 Getting a Shell Inside a Container"},{"location":"Notes/03 Running container Like A Boss/08 Getting a Shell Inside a Container/#getting-a-shell-inside-a-container","text":"To interact in the container, we need to get inside the container. We can consider 2 scenarios here, Get interactive terminal in container Run a program inside a container","title":"Getting a Shell Inside a Container"},{"location":"Notes/03 Running container Like A Boss/08 Getting a Shell Inside a Container/#interactive-terminal-on-container-startup","text":"-it is a combination of two flag i and t . They provide an standard input output along with a terminal for a container. To get an interactive terminal for nginx server, we can run, docker container run -it --name proxy nginx bash We can exit using exit We can do detail experiments for ubuntu os, docker container run -it --name ubuntu ubuntu Now in the interactive terminal, first update the packages and install curl apt-get update apt-get install -y curl We can use curl from the terminal. curl google.com We can exit from the terminal by exit If we again want to get the terminal in the running ubuntu machine, docker container start ubuntu -ai","title":"Interactive Terminal on Container Startup"},{"location":"Notes/03 Running container Like A Boss/08 Getting a Shell Inside a Container/#run-program-inside-container","text":"We can run program inside the container using the exec command. Let's run mysql inside a container, docker container run -d --name mysql -e MYSQL_RANDOM_ROOT_PASSWORD=true mysql In the mysql container, there is a preinstalled program, called bash . We can execute the bash and get access to the interactive terminal. docker container exec -it mysql bash Let's get image of alpine from the docker-hub , docker pull alpine Since, the bash is not installed in the alpine and we try to run it inside the container, we will get an error, the program is not avaiable. docker container run -it alpine bash In alpine there is another program called sh with similar functionality. We can run sh in the alpine by, docker container run -it alpine sh Alpine is a minimal featured, security focused linux distribution.","title":"Run Program Inside Container"},{"location":"Notes/03 Running container Like A Boss/09 IP Fact/","text":"IP Fact An Interesting IP Fact Let's run nginx server on port 80 , docker container run -p 80:80 --name webhost -d nginx In the container, we can check the PORT of the container by, docker container port webhost I got output of, 80/tcp -> 0.0.0.0:80 Since, we are mapping the port in container at 80 , this seems fine. Let's check the IP of the container, docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost In my machine, I got container IP, 172.17.0.3 Now, to check my host machine IP, first install a tool named net-tools . sudo apt-get install -y net-tools Assuming the net-tools is installed in the machine, I checked the IP of my host machine, ifconfig usb0 I got my host machine IP as output, 192.168.42.203 It seems, the container IP 172.17.0.3 and host machine IP 192.168.42.203 is not same.","title":"09 IP Fact"},{"location":"Notes/03 Running container Like A Boss/09 IP Fact/#ip-fact","text":"","title":"IP Fact"},{"location":"Notes/03 Running container Like A Boss/09 IP Fact/#an-interesting-ip-fact","text":"Let's run nginx server on port 80 , docker container run -p 80:80 --name webhost -d nginx In the container, we can check the PORT of the container by, docker container port webhost I got output of, 80/tcp -> 0.0.0.0:80 Since, we are mapping the port in container at 80 , this seems fine. Let's check the IP of the container, docker container inspect --format '{{ .NetworkSettings.IPAddress }}' webhost In my machine, I got container IP, 172.17.0.3 Now, to check my host machine IP, first install a tool named net-tools . sudo apt-get install -y net-tools Assuming the net-tools is installed in the machine, I checked the IP of my host machine, ifconfig usb0 I got my host machine IP as output, 192.168.42.203 It seems, the container IP 172.17.0.3 and host machine IP 192.168.42.203 is not same.","title":"An Interesting IP Fact"},{"location":"Notes/03 Running container Like A Boss/10 Container Networking/","text":"Container Networking While we start a container with -p flag, it exposes a port from the host machine to the docker container. In background, a lot more is going on, we will discuss here. This networking staff is plugable. Under the hood, We can add a container to a container, remover form a network. When we start a container, we are particularly behind a docker network, called bridge-network . This bridge-network routes through NAT Firewall of the host IP. It is configured by the docker-daemon on our behalf. So the container can go back and forth of the outside internet and other network. But whenever we need to build a communication between specific containers we do not need any port mapping using -p . If we have a network with a node application container and a mongoDB container, and they need to connect each other, we do not have to do the port mapping or open the port to the rest of the physical network. And if we have another network with php server container and mysql database container, they can communicate each other but can not communicate with node and mongoDB servers network. With this setup, if the php server have to connect with the node server , it has to go through the host machine network. All these configurations are configurable, can be add, removed or changed. We can not have two container listening at the same port at the host level Figure Networking CLI Hands On Here we will create two docker network with containers in it. Then play around of adding, removing containers of these networks. Objective We will create two container, ix_nginx_1 and ix_nginx_2 . The ix_nginx_1 should connect to a custom network ix_network . On the other hand, the ix_nginx_2 should connect to the both networks, ix_network and the default docker network. Networking Setup We can get the list of networks by, docker network ls This will output the list of network. bridge is the default docker network. host is the network, skip docker-virtual-network and attach to host network directly. Disadvantages is, this skip default container security but could be useful for high throughput networking. null is not attached to anything. To see the details of the default network, docker network inspect bridge In the output, under containers , a list of containers under the default bridge network will be displayed. To create our own docker network named ix_network , docker network create ix_network This will create the network and return the network id. We can verify this network existence by looking at the list of network by, docker network ls In output list, there should be a network named ix_network . It is using default driver bridge. Now, we will create a nginx container named ix_nginx_1 that should go under our new custom network, ix_network , docker container run -d --name ix_nginx_1 --network ix_network nginx To check, if the new container is running under our new created network, docker network inspect ix_network In the containers property, the container named ix_nginx_1 should appear. Now, create another container named ix_nginx_2 under the default network, docker container run -d --name ix_nginx_2 nginx If we inspect the default network, in the containers property, the container, ix_nginx_2 should appear. docker network inspect bridge Verify Setup Make sure, ix_nginx_1 container is inside the ix_network network, docker network inspect ix_network We can inspect it through the ix_network network inspection Or inspecting the container itself, docker container inspect ix_nginx_1 ix_nginx_2 container is inside the default network We can inspect it through the default network inspection, docker network inspect bridge Or inspecting the container itself, docker container inspect ix_nginx_2 Experiments We can add the ix_nginx_2 to the ix_network network using the connect command. Now connect ix_nginx_2 to the ix_network network docker network connect ix_network ix_nginx_2 We can verify, ix_nginx_2 is connected to two networks by, docker container inspect ix_nginx_2 Under the NetworkSettings.Networks property, there should be the default bridge network and ix_network . We can disconnect ix_nginx_2 from the ix_network by disconnect command, docker network disconnect ix_network ix_nginx_2 The beauty of containerization is using networking, even though we run all the app in a same server, we can protect them separately. DNS In the world of containers, there is constant change of containers like, launching, stopping, expanding, shrinking etc. Containers can go away, fail on runtime, can crash. In these cases, docker will bring them up with a new IP. Since things are so dynamic and complicated, We can not relay on IP addresses, or deal with IP addresses inside the container. For this, docker provide a built-in solution, DNS-Naming . Objective Figure We can get list of containers, docker container ls Make sure, the container, ix_nginx_1 is running under the ix_network . If not run them by, docker container start ix_nginx_1 And verify the ix_network contains the container ix_nginx_1 docker network inspect ix_network Let's create another container ix_dns_nginx inside the ix_network , docker container run -d --name ix_dns_nginx --network ix_network nginx Docker default bridge network does not support built in DNS Resolution . Since ix_nginx_1 and ix_dns_nginx are not the under default bridge network, it has the built in special feature, DNS Resolution . Now, let's again check, the ix_nginx_1 and ix_dns_nginx are in the same network ix_network , docker network inspect ix_network Under the Containers property, both containers should appear. Lets, try to ping ix_nginx_1 from the ix_dns_nginx container, by docker container exec -it ix_dns_nginx ping ix_nginx_1 It is possible the latest nginx container does not have the ping program pre-installed. In this case, it will throw an error like this, OCI runtime exec failed: exec failed: container_linux.go:367: starting container process caused: exec: \"ping\": executable file not found in $PATH: unknown If you notice this error, first install the ping program in the ix_dns_nginx container. Run bash in the nginx container, docker container exec -it ix_dns_nginx bash From the bash , update the package manager and install the ping command, apt-get update apt-get install iputils-ping Now we can ping the ix_nginx_1 from the bash by, ping ix_nginx_1 If you ping from the terminal first exit from the bash, exit Now from your own terminal run, docker container exec -it ix_dns_nginx ping ix_nginx_1 We can ping another container in the same network without IP. The resolution works in both ways, we can ping ix_dns_nginx from the ix_nginx_1 server also. This makes super easy when we need to talk from one container to another container. These containers IP address may not be same but their container name or the host names will always be the same. The default docker bridge network has a disadvantages. It does not have built in DNS server. In this case we have to use the --link flag. It's comparatively easier to use the custom network for this purpose, instead of using the default bridge network. Using docker-compose , we can automatically spin up a virtual network for us.","title":"10 Container Networking"},{"location":"Notes/03 Running container Like A Boss/10 Container Networking/#container-networking","text":"While we start a container with -p flag, it exposes a port from the host machine to the docker container. In background, a lot more is going on, we will discuss here. This networking staff is plugable. Under the hood, We can add a container to a container, remover form a network. When we start a container, we are particularly behind a docker network, called bridge-network . This bridge-network routes through NAT Firewall of the host IP. It is configured by the docker-daemon on our behalf. So the container can go back and forth of the outside internet and other network. But whenever we need to build a communication between specific containers we do not need any port mapping using -p . If we have a network with a node application container and a mongoDB container, and they need to connect each other, we do not have to do the port mapping or open the port to the rest of the physical network. And if we have another network with php server container and mysql database container, they can communicate each other but can not communicate with node and mongoDB servers network. With this setup, if the php server have to connect with the node server , it has to go through the host machine network. All these configurations are configurable, can be add, removed or changed. We can not have two container listening at the same port at the host level Figure","title":"Container Networking"},{"location":"Notes/03 Running container Like A Boss/10 Container Networking/#networking-cli-hands-on","text":"Here we will create two docker network with containers in it. Then play around of adding, removing containers of these networks. Objective We will create two container, ix_nginx_1 and ix_nginx_2 . The ix_nginx_1 should connect to a custom network ix_network . On the other hand, the ix_nginx_2 should connect to the both networks, ix_network and the default docker network. Networking Setup We can get the list of networks by, docker network ls This will output the list of network. bridge is the default docker network. host is the network, skip docker-virtual-network and attach to host network directly. Disadvantages is, this skip default container security but could be useful for high throughput networking. null is not attached to anything. To see the details of the default network, docker network inspect bridge In the output, under containers , a list of containers under the default bridge network will be displayed. To create our own docker network named ix_network , docker network create ix_network This will create the network and return the network id. We can verify this network existence by looking at the list of network by, docker network ls In output list, there should be a network named ix_network . It is using default driver bridge. Now, we will create a nginx container named ix_nginx_1 that should go under our new custom network, ix_network , docker container run -d --name ix_nginx_1 --network ix_network nginx To check, if the new container is running under our new created network, docker network inspect ix_network In the containers property, the container named ix_nginx_1 should appear. Now, create another container named ix_nginx_2 under the default network, docker container run -d --name ix_nginx_2 nginx If we inspect the default network, in the containers property, the container, ix_nginx_2 should appear. docker network inspect bridge Verify Setup Make sure, ix_nginx_1 container is inside the ix_network network, docker network inspect ix_network We can inspect it through the ix_network network inspection Or inspecting the container itself, docker container inspect ix_nginx_1 ix_nginx_2 container is inside the default network We can inspect it through the default network inspection, docker network inspect bridge Or inspecting the container itself, docker container inspect ix_nginx_2 Experiments We can add the ix_nginx_2 to the ix_network network using the connect command. Now connect ix_nginx_2 to the ix_network network docker network connect ix_network ix_nginx_2 We can verify, ix_nginx_2 is connected to two networks by, docker container inspect ix_nginx_2 Under the NetworkSettings.Networks property, there should be the default bridge network and ix_network . We can disconnect ix_nginx_2 from the ix_network by disconnect command, docker network disconnect ix_network ix_nginx_2 The beauty of containerization is using networking, even though we run all the app in a same server, we can protect them separately.","title":"Networking CLI Hands On"},{"location":"Notes/03 Running container Like A Boss/10 Container Networking/#dns","text":"In the world of containers, there is constant change of containers like, launching, stopping, expanding, shrinking etc. Containers can go away, fail on runtime, can crash. In these cases, docker will bring them up with a new IP. Since things are so dynamic and complicated, We can not relay on IP addresses, or deal with IP addresses inside the container. For this, docker provide a built-in solution, DNS-Naming . Objective Figure We can get list of containers, docker container ls Make sure, the container, ix_nginx_1 is running under the ix_network . If not run them by, docker container start ix_nginx_1 And verify the ix_network contains the container ix_nginx_1 docker network inspect ix_network Let's create another container ix_dns_nginx inside the ix_network , docker container run -d --name ix_dns_nginx --network ix_network nginx Docker default bridge network does not support built in DNS Resolution . Since ix_nginx_1 and ix_dns_nginx are not the under default bridge network, it has the built in special feature, DNS Resolution . Now, let's again check, the ix_nginx_1 and ix_dns_nginx are in the same network ix_network , docker network inspect ix_network Under the Containers property, both containers should appear. Lets, try to ping ix_nginx_1 from the ix_dns_nginx container, by docker container exec -it ix_dns_nginx ping ix_nginx_1 It is possible the latest nginx container does not have the ping program pre-installed. In this case, it will throw an error like this, OCI runtime exec failed: exec failed: container_linux.go:367: starting container process caused: exec: \"ping\": executable file not found in $PATH: unknown If you notice this error, first install the ping program in the ix_dns_nginx container. Run bash in the nginx container, docker container exec -it ix_dns_nginx bash From the bash , update the package manager and install the ping command, apt-get update apt-get install iputils-ping Now we can ping the ix_nginx_1 from the bash by, ping ix_nginx_1 If you ping from the terminal first exit from the bash, exit Now from your own terminal run, docker container exec -it ix_dns_nginx ping ix_nginx_1 We can ping another container in the same network without IP. The resolution works in both ways, we can ping ix_dns_nginx from the ix_nginx_1 server also. This makes super easy when we need to talk from one container to another container. These containers IP address may not be same but their container name or the host names will always be the same. The default docker bridge network has a disadvantages. It does not have built in DNS server. In this case we have to use the --link flag. It's comparatively easier to use the custom network for this purpose, instead of using the default bridge network. Using docker-compose , we can automatically spin up a virtual network for us.","title":"DNS"},{"location":"Notes/03 Running container Like A Boss/11 Assignments/","text":"Assignments Check the curl version from two container, created from Cent OS 7 Ubuntu 14.04 Remove all these containers Cent OS Create container from Cent OS 7. Cent OS with version 7 can be found docker hub . docker container run -it centos:7 bash We can check curl version from bash by curl --version This return the curl version of 7.29.0 Ubuntu Create container from ubuntu 14.04. The ubuntu image of version 14.04 can be found in docker-hub docker container run -it ubuntu:14.04 bash By default, the curl is not installed. To install curl , first update the packages by, apt-get update Now, install the curl by, apt-get install -y curl Since the curl is installed, check the version, curl --version We should get version of curl as 7.35.0 . Remove Containers To remove containers, we can use docker rm -f <cent_os_7_container_id> <ubuntu_14_04_container_id> Apparently, If we used --rm flag while starting the container, we do not have to remove these container manually.","title":"11 Assignments"},{"location":"Notes/03 Running container Like A Boss/11 Assignments/#assignments","text":"Check the curl version from two container, created from Cent OS 7 Ubuntu 14.04 Remove all these containers","title":"Assignments"},{"location":"Notes/03 Running container Like A Boss/11 Assignments/#cent-os","text":"Create container from Cent OS 7. Cent OS with version 7 can be found docker hub . docker container run -it centos:7 bash We can check curl version from bash by curl --version This return the curl version of 7.29.0","title":"Cent OS"},{"location":"Notes/03 Running container Like A Boss/11 Assignments/#ubuntu","text":"Create container from ubuntu 14.04. The ubuntu image of version 14.04 can be found in docker-hub docker container run -it ubuntu:14.04 bash By default, the curl is not installed. To install curl , first update the packages by, apt-get update Now, install the curl by, apt-get install -y curl Since the curl is installed, check the version, curl --version We should get version of curl as 7.35.0 .","title":"Ubuntu"},{"location":"Notes/03 Running container Like A Boss/11 Assignments/#remove-containers","text":"To remove containers, we can use docker rm -f <cent_os_7_container_id> <ubuntu_14_04_container_id> Apparently, If we used --rm flag while starting the container, we do not have to remove these container manually.","title":"Remove Containers"},{"location":"Notes/03 Running container Like A Boss/12 Assignment/","text":"Assignments Here we will use same host name for multiple docker container. We will run two container from elastic-search named elasticsearch1 and elasticsearch2 . Both container should have same dns name, search . From alpine, we will run nslookup and ensure, the search is the dns name of both container. From centos we will verify. search:9200 work similar to load balancer between these two containers. Create 2 containers from elasticsearch:2 Use --network-alias search while creating to provide additional DNS name Create container from alpine:3.10 From alpine use --net to see, both containers with same DNS name Create container from centos curl to search:9200 and see both name field shows Check network list. docker network ls Create a custom network named dns_rest , we will do all the work. docker network create dns_res Make sure network is created and in containers, no containers is connected. docker network inspect dns_res Create two container in the network dns_res , with named elastic_search1 , and elastic_search2 should go under network dns_res should have network alias search docker container run --name elastic_search_1 --network dns_res --network-alias search -d elasticsearch:2 docker container run --name elastic_search_2 --network dns_res --network-alias search -d elasticsearch:2 Check the network and these two container should be under the dns_res network. docker network inspect dns_res Run a container of alpine and open shell inside, docker container run --network dns_res -it alpine:3.10 sh Check the two container is exist in the search hostname using nslookup tool nslookup search This should give output like, Name: search Address 1: 172.25.0.2 elastic_search_1.dns_res Address 2: 172.25.0.3 elastic_search_2.dns_res Now create a container of centos and open terminal in it, docker container run -it --network dns_res centos:7 Run the following couple of times, only these two containers should appear randomly. curl -s search:9200","title":"12 Assignment"},{"location":"Notes/03 Running container Like A Boss/12 Assignment/#assignments","text":"Here we will use same host name for multiple docker container. We will run two container from elastic-search named elasticsearch1 and elasticsearch2 . Both container should have same dns name, search . From alpine, we will run nslookup and ensure, the search is the dns name of both container. From centos we will verify. search:9200 work similar to load balancer between these two containers. Create 2 containers from elasticsearch:2 Use --network-alias search while creating to provide additional DNS name Create container from alpine:3.10 From alpine use --net to see, both containers with same DNS name Create container from centos curl to search:9200 and see both name field shows Check network list. docker network ls Create a custom network named dns_rest , we will do all the work. docker network create dns_res Make sure network is created and in containers, no containers is connected. docker network inspect dns_res Create two container in the network dns_res , with named elastic_search1 , and elastic_search2 should go under network dns_res should have network alias search docker container run --name elastic_search_1 --network dns_res --network-alias search -d elasticsearch:2 docker container run --name elastic_search_2 --network dns_res --network-alias search -d elasticsearch:2 Check the network and these two container should be under the dns_res network. docker network inspect dns_res Run a container of alpine and open shell inside, docker container run --network dns_res -it alpine:3.10 sh Check the two container is exist in the search hostname using nslookup tool nslookup search This should give output like, Name: search Address 1: 172.25.0.2 elastic_search_1.dns_res Address 2: 172.25.0.3 elastic_search_2.dns_res Now create a container of centos and open terminal in it, docker container run -it --network dns_res centos:7 Run the following couple of times, only these two containers should appear randomly. curl -s search:9200","title":"Assignments"},{"location":"Notes/04 Images/01 Caching/","text":"Caching Image Layers Image History We can look for the image history by, docker image history nginx This should output the nginx image history. We get a list of image layers. All the images start from a scratch. Each layer contains the increment of the previous one. It could be changes in file system or adding some config/metadata to the image . We can have on layer, we can even have thousands of layers. Figure Image Layers We never store the same image layer twice in the system. Figure Copy On Write Image Inspection We can closely inspect the image, docker image inspect nginx Image inspections give us metadata, some interesting config we can look for, exposed ports env variables start up command","title":"01 Caching"},{"location":"Notes/04 Images/01 Caching/#caching","text":"","title":"Caching"},{"location":"Notes/04 Images/01 Caching/#image-layers","text":"Image History We can look for the image history by, docker image history nginx This should output the nginx image history. We get a list of image layers. All the images start from a scratch. Each layer contains the increment of the previous one. It could be changes in file system or adding some config/metadata to the image . We can have on layer, we can even have thousands of layers. Figure Image Layers We never store the same image layer twice in the system. Figure Copy On Write","title":"Image Layers"},{"location":"Notes/04 Images/01 Caching/#image-inspection","text":"We can closely inspect the image, docker image inspect nginx Image inspections give us metadata, some interesting config we can look for, exposed ports env variables start up command","title":"Image Inspection"},{"location":"Notes/04 Images/02 Tagging/","text":"Tagging we can get the list of images by, docker image ls In the image list, we can notice docker images does not have a name. Only official images can have root namespace. Other images repository name be like user_name/repository_name The image tag is not exactly a version or branch, it's the git tag and it can represent both. In docker world, latest tag is more of a meaning to default or stable image. latest tag in docker image does not imply the latest image. It is possible to take older image and tagged as latest Let's assume, we have the nginx image with latest tag. If not get it, docker pull nginx:latest The latest nginx also tagged with mainline . So If we try to pull the nginx:mainline , docker pull nginx:mainline Instead of download, it will use the downloaded nginx:latest image layer. Now, if we print the list of images, docker image ls We will see the nginx image has same IMAGE ID but different tag. Tag Existing Image We can create a tagged target image from the existing image by tag command, docker image tag nginx bmshamsnahid/nginx If we now see the image list, docker image ls We will see a new nginx image repository bmshamsnahid/nginx with latest image. This new image should also have the same image id. If we do not specify the tag, it will take the latest tag by default Pushing Image to Docker Hub Make sure you have a account in docker hub and you are logged in to your local machine, docker login Now, push our latest image to docker hub, docker push bmshamsnahid/nginx In docker hub, we should see the image bmshamsnahid/nginx with latest tag. Now to add another tag, testing , we can run, docker tag bmshamsnahid/nginx bmshamsnahid/nginx:testing We can verify the testing tag in local image list by, docker image ls In the output list, we should see a image bmshamsnahid/nginx with testing tag. We can push this newly testing tagged image to docker hub by, docker push bmshamsnahid/nginx:testing Since the image layer is same, only the tag is different, we should see Layer already exists in the console. Also it should add a image tag testing in the docker hub. So if an image layer exist in the docker hub, it does not upload twice. And same for the local machine, if an image layer exists in the local machine cache, it does not download twice. To upload a image in private repository, we first have to create a private repository in the docker hub","title":"02 Tagging"},{"location":"Notes/04 Images/02 Tagging/#tagging","text":"we can get the list of images by, docker image ls In the image list, we can notice docker images does not have a name. Only official images can have root namespace. Other images repository name be like user_name/repository_name The image tag is not exactly a version or branch, it's the git tag and it can represent both. In docker world, latest tag is more of a meaning to default or stable image. latest tag in docker image does not imply the latest image. It is possible to take older image and tagged as latest Let's assume, we have the nginx image with latest tag. If not get it, docker pull nginx:latest The latest nginx also tagged with mainline . So If we try to pull the nginx:mainline , docker pull nginx:mainline Instead of download, it will use the downloaded nginx:latest image layer. Now, if we print the list of images, docker image ls We will see the nginx image has same IMAGE ID but different tag.","title":"Tagging"},{"location":"Notes/04 Images/02 Tagging/#tag-existing-image","text":"We can create a tagged target image from the existing image by tag command, docker image tag nginx bmshamsnahid/nginx If we now see the image list, docker image ls We will see a new nginx image repository bmshamsnahid/nginx with latest image. This new image should also have the same image id. If we do not specify the tag, it will take the latest tag by default","title":"Tag Existing Image"},{"location":"Notes/04 Images/02 Tagging/#pushing-image-to-docker-hub","text":"Make sure you have a account in docker hub and you are logged in to your local machine, docker login Now, push our latest image to docker hub, docker push bmshamsnahid/nginx In docker hub, we should see the image bmshamsnahid/nginx with latest tag. Now to add another tag, testing , we can run, docker tag bmshamsnahid/nginx bmshamsnahid/nginx:testing We can verify the testing tag in local image list by, docker image ls In the output list, we should see a image bmshamsnahid/nginx with testing tag. We can push this newly testing tagged image to docker hub by, docker push bmshamsnahid/nginx:testing Since the image layer is same, only the tag is different, we should see Layer already exists in the console. Also it should add a image tag testing in the docker hub. So if an image layer exist in the docker hub, it does not upload twice. And same for the local machine, if an image layer exists in the local machine cache, it does not download twice. To upload a image in private repository, we first have to create a private repository in the docker hub","title":"Pushing Image to Docker Hub"},{"location":"Notes/04 Images/03 Building Image/","text":"Building Image Dockerfile is a recipe for building a docker image. Any docker images we used, is created from the dockerfile. Best Practices More changes should go below Less changes should stay top","title":"03 Building Image"},{"location":"Notes/04 Images/03 Building Image/#building-image","text":"Dockerfile is a recipe for building a docker image. Any docker images we used, is created from the dockerfile.","title":"Building Image"},{"location":"Notes/04 Images/03 Building Image/#best-practices","text":"More changes should go below Less changes should stay top","title":"Best Practices"},{"location":"Notes/04 Images/04 Assignment/","text":"Assignment In this assignment, objective is create a Dockerfile that can run a node app. Make sure Dockerfile run a node app in local environment Push the image to docker hub Remove local images from local machine cache Get the image from docker-hub and run the node app again Create a node app in a directory named, dockerfile-assignment-1 , mkdir dockerfile-assignment-1 # creating a directory for the node app cd dockerfile-assignment-1 # go to the directory npm init -y # create a node app Now create a Hello World app using express.js from example , Our package.json file should similar, { \"name\": \"dockerfile-assignment-1\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"dependencies\": { \"express\": \"*\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\" } And the server file will be index.js , touch index.js Our index.js file should be similar to the example const express = require('express'); const app = express(); const port = 3000; app.get('/', (req, res) => { res.send('Hello World!'); }); app.listen(port, () => console.log(`Example app listening at http://localhost:${port}`) ); Now create a Dockerfile , touch Dockerfile Our dockerfile should be like, FROM node:15.14.0 WORKDIR app COPY package.json package.json RUN npm install COPY . . EXPOSE 3000 CMD [\"node\", \"index.js\"] Now build a image and run the container from it, docker build . -t node-app && docker run -p 80:3000 node-app Now we should see the server response from browser by browsing http://127.0.0.1/ . To push the docker image to the docker hub it should have a repository name of user_name/repository_name . We can tagged the image to the docker hub format by, docker tag node-app bmshamsnahid/node-app We got the exact format to push the image to docker-hub . We can verify this by, docker image ls We should see the image with repository name bmshamsnahid/node-app . Push the image to docker hub by, docker push bmshamsnahid/node-app:latest We again run the node app from the image that is now in the docker hub. To do so, first remove the local image and running container. Remove images from local machine cache, docker image rm -f node-app bmshamsnahid/node-app Stop and remove the container, docker container stop <container_id> Now, get the image from docker-hub and run a container from it, docker container run --rm -p 80:3000 bmshamsnahid/node-app This should run our node app and should be access in http://127.0.0.1/","title":"04 Assignment"},{"location":"Notes/04 Images/04 Assignment/#assignment","text":"In this assignment, objective is create a Dockerfile that can run a node app. Make sure Dockerfile run a node app in local environment Push the image to docker hub Remove local images from local machine cache Get the image from docker-hub and run the node app again Create a node app in a directory named, dockerfile-assignment-1 , mkdir dockerfile-assignment-1 # creating a directory for the node app cd dockerfile-assignment-1 # go to the directory npm init -y # create a node app Now create a Hello World app using express.js from example , Our package.json file should similar, { \"name\": \"dockerfile-assignment-1\", \"version\": \"1.0.0\", \"description\": \"\", \"main\": \"index.js\", \"scripts\": { \"test\": \"echo \\\"Error: no test specified\\\" && exit 1\" }, \"dependencies\": { \"express\": \"*\" }, \"keywords\": [], \"author\": \"\", \"license\": \"ISC\" } And the server file will be index.js , touch index.js Our index.js file should be similar to the example const express = require('express'); const app = express(); const port = 3000; app.get('/', (req, res) => { res.send('Hello World!'); }); app.listen(port, () => console.log(`Example app listening at http://localhost:${port}`) ); Now create a Dockerfile , touch Dockerfile Our dockerfile should be like, FROM node:15.14.0 WORKDIR app COPY package.json package.json RUN npm install COPY . . EXPOSE 3000 CMD [\"node\", \"index.js\"] Now build a image and run the container from it, docker build . -t node-app && docker run -p 80:3000 node-app Now we should see the server response from browser by browsing http://127.0.0.1/ . To push the docker image to the docker hub it should have a repository name of user_name/repository_name . We can tagged the image to the docker hub format by, docker tag node-app bmshamsnahid/node-app We got the exact format to push the image to docker-hub . We can verify this by, docker image ls We should see the image with repository name bmshamsnahid/node-app . Push the image to docker hub by, docker push bmshamsnahid/node-app:latest We again run the node app from the image that is now in the docker hub. To do so, first remove the local image and running container. Remove images from local machine cache, docker image rm -f node-app bmshamsnahid/node-app Stop and remove the container, docker container stop <container_id> Now, get the image from docker-hub and run a container from it, docker container run --rm -p 80:3000 bmshamsnahid/node-app This should run our node app and should be access in http://127.0.0.1/","title":"Assignment"},{"location":"Notes/05 Volumes/01 Data Persistent/","text":"Data Persistent Containers are ephemeral by design, once stopped any data written in the container will be erased. This is not a limitation of the container, more of a design goal. This is the idea of immutable infrastructure, we do not change things once system is running. For changes, we redeploy the whole new container. But the tradeoff is the dataset, generated by the app itself. If we redeploy the whole container the unique dataset inside the container will be lost. The separation of concern suggest, we should not mix the data set with the application binaries. Docker gives us two big favors here, Data Volumes Special location outside the Union file system Under Dockers storage directory Docker sees and manages this as a local file/folder Bind Mounts Mount a host directory file/folder into docker container Processes outside the docker can modify this Volumes and Bind Mounts are both outside the container, but Volumes are under docker engine. According to the Docker doc, Bind mounts have been around since the early days of Docker. Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container. The file or directory is referenced by its full or relative path on the host machine. By contrast, when you use a volume, a new directory is created within Docker\u2019s storage directory on the host machine, and Docker manages that directory\u2019s contents.","title":"01 Data Persistent"},{"location":"Notes/05 Volumes/01 Data Persistent/#data-persistent","text":"Containers are ephemeral by design, once stopped any data written in the container will be erased. This is not a limitation of the container, more of a design goal. This is the idea of immutable infrastructure, we do not change things once system is running. For changes, we redeploy the whole new container. But the tradeoff is the dataset, generated by the app itself. If we redeploy the whole container the unique dataset inside the container will be lost. The separation of concern suggest, we should not mix the data set with the application binaries. Docker gives us two big favors here, Data Volumes Special location outside the Union file system Under Dockers storage directory Docker sees and manages this as a local file/folder Bind Mounts Mount a host directory file/folder into docker container Processes outside the docker can modify this Volumes and Bind Mounts are both outside the container, but Volumes are under docker engine. According to the Docker doc, Bind mounts have been around since the early days of Docker. Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container. The file or directory is referenced by its full or relative path on the host machine. By contrast, when you use a volume, a new directory is created within Docker\u2019s storage directory on the host machine, and Docker manages that directory\u2019s contents.","title":"Data Persistent"},{"location":"Notes/05 Volumes/02 Data Volumes/","text":"Data Volumes If we use Data Volumes to persist data, we have to define a volume. Then any files or data put in that volume will outlive the container. If we remove the container, it will not remove the data volume, instead we have to take one more step to remove the volume, docker volume prune . This extra step is just for an insurance, to ensure the the data volumes are much more important than the container itself. Let's pull the mysql image, docker pull mysql Now, inspect the image, docker image inspect mysql Inside the Config.Volumes , we should see a volumes, /var/lib/mysql . We now run a container from the image, docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql We can verify the container is running by, docker container ls In the output list, we should see the mysql container is running. We can inspect the container, docker container inspect mysql Under Mounts , we should see /var/lib/mysql as mounted volumes list. When a container is running, the container assume the directory path is /var/lib/mysql , but it's actual path is defined in the Source property and in my case, the actual path is /var/lib/docker/volumes/0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13/_data . We can see the list of mounted volumes, docker volume ls This should give all the mounted volumes by the docker containers. We can get the volume name by the docker container inspect mysql . From this command output, under Mounts , we can get the mounted volumes name. To inspect our specific volume, we can run inspect command by the name, docker volume inspect 0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13 In output, there is a property Mountpoint and from the linux machine, we can directly access that mount point, cd /var/lib/docker/volumes/0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13/_data This should take us the _data directory. Lets, remove the mysql container, docker container rm mysql Now, if we look for the volumes, docker volume ls We will notice, even though our container is removed, our data is persisted. This solves our data persistency problem. Although, from container, we can find the mounted volumes, but from volume perspective, we can not say which container it is connected to. Named Volumes (An Enhance of Data Volumes) To make Data Volumes more user friendly, we can use Named Volumes . To use Named Volumes we have to specify the volume config on the docker run command with -v flag. We can create a mysql container with Named Volume by, docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=true -v mysql-db:/var/lib/mysql mysql Now we can easily inspect the volume by name, docker volume inspect mysql-db This will give the details of the mysql-db platform. It is also possible to create volume ahead of time.","title":"02 Data Volumes"},{"location":"Notes/05 Volumes/02 Data Volumes/#data-volumes","text":"If we use Data Volumes to persist data, we have to define a volume. Then any files or data put in that volume will outlive the container. If we remove the container, it will not remove the data volume, instead we have to take one more step to remove the volume, docker volume prune . This extra step is just for an insurance, to ensure the the data volumes are much more important than the container itself. Let's pull the mysql image, docker pull mysql Now, inspect the image, docker image inspect mysql Inside the Config.Volumes , we should see a volumes, /var/lib/mysql . We now run a container from the image, docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql We can verify the container is running by, docker container ls In the output list, we should see the mysql container is running. We can inspect the container, docker container inspect mysql Under Mounts , we should see /var/lib/mysql as mounted volumes list. When a container is running, the container assume the directory path is /var/lib/mysql , but it's actual path is defined in the Source property and in my case, the actual path is /var/lib/docker/volumes/0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13/_data . We can see the list of mounted volumes, docker volume ls This should give all the mounted volumes by the docker containers. We can get the volume name by the docker container inspect mysql . From this command output, under Mounts , we can get the mounted volumes name. To inspect our specific volume, we can run inspect command by the name, docker volume inspect 0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13 In output, there is a property Mountpoint and from the linux machine, we can directly access that mount point, cd /var/lib/docker/volumes/0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13/_data This should take us the _data directory. Lets, remove the mysql container, docker container rm mysql Now, if we look for the volumes, docker volume ls We will notice, even though our container is removed, our data is persisted. This solves our data persistency problem. Although, from container, we can find the mounted volumes, but from volume perspective, we can not say which container it is connected to.","title":"Data Volumes"},{"location":"Notes/05 Volumes/02 Data Volumes/#named-volumes-an-enhance-of-data-volumes","text":"To make Data Volumes more user friendly, we can use Named Volumes . To use Named Volumes we have to specify the volume config on the docker run command with -v flag. We can create a mysql container with Named Volume by, docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=true -v mysql-db:/var/lib/mysql mysql Now we can easily inspect the volume by name, docker volume inspect mysql-db This will give the details of the mysql-db platform. It is also possible to create volume ahead of time.","title":"Named Volumes (An Enhance of Data Volumes)"},{"location":"Notes/05 Volumes/03 Bind Mounting/","text":"Bind Mounting With Bind Mounting , in local environment, we can edit files in the host machine and that is used inside the container. Bind mounting is mapping mapping host files and directories into container files and directories. It skips the union file system and on delete a container, do not erase the data from host machine. If we have files that is mapped to the host files and also exist in the container, in this case the host files will be used. To test the Bind Mounting we will use the nginx image. First create a file index.html as follows, touch index.html Make our index.html file as simple as possible, <!DOCTYPE html> <html> <head> <title>Docker Volumes Testing</title> </head> <body> Hello From Bind Mounting </body> </html> Now create a container of the nginx image to use this index.html , docker container run -d --name nginx -p 80:80 -v \"$(pwd)\":/usr/share/nginx/html nginx Now if we browse http://localhost/ from browser, we should see the the nginx server is serving our index.html instead of the container itself. If you are in the windows machine, instead of \"$(pwd)\" , for PowerShell use ${pwd} and for cmd.exe \"Command Prompt use: %cd% If we change the content of the index.html and reload the browser with address http://localhost/ , we should see the new content from the index.html .","title":"03 Bind Mounting"},{"location":"Notes/05 Volumes/03 Bind Mounting/#bind-mounting","text":"With Bind Mounting , in local environment, we can edit files in the host machine and that is used inside the container. Bind mounting is mapping mapping host files and directories into container files and directories. It skips the union file system and on delete a container, do not erase the data from host machine. If we have files that is mapped to the host files and also exist in the container, in this case the host files will be used. To test the Bind Mounting we will use the nginx image. First create a file index.html as follows, touch index.html Make our index.html file as simple as possible, <!DOCTYPE html> <html> <head> <title>Docker Volumes Testing</title> </head> <body> Hello From Bind Mounting </body> </html> Now create a container of the nginx image to use this index.html , docker container run -d --name nginx -p 80:80 -v \"$(pwd)\":/usr/share/nginx/html nginx Now if we browse http://localhost/ from browser, we should see the the nginx server is serving our index.html instead of the container itself. If you are in the windows machine, instead of \"$(pwd)\" , for PowerShell use ${pwd} and for cmd.exe \"Command Prompt use: %cd% If we change the content of the index.html and reload the browser with address http://localhost/ , we should see the new content from the index.html .","title":"Bind Mounting"},{"location":"Notes/05 Volumes/04 Assignment: Named Volumes/","text":"Assignment: Named Volumes An exercise for the Named Volumes can be database upgrade. For example, we are using a postgres of version 12.6 . Now there is a update to 13.2 . In this case, if we want to upgrade database, we should not loose the existing data. Steps Run container with postgres 12.6 Check the volumes Check logs and stop container Create another container with postgres 13.6 with same named volume Check logs to validate To do so, when we create a database, we first have to define a Named Volumes and when do the upgrade, we have to specify the data directory. Let's first create a database with legacy version and specify the Named Volumes , docker run --name postgres_legacy -d -e POSTGRES_HOST_AUTH_METHOD=trust -v postgres_data:/var/lib/postgresql/data postgres:12.6 Here POSTGRES_HOST_AUTH_METHOD=trust to allow all connections without a password. It's not a recommended approach. By default postgres put data in the /var/lib/postgresql/data directory. We can keep watching the logs of the server, docker container logs -f postgres_legacy Here, -f allow to keep watching the logs. We should see database system is ready to accept connections in the database logs. We can look at the volume list, docker volume ls We should see a volume named postgres_data . To upgrade to 13.2 , we first stop the previous version, docker container stop postgres_legacy Now create a new postgres server with updated version and configured the same Named Volume , docker run --name postgres_new -d -e POSTGRES_HOST_AUTH_METHOD=trust -v postgres_data:/var/lib/postgresql/data postgres:13.2 Since, we are using existing data directory, our server will start quickly with few logs, docker container logs postgres_new If we look into the volumes, we should there is no additional volume for the new updated postgres server, it is using the same as legacy one, postgres_data docker volume ls","title":"04 Assignment: Named Volumes"},{"location":"Notes/05 Volumes/04 Assignment: Named Volumes/#assignment-named-volumes","text":"An exercise for the Named Volumes can be database upgrade. For example, we are using a postgres of version 12.6 . Now there is a update to 13.2 . In this case, if we want to upgrade database, we should not loose the existing data. Steps Run container with postgres 12.6 Check the volumes Check logs and stop container Create another container with postgres 13.6 with same named volume Check logs to validate To do so, when we create a database, we first have to define a Named Volumes and when do the upgrade, we have to specify the data directory. Let's first create a database with legacy version and specify the Named Volumes , docker run --name postgres_legacy -d -e POSTGRES_HOST_AUTH_METHOD=trust -v postgres_data:/var/lib/postgresql/data postgres:12.6 Here POSTGRES_HOST_AUTH_METHOD=trust to allow all connections without a password. It's not a recommended approach. By default postgres put data in the /var/lib/postgresql/data directory. We can keep watching the logs of the server, docker container logs -f postgres_legacy Here, -f allow to keep watching the logs. We should see database system is ready to accept connections in the database logs. We can look at the volume list, docker volume ls We should see a volume named postgres_data . To upgrade to 13.2 , we first stop the previous version, docker container stop postgres_legacy Now create a new postgres server with updated version and configured the same Named Volume , docker run --name postgres_new -d -e POSTGRES_HOST_AUTH_METHOD=trust -v postgres_data:/var/lib/postgresql/data postgres:13.2 Since, we are using existing data directory, our server will start quickly with few logs, docker container logs postgres_new If we look into the volumes, we should there is no additional volume for the new updated postgres server, it is using the same as legacy one, postgres_data docker volume ls","title":"Assignment: Named Volumes"},{"location":"Notes/06 Docker Compose/01 Overview/","text":"Overview Docker container is a single solution process. In real world, we have to deal with multiple containers like, web server, proxy server, worker process, frontend, database server etc. We have to virtual networking, expose public ports and overall orchestrate each pieces. And this is exactly what docker-compose does for us. With docker-compose we can orchestrate the services and spin them up in the local and test environment. Docker compose consists of two separate but related things, Docker compose YML file, we used to define containers, networking and volumes. docker-compose CLI, a cli-tool used in the local dev or test environment to automate these YAML file to simplify our docker commands. docker-compose.yml docker-compose.yml file solely responsible for automation in local dev or test environment. Although now it is possible to use docker compose as docker commands (Ex, v1.13 or latest can be used in Swarm ). Default name for docker compose file is docker-compose.yml . We can use custom names and use it with -f flag. In yml file we can use indentation of 2 or 4 spaces version : Each yml file has own version. If we do not specify the version, it is assumed the docker-compose.yml file is under v1 . It is recommended to use at least v2 . services : Containers, same as docker run command. servicename : A user friendly name of the container. Also used as DNS name under the virtual network. image : Specify a image the container will build from. Can be redis , mysql etc. command : Override the container start up command. environment : Pass the container environment variables. For example, when we run a database container, we may pass the password through this environment. volumes : Do the volume mapping like Named Volume or Bind Mounting . A sample docker-compose.yml file can be as follows, version: '3.1' services: servicename: image: command: environment: volumes: servicename: volumes: network:","title":"01 Overview"},{"location":"Notes/06 Docker Compose/01 Overview/#overview","text":"Docker container is a single solution process. In real world, we have to deal with multiple containers like, web server, proxy server, worker process, frontend, database server etc. We have to virtual networking, expose public ports and overall orchestrate each pieces. And this is exactly what docker-compose does for us. With docker-compose we can orchestrate the services and spin them up in the local and test environment. Docker compose consists of two separate but related things, Docker compose YML file, we used to define containers, networking and volumes. docker-compose CLI, a cli-tool used in the local dev or test environment to automate these YAML file to simplify our docker commands.","title":"Overview"},{"location":"Notes/06 Docker Compose/01 Overview/#docker-composeyml","text":"docker-compose.yml file solely responsible for automation in local dev or test environment. Although now it is possible to use docker compose as docker commands (Ex, v1.13 or latest can be used in Swarm ). Default name for docker compose file is docker-compose.yml . We can use custom names and use it with -f flag. In yml file we can use indentation of 2 or 4 spaces version : Each yml file has own version. If we do not specify the version, it is assumed the docker-compose.yml file is under v1 . It is recommended to use at least v2 . services : Containers, same as docker run command. servicename : A user friendly name of the container. Also used as DNS name under the virtual network. image : Specify a image the container will build from. Can be redis , mysql etc. command : Override the container start up command. environment : Pass the container environment variables. For example, when we run a database container, we may pass the password through this environment. volumes : Do the volume mapping like Named Volume or Bind Mounting . A sample docker-compose.yml file can be as follows, version: '3.1' services: servicename: image: command: environment: volumes: servicename: volumes: network:","title":"docker-compose.yml"},{"location":"Notes/06 Docker Compose/02 Example/","text":"Docker Compose Example Here we will use docker compose to spin up a Nginx server and a Apache server. The Nginx server will be use as a proxy server to redirect traffic to the Apache server. To do so, first create a Nginx server configuration that will be used to redirect the traffic named nginx.conf , The nginx.conf file should be, server { listen 80; location / { proxy_pass http://web; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } You might notice, we are passing the traffic of port 80 to http://web . Here web will be the DNS name of the Apache server in our docker-compose.yml file. Now, let's create the docker-compose.yml file, touch docker-compose.yml Our docker-compose.yml file should be as follows, version: '3' services: proxy: image: nginx:1.11 ports: - '80:80' volumes: - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro web: image: httpd Here, we do port mapping of Nginx server from host machine 80 to container 80 port. We are also doing a Bind Mount of nginx.conf , so this nginx.conf will be used in the container instead of the default configuration. In the services, we named Nginx server as proxy and Apache server as web . Here these proxy and web can be used as DNS name for these server. ro stands for read only and this property is optional We can run these container by, docker-compose up This will spin up all these server and in browser http://localhost/ , we should see It works! We can stop these containers by ctrl + c . To run containers in background, we can use -d flag, docker-compose up -d To check the running containers, docker-compose ps This should show Nginx and Apache server is running. With nice formatted output we can see all the services by, docker-compose top To clean up (stopped and removed) all the containers, docker-compose down","title":"02 Example"},{"location":"Notes/06 Docker Compose/02 Example/#docker-compose-example","text":"Here we will use docker compose to spin up a Nginx server and a Apache server. The Nginx server will be use as a proxy server to redirect traffic to the Apache server. To do so, first create a Nginx server configuration that will be used to redirect the traffic named nginx.conf , The nginx.conf file should be, server { listen 80; location / { proxy_pass http://web; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } You might notice, we are passing the traffic of port 80 to http://web . Here web will be the DNS name of the Apache server in our docker-compose.yml file. Now, let's create the docker-compose.yml file, touch docker-compose.yml Our docker-compose.yml file should be as follows, version: '3' services: proxy: image: nginx:1.11 ports: - '80:80' volumes: - ./nginx.conf:/etc/nginx/conf.d/default.conf:ro web: image: httpd Here, we do port mapping of Nginx server from host machine 80 to container 80 port. We are also doing a Bind Mount of nginx.conf , so this nginx.conf will be used in the container instead of the default configuration. In the services, we named Nginx server as proxy and Apache server as web . Here these proxy and web can be used as DNS name for these server. ro stands for read only and this property is optional We can run these container by, docker-compose up This will spin up all these server and in browser http://localhost/ , we should see It works! We can stop these containers by ctrl + c . To run containers in background, we can use -d flag, docker-compose up -d To check the running containers, docker-compose ps This should show Nginx and Apache server is running. With nice formatted output we can see all the services by, docker-compose top To clean up (stopped and removed) all the containers, docker-compose down","title":"Docker Compose Example"},{"location":"Notes/06 Docker Compose/03 Assignment 01/","text":"Running a Drupal Site Using Docker Compose In this assignment, we will run a Drupal site with postgres database. With docker-compose We need two container drupal and postgres By default the drupal run on port 80 , but we make sure from host machine we can access them from port 8080 To persist the content of the site, we will do Named Volume mapping for the drupal site For postgres we will pass Database Name , Database User and Database Password . They will be required when we have to install drupal site from the browser. Create a docker-compose.yml file, touch docker-compose.yml Our docker-compose.yml file should be as follows, version: '3.1' services: drupal: image: drupal:9.1.6 ports: - '8080:80' volumes: - drupal-modules:/var/www/html/modules - drupal-profiles:/var/www/html/profiles - drupal-sites:/var/www/html/sites - drupal-themes:/var/www/html/themes postgres: image: postgres:13.2 environment: - POSTGRES_DB=drupal - POSTGRES_USER=user - POSTGRES_PASSWORD=pass volumes: drupal-modules: drupal-profiles: drupal-sites: drupal-themes: Now run the containers, docker-compose up After all the containers spined up, we can go to http://localhost:8080/ . To set up, we have to select the postgres as it is using underline. Also, DB Name , DB User and DB Password should be same as the environments we passed through the docker-compose.yml file. By default the drupal expect the database at localhost , but our database DNS name is postgres . From the Advance Options we have to also update the database host name. To clean up all the containers along with the volumes, we can use docker-compose down -v","title":"03 Assignment 01"},{"location":"Notes/06 Docker Compose/03 Assignment 01/#running-a-drupal-site-using-docker-compose","text":"In this assignment, we will run a Drupal site with postgres database. With docker-compose We need two container drupal and postgres By default the drupal run on port 80 , but we make sure from host machine we can access them from port 8080 To persist the content of the site, we will do Named Volume mapping for the drupal site For postgres we will pass Database Name , Database User and Database Password . They will be required when we have to install drupal site from the browser. Create a docker-compose.yml file, touch docker-compose.yml Our docker-compose.yml file should be as follows, version: '3.1' services: drupal: image: drupal:9.1.6 ports: - '8080:80' volumes: - drupal-modules:/var/www/html/modules - drupal-profiles:/var/www/html/profiles - drupal-sites:/var/www/html/sites - drupal-themes:/var/www/html/themes postgres: image: postgres:13.2 environment: - POSTGRES_DB=drupal - POSTGRES_USER=user - POSTGRES_PASSWORD=pass volumes: drupal-modules: drupal-profiles: drupal-sites: drupal-themes: Now run the containers, docker-compose up After all the containers spined up, we can go to http://localhost:8080/ . To set up, we have to select the postgres as it is using underline. Also, DB Name , DB User and DB Password should be same as the environments we passed through the docker-compose.yml file. By default the drupal expect the database at localhost , but our database DNS name is postgres . From the Advance Options we have to also update the database host name. To clean up all the containers along with the volumes, we can use docker-compose down -v","title":"Running a Drupal Site Using Docker Compose"},{"location":"Notes/06 Docker Compose/04 Building Image/","text":"Building Image We can configure the docker-compose to build an image runtime and use it. If we configure docker-compose to build an image in runtime, it first check if the image already exist in the local machine cache. If the image does found in the local cache, it build one, put it in cache and use. With --build flag with docker-compose cli, we can ensure, the image will be built and will overwrite the cache image. Here, we build Nginx server to serve traffic of Apache server. To build custom Nginx image, we will need a Dockerfile . Let's create a Dockerfile named, nginx.Dockerfile , touch nginx.Dockerfile We will need a Nginx configuration file that will serve the traffic as proxy server for the Apache server. Create a configuration file named nginx.conf , touch nginx.conf Assuming that the DNS name for the Apache server will be web , our nginx.conf file should be as follows, server { listen 80; location / { proxy_pass http://web; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } Our nginx.Dockerfile should be as follows, FROM nginx:1.13 COPY nginx.conf /etc/nginx/conf.d/default.conf Now, we need a docker-compose file to orchestrate all these two containers. Create a file named docker-compose.yml , touch docker-compose.yml Our docker-compose.yml file should be as follows, version: '3.1' services: proxy: build: context: . dockerfile: nginx.Dockerfile image: 'proxy' ports: - '80:80' web: image: httpd volumes: - ./html:/user/local/apache2/htdocs/ For apache web server we do a Bind Mount to server html documents from the host machine. Let's create a directory html and create a file index.html , mkdir html cd html touch index.html Our index.html file can be as follows, Hello World Now, we can run all these containers using docker compose by, docker-compose up From browser, if we browse, http://localhost , we should see, Hello World . Since we have a Bind Mount of the html directory, we can change the content from the index.html and these changes should be visible in the browser with a refresh. If we again run docker-compose up , this time the proxy image will be taken from the local image cache. If we want to build the image instead of use it from the cache, we can run, docker-compose up --build To clean up all the containers along with the images, we can use the --rmi flag, docker-compose down --rmi local","title":"04 Building Image"},{"location":"Notes/06 Docker Compose/04 Building Image/#building-image","text":"We can configure the docker-compose to build an image runtime and use it. If we configure docker-compose to build an image in runtime, it first check if the image already exist in the local machine cache. If the image does found in the local cache, it build one, put it in cache and use. With --build flag with docker-compose cli, we can ensure, the image will be built and will overwrite the cache image. Here, we build Nginx server to serve traffic of Apache server. To build custom Nginx image, we will need a Dockerfile . Let's create a Dockerfile named, nginx.Dockerfile , touch nginx.Dockerfile We will need a Nginx configuration file that will serve the traffic as proxy server for the Apache server. Create a configuration file named nginx.conf , touch nginx.conf Assuming that the DNS name for the Apache server will be web , our nginx.conf file should be as follows, server { listen 80; location / { proxy_pass http://web; proxy_redirect off; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Host $server_name; } } Our nginx.Dockerfile should be as follows, FROM nginx:1.13 COPY nginx.conf /etc/nginx/conf.d/default.conf Now, we need a docker-compose file to orchestrate all these two containers. Create a file named docker-compose.yml , touch docker-compose.yml Our docker-compose.yml file should be as follows, version: '3.1' services: proxy: build: context: . dockerfile: nginx.Dockerfile image: 'proxy' ports: - '80:80' web: image: httpd volumes: - ./html:/user/local/apache2/htdocs/ For apache web server we do a Bind Mount to server html documents from the host machine. Let's create a directory html and create a file index.html , mkdir html cd html touch index.html Our index.html file can be as follows, Hello World Now, we can run all these containers using docker compose by, docker-compose up From browser, if we browse, http://localhost , we should see, Hello World . Since we have a Bind Mount of the html directory, we can change the content from the index.html and these changes should be visible in the browser with a refresh. If we again run docker-compose up , this time the proxy image will be taken from the local image cache. If we want to build the image instead of use it from the cache, we can run, docker-compose up --build To clean up all the containers along with the images, we can use the --rmi flag, docker-compose down --rmi local","title":"Building Image"},{"location":"Notes/06 Docker Compose/05 Assignment 02/","text":"Running Drupal With a Theme In It","title":"05 Assignment 02"},{"location":"Notes/06 Docker Compose/05 Assignment 02/#running-drupal-with-a-theme-in-it","text":"","title":"Running Drupal With a Theme In It"},{"location":"Notes/07 swarm/01 Overview/","text":"Overview With containerization technology we can deploy our apps as platform service beyond any hardware, whether it is AWS, GCS, Digital Ocean Droplet ect. But after deploying a container, we have to consider scaling the app. These brings new problems for a small organization or solo developer. With Docker Swarm we can manage the follows, Automate the container lifecycle Scaling in/out Restart or create container on failure Container upgrade (Blue/Green Deployment) Cross container networking Storing secrets, keys, passwords","title":"01 Overview"},{"location":"Notes/07 swarm/01 Overview/#overview","text":"With containerization technology we can deploy our apps as platform service beyond any hardware, whether it is AWS, GCS, Digital Ocean Droplet ect. But after deploying a container, we have to consider scaling the app. These brings new problems for a small organization or solo developer. With Docker Swarm we can manage the follows, Automate the container lifecycle Scaling in/out Restart or create container on failure Container upgrade (Blue/Green Deployment) Cross container networking Storing secrets, keys, passwords","title":"Overview"},{"location":"Notes/07 swarm/02 Swarm Service/","text":"Swarm Service docker info Swarm: inactive docker swarm init Swarm initialized: current node (x4cq8kn2ub0awaljjwgzn123q) is now a manager. docker node ls docker service replaces the docker run command. docker service create alpine 8.8.8.8 docker service ls docker service ps jolly_kirch docker container ls","title":"02 Swarm Service"},{"location":"Notes/07 swarm/02 Swarm Service/#swarm-service","text":"docker info Swarm: inactive docker swarm init Swarm initialized: current node (x4cq8kn2ub0awaljjwgzn123q) is now a manager. docker node ls docker service replaces the docker run command. docker service create alpine 8.8.8.8 docker service ls docker service ps jolly_kirch docker container ls","title":"Swarm Service"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/01 ECS vs Fargate/","text":"ECS vs Fargate ECS Elastic Container Service Launch Docker Containers on AWS Must provision and maintain the infrastructure. AWS only take care of starting and stopping the containers Integration of Load Balancers Fargate Serverless container service No need to provision any infrastructure","title":"01 ECS vs Fargate"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/01 ECS vs Fargate/#ecs-vs-fargate","text":"ECS Elastic Container Service Launch Docker Containers on AWS Must provision and maintain the infrastructure. AWS only take care of starting and stopping the containers Integration of Load Balancers Fargate Serverless container service No need to provision any infrastructure","title":"ECS vs Fargate"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/02 IAM User/","text":"Creating IAM User Go to IAM Service From left Users Click Add User User name ecs-course Access Type both Programmatic and Management Console For permissions, from Attach Existing Policy , take AmazonECS Full Access and Administrator Access Review and create user","title":"02 IAM User"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/02 IAM User/#creating-iam-user","text":"Go to IAM Service From left Users Click Add User User name ecs-course Access Type both Programmatic and Management Console For permissions, from Attach Existing Policy , take AmazonECS Full Access and Administrator Access Review and create user","title":"Creating IAM User"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/03 First Container Launch/","text":"First Container Launch We are going to launch a nginx server in the Fargate . Logged in with a user ecs-course we created. Go to service Elastic Container Service Click get started We have to Define container Define task Define service Define Cluster Define Container Select nginx Ensure Compatibilities as FARGATE We can change the configuration by click Edit , ex change the name to quickstart-nginx-td Click next and define the service In the top image, you should notice, the Container and Task definition is completed by green check mark. From the Edit button we can change the service name and lets make the name quickstart-nginx-service Click next and define the cluster In the top image this time the service should also get the green check mark. In cluster section, change the name to qucikstart-nginx-cluster Click next , review all the config and created all the services, it might take couple of minutes. After completion, we can view the services by clicking the View Service button. Now, when we go to the ECS service dashboard, we should see our created cluster. Go to the cluster qucikstart-nginx-cluster From the Task details, we can get the IP of the server. If we visit, we should see Welcome to nginx! . To avoid any unnecessary costing/billing, select the service and delete it. To delete, From ECS dashboard, select the cluster and go to details Select the service and delete it To make sure the service is deleted, under the cluster dashboard, 0 services and 0 tasks are running.","title":"03 First Container Launch"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/03 First Container Launch/#first-container-launch","text":"We are going to launch a nginx server in the Fargate . Logged in with a user ecs-course we created. Go to service Elastic Container Service Click get started We have to Define container Define task Define service Define Cluster Define Container Select nginx Ensure Compatibilities as FARGATE We can change the configuration by click Edit , ex change the name to quickstart-nginx-td Click next and define the service In the top image, you should notice, the Container and Task definition is completed by green check mark. From the Edit button we can change the service name and lets make the name quickstart-nginx-service Click next and define the cluster In the top image this time the service should also get the green check mark. In cluster section, change the name to qucikstart-nginx-cluster Click next , review all the config and created all the services, it might take couple of minutes. After completion, we can view the services by clicking the View Service button. Now, when we go to the ECS service dashboard, we should see our created cluster. Go to the cluster qucikstart-nginx-cluster From the Task details, we can get the IP of the server. If we visit, we should see Welcome to nginx! . To avoid any unnecessary costing/billing, select the service and delete it. To delete, From ECS dashboard, select the cluster and go to details Select the service and delete it To make sure the service is deleted, under the cluster dashboard, 0 services and 0 tasks are running.","title":"First Container Launch"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/04 ECS Pricing/","text":"ECS Pricing For EC2 launch type model, ECS is free Have to pay for the underlying hardware and computations, like, ec2 instance EBS Load balancers EFS For Fargate , Have to pay for using Ram and vCPU in hourly basis It is possible to use spot instance here","title":"04 ECS Pricing"},{"location":"Notes/08 Docker on AWS/01 ECS Quick Start/04 ECS Pricing/#ecs-pricing","text":"For EC2 launch type model, ECS is free Have to pay for the underlying hardware and computations, like, ec2 instance EBS Load balancers EFS For Fargate , Have to pay for using Ram and vCPU in hourly basis It is possible to use spot instance here","title":"ECS Pricing"},{"location":"Notes/08 Docker on AWS/02 ECS Cluster Setup/01 Overview/","text":"Overview ECS Clusters Logical group of EC2 instance In this case, the EC2 instance has the ECS agents. Alternatively, we can use AMI which is already included the ECS agents. ECS Agents responsible for Connect ECS Service Logging in Cloudwatch Integrate with ECR In these EC2 instance, there should be multiple tasks with appropriate IAM role. If the task needs to interact with S3, it should have the IAM role with S3 related permission included.","title":"01 Overview"},{"location":"Notes/08 Docker on AWS/02 ECS Cluster Setup/01 Overview/#overview","text":"ECS Clusters Logical group of EC2 instance In this case, the EC2 instance has the ECS agents. Alternatively, we can use AMI which is already included the ECS agents. ECS Agents responsible for Connect ECS Service Logging in Cloudwatch Integrate with ECR In these EC2 instance, there should be multiple tasks with appropriate IAM role. If the task needs to interact with S3, it should have the IAM role with S3 related permission included.","title":"Overview"},{"location":"Notes/08 Docker on AWS/02 ECS Cluster Setup/02 IAM Roles For ECS/","text":"IAM Roles For ECS To set up the ECS we will require 4 types of roles. If you go to IAM -> Roles -> Create Role -> Elastic Container Service , you will notice the 4 roles use case. We will have to go through each of them and create these roles. Role For EC2 : Will be attached with to the EC2 instance. This will allow the ECS Agent to communicate with the ECS and ECR . Create a role named ecsInstanceRole from EC2 Role for Elastic Container Service . Allows EC2 instances in an ECS cluster to access ECS. Role for ECS : Will be attached to the ECS . This will allow ECS to manage resources on our behalf. Create a role named ecsRole from Elastic Container Service . Allows ECS to create and manage AWS resources on your behalf. Role for ECS Task : Will be attached to the ECS Task . This will allow execute the task. Create a role named ecsAutoscalingRole from the Elastic Container Service Autoscale . Allows Auto Scaling to access and update ECS services. Role for Auto Scaling : Only if we use EC2 instance to run docker this role will be required. For Fargate we will not require any of these. Create a role named ecsTaskExecutionRole with only AmazonECSTaskExecutionRolePolicy policy from Elastic Container Service Task . Allows ECS tasks to call AWS services on your behalf.","title":"02 IAM Roles For ECS"},{"location":"Notes/08 Docker on AWS/02 ECS Cluster Setup/02 IAM Roles For ECS/#iam-roles-for-ecs","text":"To set up the ECS we will require 4 types of roles. If you go to IAM -> Roles -> Create Role -> Elastic Container Service , you will notice the 4 roles use case. We will have to go through each of them and create these roles. Role For EC2 : Will be attached with to the EC2 instance. This will allow the ECS Agent to communicate with the ECS and ECR . Create a role named ecsInstanceRole from EC2 Role for Elastic Container Service . Allows EC2 instances in an ECS cluster to access ECS. Role for ECS : Will be attached to the ECS . This will allow ECS to manage resources on our behalf. Create a role named ecsRole from Elastic Container Service . Allows ECS to create and manage AWS resources on your behalf. Role for ECS Task : Will be attached to the ECS Task . This will allow execute the task. Create a role named ecsAutoscalingRole from the Elastic Container Service Autoscale . Allows Auto Scaling to access and update ECS services. Role for Auto Scaling : Only if we use EC2 instance to run docker this role will be required. For Fargate we will not require any of these. Create a role named ecsTaskExecutionRole with only AmazonECSTaskExecutionRolePolicy policy from Elastic Container Service Task . Allows ECS tasks to call AWS services on your behalf.","title":"IAM Roles For ECS"},{"location":"Notes/08 Docker on AWS/02 ECS Cluster Setup/03 Core Infrastructure for ECS/","text":"Core Infrastructure for ECS We will use a Cloudformation template to build up the infrastructure. In the infrastructure, we will have a VPC , Subnets on different AZ , Internet Gateway , Route Table . Lets create a Cloudformation template named core-infrastructure-setup.yml which should look like, AWSTemplateFormatVersion: '2010-09-09' Description: VPC and subnets as base for an ECS cluster Parameters: EnvironmentName: Type: String Default: ecs-course Mappings: SubnetConfig: VPC: CIDR: '172.16.0.0/16' PublicOne: CIDR: '172.16.0.0/24' PublicTwo: CIDR: '172.16.1.0/24' Resources: VPC: Type: AWS::EC2::VPC Properties: EnableDnsSupport: true EnableDnsHostnames: true CidrBlock: !FindInMap ['SubnetConfig', 'VPC', 'CIDR'] PublicSubnetOne: Type: AWS::EC2::Subnet Properties: AvailabilityZone: Fn::Select: - 0 - Fn::GetAZs: { Ref: 'AWS::Region' } VpcId: !Ref 'VPC' CidrBlock: !FindInMap ['SubnetConfig', 'PublicOne', 'CIDR'] MapPublicIpOnLaunch: true PublicSubnetTwo: Type: AWS::EC2::Subnet Properties: AvailabilityZone: Fn::Select: - 1 - Fn::GetAZs: { Ref: 'AWS::Region' } VpcId: !Ref 'VPC' CidrBlock: !FindInMap ['SubnetConfig', 'PublicTwo', 'CIDR'] MapPublicIpOnLaunch: true InternetGateway: Type: AWS::EC2::InternetGateway GatewayAttachement: Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref 'VPC' InternetGatewayId: !Ref 'InternetGateway' PublicRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref 'VPC' PublicRoute: Type: AWS::EC2::Route DependsOn: GatewayAttachement Properties: RouteTableId: !Ref 'PublicRouteTable' DestinationCidrBlock: '0.0.0.0/0' GatewayId: !Ref 'InternetGateway' PublicSubnetOneRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnetOne RouteTableId: !Ref PublicRouteTable PublicSubnetTwoRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnetTwo RouteTableId: !Ref PublicRouteTable Outputs: VpcId: Description: The ID of the VPC that this stack is deployed in Value: !Ref 'VPC' Export: Name: !Sub ${EnvironmentName}:VpcId PublicSubnetOne: Description: Public subnet one Value: !Ref 'PublicSubnetOne' Export: Name: !Sub ${EnvironmentName}:PublicSubnetOne PublicSubnetTwo: Description: Public subnet two Value: !Ref 'PublicSubnetTwo' Export: Name: !Sub ${EnvironmentName}:PublicSubnetTwo Make sure the aws-cli is installed in your machine. Now create the stack from the Cloudformation template by, aws cloudformation create-stack --capabilities CAPABILITY_IAM --stack-name ecs-core-infrastructure --template-body file://./core-infrastructure-setup.yml","title":"03 Core Infrastructure for ECS"},{"location":"Notes/08 Docker on AWS/02 ECS Cluster Setup/03 Core Infrastructure for ECS/#core-infrastructure-for-ecs","text":"We will use a Cloudformation template to build up the infrastructure. In the infrastructure, we will have a VPC , Subnets on different AZ , Internet Gateway , Route Table . Lets create a Cloudformation template named core-infrastructure-setup.yml which should look like, AWSTemplateFormatVersion: '2010-09-09' Description: VPC and subnets as base for an ECS cluster Parameters: EnvironmentName: Type: String Default: ecs-course Mappings: SubnetConfig: VPC: CIDR: '172.16.0.0/16' PublicOne: CIDR: '172.16.0.0/24' PublicTwo: CIDR: '172.16.1.0/24' Resources: VPC: Type: AWS::EC2::VPC Properties: EnableDnsSupport: true EnableDnsHostnames: true CidrBlock: !FindInMap ['SubnetConfig', 'VPC', 'CIDR'] PublicSubnetOne: Type: AWS::EC2::Subnet Properties: AvailabilityZone: Fn::Select: - 0 - Fn::GetAZs: { Ref: 'AWS::Region' } VpcId: !Ref 'VPC' CidrBlock: !FindInMap ['SubnetConfig', 'PublicOne', 'CIDR'] MapPublicIpOnLaunch: true PublicSubnetTwo: Type: AWS::EC2::Subnet Properties: AvailabilityZone: Fn::Select: - 1 - Fn::GetAZs: { Ref: 'AWS::Region' } VpcId: !Ref 'VPC' CidrBlock: !FindInMap ['SubnetConfig', 'PublicTwo', 'CIDR'] MapPublicIpOnLaunch: true InternetGateway: Type: AWS::EC2::InternetGateway GatewayAttachement: Type: AWS::EC2::VPCGatewayAttachment Properties: VpcId: !Ref 'VPC' InternetGatewayId: !Ref 'InternetGateway' PublicRouteTable: Type: AWS::EC2::RouteTable Properties: VpcId: !Ref 'VPC' PublicRoute: Type: AWS::EC2::Route DependsOn: GatewayAttachement Properties: RouteTableId: !Ref 'PublicRouteTable' DestinationCidrBlock: '0.0.0.0/0' GatewayId: !Ref 'InternetGateway' PublicSubnetOneRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnetOne RouteTableId: !Ref PublicRouteTable PublicSubnetTwoRouteTableAssociation: Type: AWS::EC2::SubnetRouteTableAssociation Properties: SubnetId: !Ref PublicSubnetTwo RouteTableId: !Ref PublicRouteTable Outputs: VpcId: Description: The ID of the VPC that this stack is deployed in Value: !Ref 'VPC' Export: Name: !Sub ${EnvironmentName}:VpcId PublicSubnetOne: Description: Public subnet one Value: !Ref 'PublicSubnetOne' Export: Name: !Sub ${EnvironmentName}:PublicSubnetOne PublicSubnetTwo: Description: Public subnet two Value: !Ref 'PublicSubnetTwo' Export: Name: !Sub ${EnvironmentName}:PublicSubnetTwo Make sure the aws-cli is installed in your machine. Now create the stack from the Cloudformation template by, aws cloudformation create-stack --capabilities CAPABILITY_IAM --stack-name ecs-core-infrastructure --template-body file://./core-infrastructure-setup.yml","title":"Core Infrastructure for ECS"}]}