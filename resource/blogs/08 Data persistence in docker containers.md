Containers are ephemeral by design, once stopped any data written in the container will be erased. This is not a limitation of the container, more of a design goal. This is the idea of immutable infrastructure is, we do not change things once system is running. For any changes, we redeploy the whole new container. But the tradeoff is the unique dataset, generated by the app itself. If we redeploy the whole container the unique dataset inside the container will be lost.

The `separation of concern` suggest, we should not mix the data set with the application binaries. Docker gives us two big favors here,

- Data Volumes (Named Volumes)
  - Special location outside the Union file system
  - Under Dockers storage directory
  - Docker sees and manages this as a local file/folder
- Bind Mounts
  - Mount a host machines file/folder into docker container
  - Processes outside the docker has access, so can modify this from host machine

> `Volumes` and `Bind Mounts` are both outside the container, but `Volumes` are under docker engine.

According to the Docker doc,

> Bind mounts have been around since the early days of Docker. Bind mounts have limited functionality compared to volumes. When you use a bind mount, a file or directory on the host machine is mounted into a container. The file or directory is referenced by its full or relative path on the host machine. By contrast, when you use a volume, a new directory is created within Docker’s storage directory on the host machine, and Docker manages that directory’s contents.

### Data Volumes

---

If we use `Data Volumes` to persist data, we have to define a volume. Then any files or data put in that volume will outlive the container. If we remove the container, it will not remove the data volume, instead we have to take one more step to remove the volume, `docker volume prune`. This extra step is just for an insurance, to ensure the the data volumes are much more important than the container itself.

Let's pull the `mysql` image,

```docker
docker pull mysql
```

Now, inspect the image,

```docker
docker image inspect mysql
```

Inside the `Config.Volumes`, we should see a volumes, `/var/lib/mysql`.

We now run a container from the image,

```docker
docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=true mysql
```

We can verify the container is running by,

```docker
docker container ls
```

In the output list, we should see the `mysql` container is running.

We can inspect the container,

```docker
docker container inspect mysql
```

Under `Mounts`, we should see `/var/lib/mysql` as mounted volumes list. When a container is running, the container assume the directory path is `/var/lib/mysql`, but it's actual path is defined in the `Source` property and in my case, the actual path is `/var/lib/docker/volumes/0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13/_data`.

We can see the list of mounted volumes,

```docker
docker volume ls
```

This should give all the mounted volumes by the docker containers.

We can get the volume name by the `docker container inspect mysql`. From this command output, under `Mounts`, we can get the mounted volumes name. To inspect our specific volume, we can run inspect command by the name,

```docker
docker volume inspect 0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13
```

In output, there is a property `Mountpoint` and from the linux machine, we can directly access that mount point,

```bash
cd /var/lib/docker/volumes/0097b12ff7e460e44174a208e8c52ba40eeb882723696e8057906403c3a17e13/_data
```

This should take us the `_data` directory.

Lets, remove the mysql container,

```docker
docker container rm mysql
```

Now, if we look for the volumes,

```docker
docker volume ls
```

We will notice, even though our container is removed, our data is persisted. This solves our data persistency problem.

Although, from container, we can find the mounted volumes, but from volume perspective, we can not say which container it is connected to.

### Named Volumes (An Enhance of Data Volumes)

---

To make `Data Volumes` more user friendly, we can use `Named Volumes`. To use `Named Volumes` we have to specify the volume config on the `docker run` command with `-v` flag.

We can create a mysql container with `Named Volume` by,

```docker
docker container run -d --name mysql -e MYSQL_ALLOW_EMPTY_PASSWORD=true -v mysql-db:/var/lib/mysql mysql
```

Now we can easily inspect the volume by name,

```docker
docker volume inspect mysql-db
```

This will give the details of the `mysql-db` platform.

It is also possible to create volume ahead of time.

An example for the `Named Volumes` can be database upgrade. For example, we are using a `postgres` of version `12.6`. Now there is a update to `13.2`. In this case, if we want to upgrade database, we should not loose the existing data.

**Steps**

- Run container with postgres 12.6
- Check the volumes
- Check logs and stop container
- Create another container with postgres 13.6 with same named volume
- Check logs to validate

To do so, when we create a database, we first have to define a `Named Volumes` and when do the upgrade, we have to specify the data directory.

Let's first create a database with legacy version and specify the `Named Volumes`,

```docker
docker run --name postgres_legacy -d -e POSTGRES_HOST_AUTH_METHOD=trust -v postgres_data:/var/lib/postgresql/data postgres:12.6
```

Here `POSTGRES_HOST_AUTH_METHOD=trust` to allow all connections without a password. It's not a recommended approach. By default postgres put data in the `/var/lib/postgresql/data` directory.

We can keep watching the logs of the server,

```docker
docker container logs -f postgres_legacy
```

Here, `-f` allow to keep watching the logs.

We should see `database system is ready to accept connections` in the database logs.

We can look at the volume list,

```docker
docker volume ls
```

We should see a volume named `postgres_data`.

To upgrade to `13.2`, we first stop the previous version,

```docker
docker container stop postgres_legacy
```

Now create a new postgres server with updated version and configured the same `Named Volume`,

```docker
docker run --name postgres_new -d -e POSTGRES_HOST_AUTH_METHOD=trust -v postgres_data:/var/lib/postgresql/data postgres:13.2
```

Since, we are using existing data directory, our server will start quickly with few logs,

```docker
docker container logs postgres_new
```

If we look into the volumes, we should there is no additional volume for the new updated postgres server, it is using the same as legacy one, `postgres_data`

```docker
docker volume ls
```

### Bind Mounting

---

With `Bind Mounting`, in local environment, we can edit files in the host machine and that is used inside the container.

Bind mounting is mapping mapping host files and directories into container files and directories. It skips the union file system and on delete a container, do not erase the data from host machine.

If we have files that is mapped to the host files and also exist in the container, in this case the host files will be used.

To test the `Bind Mounting` we will use the `nginx` image. First create a file `index.html` as follows,

```bash
touch index.html
```

Make our `index.html` file as simple as possible,

```html
<!DOCTYPE html>
<html>
  <head>
    <title>Docker Volumes Testing</title>
  </head>
  <body>
    Hello From Bind Mounting
  </body>
</html>
```

Now create a container of the `nginx` image to use this `index.html`,

```docker
docker container run -d --name nginx -p 80:80 -v "$(pwd)":/usr/share/nginx/html nginx
```

Now if we browse `http://localhost/` from browser, we should see the the `nginx` server is serving our `index.html` instead of the container itself.

> If you are in the windows machine, instead of `"$(pwd)"`, for PowerShell use `${pwd}` and for cmd.exe "Command Prompt use: `%cd%`

If we change the content of the `index.html` and reload the browser with address `http://localhost/`, we should see the new content from the `index.html`.
